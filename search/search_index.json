{
    "docs": [
        {
            "location": "/",
            "text": "OSG Operations\n\n\nWelcome to the home page of the OSG Operations Team documentation area!\n\n\nMission\n\n\nThe mission of OSG Operations is to maintain and improve distributed high throughput computing services to support\nresearch communities.\n\n\nThis is accomplished by:\n\n\n\n\nOperating and maintaining our services in a user-oriented, robust, and reliable manner.\n\n\nDeveloping a professional and skilled staff dedicated to a service philosophy.\n\n\nManaging resources responsibly, efficiently, and with accountability.\n\n\nEvaluating and continually improving the actions, methods and processes that allow the OSG to operate.\n\n\n\n\nContact Us\n\n\n\n\nOpen a Ticket\n\n\nSlack channel\n - if you can't create an account, send an e-mail\n  to \nhelp@opensciencegrid.org\n\n\nEmail: \nhelp@opensciencegrid.org\n\n\n\n\nRegistration (Contact, Resource, VO, or Project)\n\n\nRegister with OSG\n\n\nWeekly Operations Meetings\n\n\nWhen:\n Fridays 12:30 pm Central\n\n\nURL:\n \nhttps://unl.zoom.us/j/183382852\n\n\nPhone:\n +1 669 900 6833  or +1 408 638 0968  or +1 646 876 9923\n\n\nMeeting ID:\n 183 382 852\n\n\nMeeting Minutes\n\n\n\n\nOctober 11, 2019\n\n\nOctober 4, 2019\n\n\nSeptember 27, 2019\n\n\nSeptember 20, 2019\n\n\nSeptember 13, 2019\n\n\nSeptember 6, 2019\n\n\nAugust 30, 2019\n\n\nAugust 23, 2019\n\n\nAugust 16, 2019\n\n\nAugust 9, 2019\n\n\nAugust 2, 2019\n\n\nJuly 26, 2019\n\n\nJuly 19, 2019\n\n\nJuly 12, 2019\n\n\nJuly 8, 2019\n\n\nJuly 1, 2019\n\n\nJune 24, 2019\n\n\nJune 17, 2019\n\n\nJune 10, 2019\n\n\nJune 3, 2019\n\n\nMay 28, 2019\n\n\nMay 20, 2019\n\n\nMay 13, 2019\n\n\nMay 6, 2019\n\n\nApril 29, 2019\n\n\nApril 22, 2019\n\n\nApril 15, 2019\n\n\nApril 8, 2019\n\n\nApril 1, 2019\n\n\nMarch 25, 2019\n\n\nMarch 18, 2019 (canceled due to HOW 2019)\n\n\nMarch 11, 2019\n\n\nMarch 4, 2019\n\n\nFebruary 25, 2019\n\n\nFebruary 19, 2019\n\n\nFebruary 11, 2019\n\n\nFebruary 4, 2019\n\n\nJanuary 28, 2019 (canceled due to F2F meeting)\n\n\nJanuary 22, 2019\n\n\nJanuary 14, 2019\n\n\nJanuary 7, 2019\n\n\nDecember 31, 2018 (canceled)\n\n\nDecember 24, 2018 (canceled)\n\n\nDecember 17, 2018\n\n\nDecember 10, 2018\n\n\nDecember 3, 2018\n\n\nNovember 26, 2018\n\n\nNovember 19, 2018\n\n\nNovember 13, 2018\n\n\nNovember 5, 2018 (canceled)\n\n\nOctober 29, 2018 (canceled)\n\n\nOctober 22, 2018 (canceled)\n\n\nOctober 15, 2018\n\n\nOctober 8, 2018\n\n\nOctober 1, 2018\n\n\nSeptember 24, 2018\n\n\nSeptember 17, 2018\n\n\nSeptember 10, 2018\n\n\nSeptember 4, 2018\n\n\nAugust 27, 2018\n\n\nAugust 20, 2018\n\n\nAugust 13, 2018\n\n\nAugust 6, 2018\n\n\n\n\nArchived Meeting Minutes\n\n\nFor archived meeting minutes, see the \nGitHub repository",
            "title": "Home"
        },
        {
            "location": "/#osg-operations",
            "text": "Welcome to the home page of the OSG Operations Team documentation area!",
            "title": "OSG Operations"
        },
        {
            "location": "/#mission",
            "text": "The mission of OSG Operations is to maintain and improve distributed high throughput computing services to support\nresearch communities.  This is accomplished by:   Operating and maintaining our services in a user-oriented, robust, and reliable manner.  Developing a professional and skilled staff dedicated to a service philosophy.  Managing resources responsibly, efficiently, and with accountability.  Evaluating and continually improving the actions, methods and processes that allow the OSG to operate.",
            "title": "Mission"
        },
        {
            "location": "/#contact-us",
            "text": "Open a Ticket  Slack channel  - if you can't create an account, send an e-mail\n  to  help@opensciencegrid.org  Email:  help@opensciencegrid.org",
            "title": "Contact Us"
        },
        {
            "location": "/#registration-contact-resource-vo-or-project",
            "text": "Register with OSG",
            "title": "Registration (Contact, Resource, VO, or Project)"
        },
        {
            "location": "/#weekly-operations-meetings",
            "text": "When:  Fridays 12:30 pm Central  URL:   https://unl.zoom.us/j/183382852  Phone:  +1 669 900 6833  or +1 408 638 0968  or +1 646 876 9923  Meeting ID:  183 382 852",
            "title": "Weekly Operations Meetings"
        },
        {
            "location": "/#meeting-minutes",
            "text": "October 11, 2019  October 4, 2019  September 27, 2019  September 20, 2019  September 13, 2019  September 6, 2019  August 30, 2019  August 23, 2019  August 16, 2019  August 9, 2019  August 2, 2019  July 26, 2019  July 19, 2019  July 12, 2019  July 8, 2019  July 1, 2019  June 24, 2019  June 17, 2019  June 10, 2019  June 3, 2019  May 28, 2019  May 20, 2019  May 13, 2019  May 6, 2019  April 29, 2019  April 22, 2019  April 15, 2019  April 8, 2019  April 1, 2019  March 25, 2019  March 18, 2019 (canceled due to HOW 2019)  March 11, 2019  March 4, 2019  February 25, 2019  February 19, 2019  February 11, 2019  February 4, 2019  January 28, 2019 (canceled due to F2F meeting)  January 22, 2019  January 14, 2019  January 7, 2019  December 31, 2018 (canceled)  December 24, 2018 (canceled)  December 17, 2018  December 10, 2018  December 3, 2018  November 26, 2018  November 19, 2018  November 13, 2018  November 5, 2018 (canceled)  October 29, 2018 (canceled)  October 22, 2018 (canceled)  October 15, 2018  October 8, 2018  October 1, 2018  September 24, 2018  September 17, 2018  September 10, 2018  September 4, 2018  August 27, 2018  August 20, 2018  August 13, 2018  August 6, 2018",
            "title": "Meeting Minutes"
        },
        {
            "location": "/#archived-meeting-minutes",
            "text": "For archived meeting minutes, see the  GitHub repository",
            "title": "Archived Meeting Minutes"
        },
        {
            "location": "/services/install-gwms-factory/",
            "text": "GlideinWMS Factory Installation\n\n\nThis document describes how to install a Glidein Workflow Managment System (GlideinWMS) Factory instance. \n\n\nThis document assumes expertise with HTCondor and familiarity with the GlideinWMS software.  It \ndoes not\n cover anything but the simplest possible install.   Please consult the \nGlideinWMS reference documentation\n for advanced topics, including non-root, non-RPM-based installation.  In this document the terms glidein and pilot (job) will be used interchangeably.\n\n\n\n\nThis parts covers these primary components of the GlideinWMS system:\n\n\n\n\nWMS Collector / Schedd\n: A set of \ncondor_collector\n and \ncondor_schedd\n processes that allow the submission of pilots to Grid entries.  \n\n\nGlideinWMS Factory\n: The process submitting the pilots when needed\n\n\n\n\n\n\nWarning\n\n\nWe really recommend you to \nuse the OSG provided Factory and not to install your own\n. A \nVO Frontend\n is sufficient to submit your jobs and to decide scheduling policies. And this will avoid for you the complexity to deal directly with grid/cloud sites. If you really need you own Factory be aware that it is a complex component and may require a non trivial maintenance effort.\n\n\n\n\nBefore Starting\n\n\nBefore starting the installation process, consider the following points (consulting \nthe Reference section below\n as needed):\n\n\nRequirements\n\n\nHost and OS\n\n\n\n\nA host to install the \nGlideinWMS Factory\n (pristine node). \n\n\nCurrently most of our testing has been done on Scientific Linux 6 and 7.\n\n\nRoot access\n\n\n\n\nThe GlideinWMS Factory has the following requirements:\n\n\n\n\nCPU\n: 4-8 cores for a large installation (1 should suffice on a small install)\n\n\nRAM\n: 4-8GB on a large installation (1GB should suffice for small installs)\n\n\nDisk\n:  10GB will be plenty sufficient for all the binaries, config and log files related to GlideinWMS.  If you are a large site with need to keep significant history and logs, you may want to allocate 100GB+ to store long histories.\n\n\n\n\nUsers\n\n\nThe GlideinWMS Factory installation will create the following users unless they are \nalready created\n.\n\n\n\n\n\n\n\n\nUser\n\n\nDefault uid\n\n\nComment\n\n\n\n\n\n\n\n\n\n\ncondor\n\n\nnone\n\n\nHTCondor user (installed via dependencies).\n\n\n\n\n\n\ngfactory\n\n\nnone\n\n\nThis user runs the GlideinWMS VO factory.\n\n\n\n\n\n\n\n\nTo verify that the user \ngfactory\n has \ngfactory\n as primary group check the output of \n\n\nroot@host #\n getent passwd gfactory \n|\n cut -d: -f4 \n|\n xargs getent group\n\n\n\n\n\nIt should be the \ngfactory\n group. \n\n\n\n\nNote\n\n\nIf you get another one, you just need to modify the \nPrivSep Kernel configuration file\n \n/etc/condor/privsep_config\n as indicated above and establish the corresponding GID. The lists that you see in this file, specify the IDs of all users and groups that HTCondor jobs may use on the given execute machine. In other words, users and groups that HTCondor will be allowed to act on behalf of.\n\n\n\n\nCertificates\n\n\n\n\n\n\n\n\nCertificate\n\n\nUser that owns certificate\n\n\nPath to certificate\n\n\n\n\n\n\n\n\n\n\nHost certificate\n\n\nroot\n\n\n/etc/grid-security/hostcert.pem  /etc/grid-security/hostkey.pem\n\n\n\n\n\n\n\n\nHere\n are instructions to request a host certificate.\n\n\nThe host \ncertificate/key\n is used for authorization,  however, authorization between the Factory and the GlideinWMS collector is done by file system authentication.\n\n\nNetworking\n\n\nFirewalls\n\n\nIt must be on the public internet, with at least one port open to the world; all worker nodes will load data from this node trough HTTP. Note that worker nodes will also need outbound access in order to access this HTTP port. \n\n\nInstallation Procedure\n\n\nAs with all OSG software installations, there are some one-time (per host) steps to prepare in advance:\n\n\n\n\nEnsure the host has a \nsupported operating system\n\n\nObtain root access to the host\n\n\nPrepare the \nrequired Yum repositories\n\n\nInstall \nCA certificates\n\n\n\n\nInstalling HTCondor\n\n\nMost required software is installed from the Factory RPM installation. HTCondor is the only exception since there are \nmany different ways to install it\n, using the RPM system or not. \nYou need to have HTCondor installed before installing the GlideinWMS Factory. If yum cannot find a HTCondor RPM, it will install the dummy \nempty-condor\n RPM, assuming that you installed HTCondor using a tarball distribution.\n\n\n\n\n\n\nIf you don't have HTCondor already installed, you can install the HTCondor RPM from the OSG repository:\n\n\nroot@host #\n yum install condor.x86_64\n\n\n\n\n\n\n\n\n\nIf you have installed HTCondor version > 8.7.2, you must install the \nglideinwms-switchboard\n:\n\n\nroot@host #\n yum install glideinwms-switchboard --enablerepo\n=\nosg-upcoming\n\n\n\n\n\n\n\n\n\nInstalling HTCondor-BOSCO\n\n\nIf you plan to send jobs using \ndirect batch submission\n (aka BOSCO), then you need also the \ncondor-bosco\n package. You'll have to install the package and remove one of its files \n/etc/condor/config.d/60-campus_factory.config\n because it interferes with the Factory configuration.\n\n\nroot@host #\n yum install condor-bosco\n\nroot@host #\n rm /etc/condor/config.d/60-campus_factory.config\n\nroot@host #\n touch /etc/condor/config.d/60-campus_factory.config\n\n\n\n\n\nInstall GWMS Factory\n\n\nDownload and install the Factory RPM\n\n\nInstall the RPM and dependencies (be prepared for a lot of dependencies).\n\n\nroot@host #\n yum install glideinwms-factory\n\n\n\n\n\nThis will install the current production release verified and tested by OSG with default HTCondor configuration.\nThis command will install the GlideinWMS Factory, HTCondor, the OSG client, and all the required dependencies.\n\n\nIf you wish to install a different version of GlideinWMS, add the \"--enablerepo\" argument to the command as follows:\n\n\n\n\nyum install --enablerepo=osg-testing glideinwms-factory\n: The most recent production release, still in testing phase.  This will usually match the current tarball version on the \nGlideinWMS home page\n.\n    (The osg-release production version may lag behind the tarball release by a few weeks as it is verified and packaged by OSG).  Note that this will also take the osg-testing versions of all dependencies as well.\n\n\nyum install --enablerepo=osg-upcoming glideinwms-factory\n:  The most recent development series release, ie version 3.3.x release.  This has newer features such as cloud submission support, but is less tested.\n\n\n\n\nDownload HTCondor tarballs\n\n\nYou will need to download HTCondor tarballs for each architecture that \nyou want to deploy pilots on\n.\nAt this point, GlideinWMS factory does not support pulling HTCondor binaries from your system area.\nSuggested is that you put these binaries in \n/var/lib/gwms-factory/condor\n but any \ngfactory\n accessible location should suffice.\n\n\nConfiguration Procedure\n\n\nAfter installing the RPM you need to configure the components of the GlideinWMS Factory:\n\n\n\n\nEdit Factory configuration options\n\n\nEdit HTCondor configuration options\n\n\nCreate a HTCondor grid map file\n\n\nReconfigure and Start Factory\n\n\n\n\nConfiguring the Factory\n\n\nThe configuration file is \n/etc/gwms-factory/glideinWMS.xml\n.  The next steps will describe each line that you will need to edit for most cases, but you may want to review the whole file to be sure that it is configured correctly.\n\n\nSecurity configuration\n\n\nIn the security section, you will need to provide \neach Frontend\n that is allowed to communicate with the Factory:\n\n\nsecurity key_length=\"2048\" pub_key=\"RSA\" remove_old_cred_age=\"30\" remove_old_cred_freq=\"24\" reuse_oldkey_onstartup_gracetime=\"900\">\n  \n<frontends>\n\n     \n<frontend\n \nidentity=\n\"vofrontend_service@FACTORY_COLLECTOR_HOSTNAME\"\n \nname=\n\"\nORANGE\nvofrontend_sec_name\n\"\n>\n\n        \n<security_classes>\n\n           \n<security_class\n \nname=\n\"\nfrontend_sec_class\n\"\n \nusername=\n\"frontend\"\n \n/>\n\n        \n</security_classes>\n\n     \n</frontend>\n\n  \n</frontends>\n\n\n</security>\n\n\n\n\n\n\nThese attributes are very important to get exactly right or the Frontend will not be trusted.  This should match one of the \nfactory\n and \nsecurity\n sections of the Frontend configuration \nConfiguring the GlideinWMS Frontend\n in the following way:\n\n\n\n\nNote\n\n\nThis is a snippet from the Frontend configuration (for reference), not the Factory that you are configuring now!\n\n\n\n\nFor the factory section:\n\n\n# from frontend.xml\n\n<factory\n \nquery_expr=\n'((stringListMember(\"VO\", GLIDEIN_Supported_VOs)))'\n>\n\n....\n   \n<collectors>\n\n      \n<collector\n \nDN=\n\"/DC=org/DC=doegrids/OU=Services/CN=FACTORY_COLLECTOR_HOSTNAME\"\n \n      \ncomment=\n\"Define factory collector globally for simplicity\"\n            \n      \nfactory_identity=\n\"gfactory@FACTORY_COLLECTOR_HOSTNAME\"\n \n      \nmy_identity=\n\"\nusername\n@FACTORY_COLLECTOR_HOSTNAME\"\n \n      \nnode=\n\"FACTORY_COLLECTOR_HOSTNAME\"\n/>\n\n   \n</collectors>\n\n\n</factory>\n\n\n\n\n\n\nFor the security:\n\n\n# from frontend.xml\n\n<security\n \nclassad_proxy=\n\"/tmp/vo_proxy\"\n \nproxy_DN=\n\"DN of vo_proxy\"\n\n      \nproxy_selection_plugin=\n\"ProxyAll\"\n\n      \nsecurity_name=\n\"The security name, this is used by factory\"\n\n      \nsym_key=\n\"aes_256_cbc\"\n>\n\n      \n<credentials>\n\n         \n<credential\n \nabsfname=\n\"/tmp/pilot_proxy\"\n \nsecurity_class=\n\"frontend\"\n\n         \ntrust_domain=\n\"OSG\"\n \ntype=\n\"grid_proxy\"\n/>\n\n      \n</credentials>\n\n\n</security>\n        \n\n\n\n\n\nNote that the identity of the Frontend must match what HTCondor authenticates the DN of the frontend to.  In \n/etc/condor/certs/condor_mapfile\n, there must be an entry with \nvofrontend_service\n definition (in this case):\n\n\nGSI\n \n\"^\\/DC\\=org\\/DC\\=doegrids\\/OU\\=Services\\/CN\\=Some\\ Name\\ 834323\n$\"\n \n%\nGREEN\n%\nvofrontend_service\n%\nENDCOLOR\n%\n\n\n\n\n\n\nEntry configuration\n\n\nEntries are grid/cloud endpoints (aka Compute Elements, or gatekeepers) that can accept job requests and run pilots (which will run user jobs).\nEach entry needs to be configured to communicate to a specific gatekeeper.\n\n\nAn example test entry is provided in the default GlideinWMS configuration file.  At the very least, you will need to modify the entry line:\n\n\n<entry\n \nname=\n\"\nENTRY_NAME\n\"\n \nenabled=\n\"True\"\n \nauth_method=\n\"grid_proxy\"\n \ntrust_domain=\n\"OSG\"\n \n\ngatekeeper=\n\"\ngatekeeper.domain.tld/jobmanager-type\n\"\n \ngridtype=\n\"gt2\"\n \nrsl=\n\"(queue=default)(jobtype=single)\"\n\n\nschedd_name=\n\"\nschedd_glideins2@FACTORY_HOSTNAME\n\"\n \nverbosity=\n\"std\"\n \nwork_dir=\n\"OSG\"\n>\n\n\n\n\n\n\nYou will need to modify the entry \nname\n and \ngatekeeper\n.  This will determine the gatekeeper that you access.  Specific gatekeepers often require specific \"rsl\" attributes that determine the job queue that you are submitting to, or other attributes.  Add them in the \nrsl\n attribute.  \n\n\nAlso, be sure to distribute your entries across the various HTCondor schedd work managers to balance load.  To see the available schedd use \ncondor_status -schedd -l | grep Name\n.  \n\n\nSeveral schedd options are configured by default for you:  \nschedd_glideins2, schedd_glideins3, schedd_glideins4, schedd_glideins5\n, as well as the default \nschedd\n. This can be modified in the HTCondor configuration.  Add any specific options, such as limitations on jobs/pilots or glexec/voms requirements in the entry section below the above line. More details are in the \nGlideinWMS Factory configuration guide\n.\n\n\n!!! warning\n      If there is no match between \nauth_metod\n and \ntrust_domain\n of the entry and the \ntype\n and \ntrust_domain\n listed in one of the \ncredentials of one of the Frontends\n using this Factory, then no job can run on that entry.\n\n\nThe Factory must advertise the correct Resource Name of each entry for accounting purposes. Then the Factory must also advertise in the entry all the attributes that will allow to match the query expression used in the Frontends connecting to this Factory (e.g. \n<factory query_expr='((stringListMember(\"\nPINK\nVO\n\", GLIDEIN_Supported_VOs)))'>\n as explained in the \nVO frontend configuration document\n ). \n\n\n\n\nNote\n\n\nKeep an eye on this part as we're dealing with singularity.\nThen you must advertise correctly if the site supports \ngLExec\n.\nIf it does not set \nGLEXEC_BIN\n to \nNONE\n, if \ngLExec\n is installed via OSG set it to \nOSG\n, \notherwise set it to the path of gLExec.\n\n\n\n\nFor example this snippet advertises \nGLIDEIN_Supported_VOs\n attribute with the supported VO so that can be used with the query above in the VO frontend and says that the resource does not support gLExec:\n\n\n<entry\n \nname=\n\"RESOURCE_NAME\"\n \n...\n\n   \n<config\n>\n\n   ...\n      \n<attrs>\n\n      ...\n         \n<attr\n \nname=\n\"GLIDEIN_Supported_VOs\"\n \nconst=\n\"True\"\n \nglidein_publish=\n\"True\"\n \njob_publish=\n\"True\"\n \n         \nparameter=\n\"True\"\n \npublish=\n\"True\"\n \ntype=\n\"string\"\n \nvalue=\n\"\nPINK\nVO\n\"\n/>\n\n         \n<attr\n \nname=\n\"GLEXEC_BIN\"\n \nconst=\n\"True\"\n \nglidein_publish=\n\"False\"\n \njob_publish=\n\"False\"\n \nparameter=\n\"True\"\n \n         \npublish=\n\"True\"\n \ntype=\n\"string\"\n \nvalue=\n\"\nNONE\n\"\n/>\n\n         \n<attr\n \nname=\n\"GLIDEIN_Resource_Name\"\n \nconst=\n\"True\"\n \nglidein_publish=\n\"True\"\n \njob_publish=\n\"True\"\n \n       \nparameter=\n\"True\"\n \npublish=\n\"True\"\n \ntype=\n\"string\"\n \nvalue=\n\"\nSiteNameFromOIM\n\"\n/>\n\n     \n</attrs>\n\n\n\n\n\n\n\n\nNote\n\n\nSpecially if jobs are sent to OSG resources, it is very important to set the GLIDEIN_Resource_Name and to be consistent with the Resource Name reported in OIM because that name will be used for job accounting in Gratia. It should be the name of the Resource in OIM or the name of the Resource Group (specially if there are many gatekeepers submitting to the same cluster).\n\n\nMore information on options can be found \nhere\n\n\n\n\nConfiguring Tarballs\n\n\nEach pilot will download HTCondor binaries from the staging area.  Often, multiple binaries are needed to support various architectures and platforms. \nCurrently, you will need to provide at least one tarball for GlideinWMS to use.  (Using the system binaries is currently not supported).\n\n\nDownload a HTCondor tarball from \nhere\n.  Suggested is to put the binaries in \n/var/lib/gwms-factory/condor\n, but any factory-accessible location will do just fine.\n\n\nOnce you have downloaded the tarball, configure it in \n/etc/gwms-factory/glideinWMS.xml\n like in the following:\n\n\n<condor_tarball\n \narch=\n\"default\"\n \nbase_dir=\n\"/var/lib/gwms-factory/condor/condor-8.7.6-x86_64_RedHat6-stripped\"\n \nos=\n\"rhel6\"\n \nversion=\n\"default\"\n/>\n\n\n\n\n\n\nRemember also to modify the \ncondor_os\n and \ncondor_arch\n attributes in the entries (the configured Compute Elements) to pick the correct HTCondor binary.\n\nHere\n are more details on using multiple HTCondor binaries. Note that is sufficient to set the \nbase_dir\n; the reconfigure command will prepare the tarball and add it to the XML config file.\n\n\nConfiguring HTCondor\n\n\nThe HTCondor configuration for the Factory is placed in \n/etc/condor/config.d\n.\n\n\n\n\n00_gwms_factory_general.config\n\n\n00-restart_peaceful.config\n\n\n01_gwms_factory_collectors.config\n\n\n02_gwms_factory_schedds.config\n\n\n03_gwms_local.config\n\n\n10-batch_gahp_blahp.config\n\n\n\n\nGet rid of the pre-loaded HTCondor default\n\n\nroot@host #\n rm /etc/condor/config.d/00personal_condor.config\n\nroot@host #\n touch /etc/condor/config.d/00personal_condor.config\n\n\n\n\n\nFor most installations, the items you need to modify are in \n03_gwms_factory_local.config\n.  The lines you will have to edit are:\n\n\n\n\nCredentials of the machine.  You can either run using a proxy, or a service certificate.  It is recommended to use a host certificate and specify its location in the variables \nGSI_DAEMON_CERT\n and \nGSI_DAEMON_KEY\n.  The host certificate should be owned by \nroot\n and have the correct permissions, 600.\n\n\nHTCondor ids in the form UID.GID (both are integers)\n\n\nHTCondor admin email. Will receive messages when services fail.\n\n\n\n\n#\n-- HTCondor user: condor\n\n\nCONDOR_IDS\n \n=\n\n\n#\n--  Contact (via email) when problems occur\n\n\nCONDOR_ADMIN\n \n=\n\n\n\n############################\n\n\n#\n \nGSI\n \nSecurity\n \nconfig\n\n\n############################\n\n\n#\n-- Grid Certificate directory\n\n\nGSI_DAEMON_TRUSTED_CA_DIR\n=\n \n/\netc\n/\ngrid\n-\nsecurity\n/\ncertificates\n\n\n\n#\n-- Credentials\n\n\nGSI_DAEMON_CERT\n \n=\n  \n/\netc\n/\ngrid\n-\nsecurity\n/\nhostcert\n.\npem\n\n\nGSI_DAEMON_KEY\n  \n=\n  \n/\netc\n/\ngrid\n-\nsecurity\n/\nhostkey\n.\npem\n\n\n\n#\n-- HTCondor mapfile\n\n\nCERTIFICATE_MAPFILE\n=\n \n/\netc\n/\ncondor\n/\ncerts\n/\ncondor_mapfile\n\n\n\n###################################\n\n\n#\n \nWhitelist\n \nof\n \nHTCondor\n \ndaemon\n \nDNs\n\n\n###################################\n\n\n#\nDAEMON_LIST\n \n=\n \nCOLLECTOR\n,\n \nMASTER\n,\n \nNEGOTIATOR\n,\n \nSCHEDD\n,\n \nSTARTD\n\n\n\n\n\n\nUsing other HTCondor RPMs, e.g. UW Madison HTCondor RPM\n\n\nThe above procedure will work if you are using the OSG HTCondor RPMS. You can verify that you used the OSG HTCondor RPM by using \nyum list condor\n. The version name should include \"osg\", e.g. \n8.6.9-1.1.osg34.el7\n.\n\n\nIf you are using the UW Madison HTCondor RPMS, be aware of the following changes:\n\n\n\n\nThis HTCondor RPM uses a file \n/etc/condor/condor_config.local\n to add your local machine slot to the user pool.\n\n\nIf you want to disable this behavior (recommended), you should blank out that file or comment out the line in \n/etc/condor/condor_config\n for LOCAL_CONFIG_FILE. (Make sure that LOCAL_CONFIG_DIR is set to \n/etc/condor/config.d\n)\n\n\nNote that the variable LOCAL_DIR is set differently in UW Madison and OSG RPMs. This should not cause any more problems in the Glideinwms RPMs, but please take note if you use this variable in your job submissions or other customizations.\n\n\n\n\nIn general if you are using a non OSG RPM or if you added custom configuration files for HTCondor please check the order of the configuration files:\n\n\nroot@host #\n condor_config_val -config\n\nConfiguration source:\n\n\n    /etc/condor/condor_config\n\n\nLocal configuration sources:\n\n\n    /etc/condor/config.d/00-restart_peaceful.config\n\n\n    /etc/condor/config.d/00_gwms_factory_general.config\n\n\n    /etc/condor/config.d/01_gwms_factory_collectors.config\n\n\n    /etc/condor/config.d/02_gwms_factory_schedds.config\n\n\n    /etc/condor/config.d/03_gwms_local.config\n\n\n    /etc/condor/config.d/10-batch_gahp_blahp.config\n\n\n    /etc/condor/condor_config.local\n\n\n\n\n\n\nRestarting HTCondor\n\n\nAfter configuring HTCondor, be sure to restart HTCondor:\n\n\nroot@host #\n service condor restart\n\n\n\n\n\nConfiguring HTCondor Privilege Separation\n\n\nLastly, verify the settings in \n/etc/condor/privsep_config\n.  By default,\nthe values in this file should not need to be modified.  However,\nif your user/group differs from \ngfactory\n / \ngfactory\n, or if you are operating\na Factory with multiple frontends, you will have to modify this file.\n\n\nThis file controls the HTCondor root switchboard, which allows the Factory\nto change permissions of files, specifically the proxies and files\npassed to it by the frontends.\n\n\nvalid\n-\ncaller\n-\nuids\n \n=\n \ngfactory\n\n\nvalid\n-\ncaller\n-\ngids\n \n=\n \ngfactory\n\n\nvalid\n-\ntarget\n-\nuids\n \n=\n \nfecmsucsd\n \n:\n \nfehcc\n \n:\n \nfecmscern\n\n\nvalid\n-\ntarget\n-\ngids\n \n=\n \nfecmsucsd\n \n:\n \nfehcc\n \n:\n \nfecmscern\n\n\nvalid\n-\ndirs\n \n=\n \n/\nvar\n/\nlib\n/\ngwms\n-\nfactory\n/\nclient\n-\nproxies\n\n\nvalid\n-\ndirs\n \n=\n \n/\nvar\n/\nlib\n/\ngwms\n-\nfactory\n/\nclient\n-\nlogs\n\n\nvalid\n-\ndirs\n \n=\n \n/\nvar\n/\nlog\n/\ngwms\n-\nfactory\n/\nclient\n\n\nvalid\n-\ndirs\n \n=\n \n/\nvar\n/\nlib\n/\ngwms\n-\nfactory\n\n\nvalid\n-\ndirs\n \n=\n \n/\nvar\n/\nlog\n/\ngwms\n-\nfactory\n\n\nprocd\n-\nexecutable\n \n=\n \n/\nusr\n/\nsbin\n/\ncondor_procd\n\n\n\n\n\n\nThus, the \nvalid-called-uids\n and \nvalid-caller-gids\n should match the user/group of your Factory user.\nThe \nvalid-target-uids\n and \nvalid-target-gids\n should be a colon-separated list of all the frontend\nusers and groups.  This should match the security_classes section in \n/etc/gwms-factory/glideinWMS.xml\n.\n\n\n\n\nNote\n\n\nPlease notice that from version 3.5, there is no need to configure HTCondor privilege separation. The Factory and all pilot jobs run under a single user (gfactory), which eliminates the need for the switchboard and setuid/user-switching.\n\n\n\n\nCreate a HTCondor grid mapfile.\n\n\nThe HTCondor grid mapfile \n/etc/condor/certs/condor_mapfile\n is used for authentication between the glidein running on a remote worker node, and the local collector.  HTCondor uses the mapfile to map certificates to pseudo-users on the local machine.  It is important that you map the DN's of each frontend you are talking to.\n\n\nBelow is an example mapfile, by default found in \n/etc/condor/certs/condor_mapfile\n:\n\n\nGSI\n \n\"^\\/DC\\=org\\/DC\\=doegrids\\/OU\\=People\\/CN\\=Some\\ Name\\ 123456$\"\n \nfrontend\n\n\nGSI\n \n(.\n*\n)\n \nanonymous\n\n\nFS\n \n(.\n*\n)\n \n\\\n1\n \n\n\n\n\n\nEach frontend needs a line that maps to the user specified in the identity argument in the frontend security section of the Factory configuration.\n\n\nReconfiguring GlideinWMS\n\n\nAfter changing the configuration of GlideinWMS and making sure that Factory is running, use the following table to find the appropriate command for your operating system (run as \nroot\n):\n\n\n\n\n\n\n\n\nIf your operating system is...\n\n\nRun the following command...\n\n\n\n\n\n\n\n\n\n\nEnterprise Linux 7\n\n\nsystemctl reload gwms-factory\n\n\n\n\n\n\nEnterprise Linux 6\n\n\nservice gwms-factory reconfig\n\n\n\n\n\n\n\n\nUpgrading GlideinWMS\n\n\nBefore you start the Factory service for the first time or after an update of the RPM or after you change GlideinWMS scripts, you should always use the GlideinWMS \"upgrade\" command. To do so:\n\n\n\n\n\n\nMake sure the \ncondor\n and \ngwms-factory\n services are stopped (in EL6 this will be done for you).\n\n\n\n\n\n\nIssue the upgrade command:\n\n\n\n\n\n\nIf you are using Enterprise Linux 7:\n\n\nroot@host #\n /usr/sbin/gwms-factory upgrade\n\n\n\n\n\n\n\n\n\nIf you are using Enterprise Linux 6:\n\n\nroot@host #\n service gwms-factory upgrade\n\n\n\n\n\n\n\n\n\nStart the \ncondor\n and \ngwms-factory\n services (see next part).\n\n\n\n\n\n\nImportant:\n\n\nIf you are upgrading to v3.5.x from 3.4.x or earlier you need some additional steps. The Factory and all pilot jobs will run under a single user (gfactory) to eliminate the need of switchboard and setuid/user-switching, no more separated users per-VO). After the RPM upgrade, you will need to::\n\n\n1.\n \nStop\n \nFactory\n \nand\n \nHTCondor\n\n\n2.\n \nMigrate\n \nthe\n \nHTCondor\n \nstatus\n \nrunning\n \nthe\n \nfact_chown\n \nscript\n,\n \nlocated\n \nin\n \nfactory\n/\ntools\n.\n \nAdd\n \nthe\n \nflag\n \n--backup to have backup of everything:\n\n\n\n        \n::\n:\nconsole\n\n\n        \nroot\n@host\n \n#\n \nsudo\n \nfact_chown\n \n--user=gfactory --group=gfactory --backup\n\n\n\n3.\n \nRestart\n \nHTCondor\n \nand\n \nthe\n \nFactory\n.\n\n\n\n\n\n\nTo revert to a version of GlideinWMS lower than 3.5, you need to restore the job_queue files and change back the permissions of the log directories. Those operations need to be also performed with both HTCondor and Factory stoppped. \n\n\nFor detailed instructions see \nreference documentation\n\n\nService Activation and Deactivation\n\n\nTo \nstart the Factory\n you must start also \nHTCondor\n and the \nWeb server\n beside the Factory itself:\n\n\n#\n \nFor RHEL \n6\n, CentOS \n6\n, and SL6\n\n\nroot@host #\n service condor start\n\nroot@host #\n service httpd start\n\nroot@host #\n service gwms-factory start\n\n\n#\n \n For RHEL \n7\n, CentOS \n7\n, and SL7\n\n\nroot@host #\n systemctl start condor\n\nroot@host #\n systemctl start httpd\n\nroot@host #\n systemctl start gwms-factory\n\n\n\n\n\n\n\nNote\n\n\nOnce you successfully start using the Factory service, anytime you change the \n/etc/gwms-factory/glideinWMS.xml\n file you will need to run a reconfig/reload command. If you change also some code you need the upgrade command mentioned above:\n\n\n\n\n#\n \n For RHEL \n6\n, CentOS \n6\n, and SL6\n\n\nroot@host #\n service gwms-factory reconfig\n\n\n\n#\n \n But the situation is a bit more complicated in RHEL \n7\n, CentOS \n7\n, and SL7 due to systemd restrictions\n\n\n#\n \n For reconfig:\n\n\nA. \n when the Factory is running\n\n\nA.1 \n without any additional options\n\n\nroot@host #\n /usr/sbin/gwms-factory reconfig\n\n\nor\n\n\nroot@host #\n systemctl reload gwms-factory\n\n\nA.2 \n if you want to give additional options \n\n\nsystemctl stop gwms-factory\n\n\n/usr/sbin/gwms-factory reconfig \"and your options\"\n\n\nsystemctl start gwms-factory\n\n\n\nB. \n when the Factory is NOT running \n\n\nroot@host #\n /usr/sbin/gwms-factory reconfig \n(\n\"and your options\"\n)\n\n\n\n\n\n\nTo enable the services so that they restart after a reboot:\n\n\n#\n \n# For RHEL \n6\n, CentOS \n6\n, and SL6\n\n\nroot@host #\n /sbin/chkconfig fetch-crl-cron on \n\nroot@host #\n /sbin/chkconfig fetch-crl-boot on \n\nroot@host #\n /sbin/chkconfig condor on\n\nroot@host #\n /sbin/chkconfig httpd on\n\nroot@host #\n /sbin/chkconfig gwms-factory on\n\n#\n \n# For RHEL \n7\n, CentOS \n7\n, and SL7\n\n\nroot@host #\n systemctl \nenable\n fetch-crl-cron \n\nroot@host #\n systemctl \nenable\n fetch-crl-boot\n\nroot@host #\n systemctl \nenable\n condor \n\nroot@host #\n systemctl \nenable\n httpd \n\nroot@host #\n systemctl \nenable\n gwms-factory\n\n\n\n\n\nTo stop the Factory:\n\n\n#\n \nFor RHEL \n6\n, CentOS \n6\n, and SL6 \n\n\nroot@host #\n service gwms-factory stop\n\n#\n \nFor RHEL \n7\n, CentOS \n7\n, and SL7\n\n\nroot@host #\n systemctl stop gwms-factory\n\n\n\n\n\nAnd you can stop also the other services if you are not using them independently of the Factory.\n\n\nValidating GlideinWMS Factory\n\n\nThe complete validation of the Factory is the submission of actual jobs.\n\n\nYou can also check that the services are up and running:\n\n\nroot@host #\n condor_status -any\n\n\nMyType               TargetType           Name\n\n\n\nglidefactoryclient   None                 12345_TEST_ENTRY@gfactory_instance@\n\n\nglideclient          None                 12345_TEST_ENTRY@gfactory_instance@\n\n\nglidefactory         None                 TEST_ENTRY@gfactory_instance@\n\n\nglidefactoryglobal   None                 gfactory_instance@gfactory_ser\n\n\nglideclientglobal    None                 gfactory_instance@gfactory_ser\n\n\nScheduler            None                 hostname.fnal.gov\n\n\nDaemonMaster         None                 hostname.fnal.gov\n\n\nNegotiator           None                 hostname.fnal.gov\n\n\nScheduler            None                 schedd_glideins2@hostname\n\n\nScheduler            None                 schedd_glideins3@hostname\n\n\nScheduler            None                 schedd_glideins4@hostname\n\n\nScheduler            None                 schedd_glideins5@hostname\n\n\nCollector            None                 wmscollector_service@hostname\n\n\n\n\n\n\n\n\n\n\nYou should have one \"glidefactory\" classAd for each entry that you have enabled.\nIf you have already configured the frontends, you will also have one glidefactoryclient and one glideclient classAd for each frontend / entry.\n\n\n\n\n\n\nYou can check also the monitoring Web page: \nhttp://YOUR_HOST_FQDN/factory/monitor/\n\n\n\n\n\n\nYou can also test the local submission of a job to a resource using the test script \nlocal_start.sh\n but you must first install the \nOSG client tools\n and generate a proxy. After that you can run the test (replace ENTRY_NAME with the name of one of the entries in \n/etc/gwms-factory/glideinWMS.xml\n):\n\n\n\n\n\n\nCheck Web server configuration for the monitoring\n\n\nVerify path and specially the URL for the GlideinWMS files served by your web server:\n\n\nstage\n \nbase_dir\n=\n\"/var/lib/gwms-factory/web-area/stage\"\n \nuse_symlink\n=\n\"True\"\n \nweb_base_url\n=\n\"http://HOSTNAME:PORT/factory/stage\"\n\n\n\n\n\n\nThis will determine the location of your web server\n.  Make sure that the URL is visible. Depending on your firewall or the one of your organization, you may need to change the port here and in the httpd configuration (by modifying the \"Listen\" directive in \n/etc/httpd/conf/httpd.conf\n).  Note that web servers are an often an attacked piece of infrastruture, so you may want to go through the Apache configuration in \n/etc/httpd/conf/httpd.conf\n and disable unneeded modules.\n\n\nTroubleshooting GlideinWMS Factory\n\n\nFile Locations\n\n\n\n\n\n\n\n\nFile Description\n\n\nFile Location\n\n\nComment\n\n\n\n\n\n\n\n\n\n\nConfiguration file\n\n\n/etc/gwms-factory/glideinWMS.xml\n\n\nMain configuration file\n\n\n\n\n\n\nLogs\n\n\n/var/log/gwms-factory/server/factory\n\n\nOverall server logs\n\n\n\n\n\n\n\n\n/var/log/gwms-factory/server/entry_NAME\n\n\nSpecific entry logs (generally more useful)\n\n\n\n\n\n\n\n\n/var/log/gwms-factory/client\n\n\nGlidein Pilot logs seperated by user and entry\n\n\n\n\n\n\nStartup script\n\n\n/etc/init.d/gwms-factory\n\n\n\n\n\n\n\n\nWeb Directory\n\n\n/var/lib/gwms-factory/web-area\n\n\n\n\n\n\n\n\nWeb Base\n\n\n/var/lib/gwms-factory/web-base\n\n\n\n\n\n\n\n\nWorking Directory\n\n\n/var/lib/gwms-factory/work-dir/\n\n\n\n\n\n\n\n\n\n\nIncrease the log level and change rotation policies\n\n\nYou can increase the log level of the frontend. To add a log file with all the log information add the following line with all the message types in the \nprocess_log\n section of \n/etc/gwms-factory/glideinWMS.xml\n:\n\n\n<log_retention>\n\n   \n<process_logs>\n\n       \n<process_log\n \nextension=\n\"all\"\n \nmax_days=\n\"7.0\"\n \nmax_mbytes=\n\"100.0\"\n \nmin_days=\n\"3.0\"\n \nmsg_types=\n\"DEBUG,EXCEPTION,INFO,ERROR,ERR\"\n/>\n\n\n\n\n\n\nYou can also change the rotation policy and choose whether compress the rotated files, all in the same section of the config files:\n\n\n\n\nmax_bytes is the max size of the log files\n\n\nmax_days it will be rotated.\n\n\ncompression specifies if rotated files are compressed\n\n\nbackup_count is the number of rotated log files kept\n\n\n\n\nFurther details are in the \nreference documentation\n.\n\n\nFailed authentication errors\n\n\nIf you get messages such as these in the logs, the Factory does not trust the frontend and will not submit glideins.\n\n\nWARNING: Client fermicloud128-fnal-gov_OSG_gWMSFrontend.main (secid: frontend_name) not in white list. Skipping request\n\n\n\n\n\n\nThis error means that the frontend name in the security section of the Factory does not match the \nsecurity_name\n in the frontend.\n\n\nClient fermicloud128-fnal-gov_OSG_gWMSFrontend.main (secid: frontend_name) is not coming from a trusted source;\n\n\n AuthenticatedIdentity vofrontend_condor@fermicloud130.fnal.gov!=vofrontend_factory@fermicloud130.fnal.gov. \n\n\nSkipping for security reasons.\n\n\n\n\n\n\nThis error means that the identity in the security section of the Factory does not match what the \n/etc/condor/certs/condor_mapfile\n authenticates the Frontend to in HTCondor (!Authenticated Identity in the classad).\n\n\nMake sure the attributes are correctly lined up as in the Frontend security configuration section above.\n\n\nGlideins start but do not connect to User pool / VO Frontend\n\n\nCheck the appropriate job err and out logs in \n/var/log/gwms-factory/client\n to see if any errors were reported.\nOften, this will be a pilot unable to access a web server or with an invalid proxy.  Also, verify that the \ncondor_mapfile\n is correct on the VO Frontend's user pool collector and configuration.\n\n\nGlideins start but fail before running job with error \"Proxy not long lived enough\"\n\n\nIf the glideins are running on a resource (entry) but the jobs are not running and the log files in \n/var/log/gwms-factory/client/user_frontend/glidein_gfactory_instance/ENTRY_NAME\n report an error like \"Proxy not long lived enough (86096 s left), shortened retire time ...\",  then probably the HTCondor RLM on the Compute Element is delegating the proxy and shortening its lifespan.\n\n\nThis can be fixed by setting \nDELEGATE_JOB_GSI_CREDENTIALS = FALSE\n as suggested in the \nCE install document\n.\n\n\nCondor_root_switchboard errors\n\n\nMake sure that you run \nservice gwms-factory upgrade\n instead of the more light-weight \nservice gwms-factory reconfig\n\nto ensure that all scripts are created correctly. Just make sure that gwms-factory is stopped.\n\n\nNext verify \n/etc/condor/privsep_config\n to make sure the users and groups are listed correctly.\n\n\nLastly, verify that permissions are correct.  The parent directories (all the way to the root) of all valid-dirs in the file must be owned by root.\n\n\n\n\nNote\n\n\nPlease notice that from version 3.5, Factory and all pilot jobs run under a single user (gfactory) to eliminate the need of switchboard and setuid/user-switching. Therefore, these errors will not exist in v3.5.\n\n\n\n\nReferences\n\n\n\n\nhttp://glideinwms.fnal.gov/doc.prd/\n\n\nhttps://opensciencegrid.org/docs/other/install-gwms-frontend/",
            "title": "Installing GlideinWMS Factory"
        },
        {
            "location": "/services/install-gwms-factory/#glideinwms-factory-installation",
            "text": "This document describes how to install a Glidein Workflow Managment System (GlideinWMS) Factory instance.   This document assumes expertise with HTCondor and familiarity with the GlideinWMS software.  It  does not  cover anything but the simplest possible install.   Please consult the  GlideinWMS reference documentation  for advanced topics, including non-root, non-RPM-based installation.  In this document the terms glidein and pilot (job) will be used interchangeably.   This parts covers these primary components of the GlideinWMS system:   WMS Collector / Schedd : A set of  condor_collector  and  condor_schedd  processes that allow the submission of pilots to Grid entries.    GlideinWMS Factory : The process submitting the pilots when needed    Warning  We really recommend you to  use the OSG provided Factory and not to install your own . A  VO Frontend  is sufficient to submit your jobs and to decide scheduling policies. And this will avoid for you the complexity to deal directly with grid/cloud sites. If you really need you own Factory be aware that it is a complex component and may require a non trivial maintenance effort.",
            "title": "GlideinWMS Factory Installation"
        },
        {
            "location": "/services/install-gwms-factory/#before-starting",
            "text": "Before starting the installation process, consider the following points (consulting  the Reference section below  as needed):",
            "title": "Before Starting"
        },
        {
            "location": "/services/install-gwms-factory/#requirements",
            "text": "",
            "title": "Requirements"
        },
        {
            "location": "/services/install-gwms-factory/#host-and-os",
            "text": "A host to install the  GlideinWMS Factory  (pristine node).   Currently most of our testing has been done on Scientific Linux 6 and 7.  Root access   The GlideinWMS Factory has the following requirements:   CPU : 4-8 cores for a large installation (1 should suffice on a small install)  RAM : 4-8GB on a large installation (1GB should suffice for small installs)  Disk :  10GB will be plenty sufficient for all the binaries, config and log files related to GlideinWMS.  If you are a large site with need to keep significant history and logs, you may want to allocate 100GB+ to store long histories.",
            "title": "Host and OS"
        },
        {
            "location": "/services/install-gwms-factory/#users",
            "text": "The GlideinWMS Factory installation will create the following users unless they are  already created .     User  Default uid  Comment      condor  none  HTCondor user (installed via dependencies).    gfactory  none  This user runs the GlideinWMS VO factory.     To verify that the user  gfactory  has  gfactory  as primary group check the output of   root@host #  getent passwd gfactory  |  cut -d: -f4  |  xargs getent group  It should be the  gfactory  group.    Note  If you get another one, you just need to modify the  PrivSep Kernel configuration file   /etc/condor/privsep_config  as indicated above and establish the corresponding GID. The lists that you see in this file, specify the IDs of all users and groups that HTCondor jobs may use on the given execute machine. In other words, users and groups that HTCondor will be allowed to act on behalf of.",
            "title": "Users"
        },
        {
            "location": "/services/install-gwms-factory/#certificates",
            "text": "Certificate  User that owns certificate  Path to certificate      Host certificate  root  /etc/grid-security/hostcert.pem  /etc/grid-security/hostkey.pem     Here  are instructions to request a host certificate.  The host  certificate/key  is used for authorization,  however, authorization between the Factory and the GlideinWMS collector is done by file system authentication.",
            "title": "Certificates"
        },
        {
            "location": "/services/install-gwms-factory/#networking",
            "text": "",
            "title": "Networking"
        },
        {
            "location": "/services/install-gwms-factory/#firewalls",
            "text": "It must be on the public internet, with at least one port open to the world; all worker nodes will load data from this node trough HTTP. Note that worker nodes will also need outbound access in order to access this HTTP port.",
            "title": "Firewalls"
        },
        {
            "location": "/services/install-gwms-factory/#installation-procedure",
            "text": "As with all OSG software installations, there are some one-time (per host) steps to prepare in advance:   Ensure the host has a  supported operating system  Obtain root access to the host  Prepare the  required Yum repositories  Install  CA certificates",
            "title": "Installation Procedure"
        },
        {
            "location": "/services/install-gwms-factory/#installing-htcondor",
            "text": "Most required software is installed from the Factory RPM installation. HTCondor is the only exception since there are  many different ways to install it , using the RPM system or not. \nYou need to have HTCondor installed before installing the GlideinWMS Factory. If yum cannot find a HTCondor RPM, it will install the dummy  empty-condor  RPM, assuming that you installed HTCondor using a tarball distribution.    If you don't have HTCondor already installed, you can install the HTCondor RPM from the OSG repository:  root@host #  yum install condor.x86_64    If you have installed HTCondor version > 8.7.2, you must install the  glideinwms-switchboard :  root@host #  yum install glideinwms-switchboard --enablerepo = osg-upcoming",
            "title": "Installing HTCondor"
        },
        {
            "location": "/services/install-gwms-factory/#installing-htcondor-bosco",
            "text": "If you plan to send jobs using  direct batch submission  (aka BOSCO), then you need also the  condor-bosco  package. You'll have to install the package and remove one of its files  /etc/condor/config.d/60-campus_factory.config  because it interferes with the Factory configuration.  root@host #  yum install condor-bosco root@host #  rm /etc/condor/config.d/60-campus_factory.config root@host #  touch /etc/condor/config.d/60-campus_factory.config",
            "title": "Installing HTCondor-BOSCO"
        },
        {
            "location": "/services/install-gwms-factory/#install-gwms-factory",
            "text": "",
            "title": "Install GWMS Factory"
        },
        {
            "location": "/services/install-gwms-factory/#download-and-install-the-factory-rpm",
            "text": "Install the RPM and dependencies (be prepared for a lot of dependencies).  root@host #  yum install glideinwms-factory  This will install the current production release verified and tested by OSG with default HTCondor configuration.\nThis command will install the GlideinWMS Factory, HTCondor, the OSG client, and all the required dependencies.  If you wish to install a different version of GlideinWMS, add the \"--enablerepo\" argument to the command as follows:   yum install --enablerepo=osg-testing glideinwms-factory : The most recent production release, still in testing phase.  This will usually match the current tarball version on the  GlideinWMS home page .\n    (The osg-release production version may lag behind the tarball release by a few weeks as it is verified and packaged by OSG).  Note that this will also take the osg-testing versions of all dependencies as well.  yum install --enablerepo=osg-upcoming glideinwms-factory :  The most recent development series release, ie version 3.3.x release.  This has newer features such as cloud submission support, but is less tested.",
            "title": "Download and install the Factory RPM"
        },
        {
            "location": "/services/install-gwms-factory/#download-htcondor-tarballs",
            "text": "You will need to download HTCondor tarballs for each architecture that  you want to deploy pilots on .\nAt this point, GlideinWMS factory does not support pulling HTCondor binaries from your system area.\nSuggested is that you put these binaries in  /var/lib/gwms-factory/condor  but any  gfactory  accessible location should suffice.",
            "title": "Download HTCondor tarballs"
        },
        {
            "location": "/services/install-gwms-factory/#configuration-procedure",
            "text": "After installing the RPM you need to configure the components of the GlideinWMS Factory:   Edit Factory configuration options  Edit HTCondor configuration options  Create a HTCondor grid map file  Reconfigure and Start Factory",
            "title": "Configuration Procedure"
        },
        {
            "location": "/services/install-gwms-factory/#configuring-the-factory",
            "text": "The configuration file is  /etc/gwms-factory/glideinWMS.xml .  The next steps will describe each line that you will need to edit for most cases, but you may want to review the whole file to be sure that it is configured correctly.",
            "title": "Configuring the Factory"
        },
        {
            "location": "/services/install-gwms-factory/#security-configuration",
            "text": "In the security section, you will need to provide  each Frontend  that is allowed to communicate with the Factory:  security key_length=\"2048\" pub_key=\"RSA\" remove_old_cred_age=\"30\" remove_old_cred_freq=\"24\" reuse_oldkey_onstartup_gracetime=\"900\">\n   <frontends> \n      <frontend   identity= \"vofrontend_service@FACTORY_COLLECTOR_HOSTNAME\"   name= \" ORANGE vofrontend_sec_name \" > \n         <security_classes> \n            <security_class   name= \" frontend_sec_class \"   username= \"frontend\"   /> \n         </security_classes> \n      </frontend> \n   </frontends>  </security>   These attributes are very important to get exactly right or the Frontend will not be trusted.  This should match one of the  factory  and  security  sections of the Frontend configuration  Configuring the GlideinWMS Frontend  in the following way:   Note  This is a snippet from the Frontend configuration (for reference), not the Factory that you are configuring now!   For the factory section:  # from frontend.xml <factory   query_expr= '((stringListMember(\"VO\", GLIDEIN_Supported_VOs)))' > \n....\n    <collectors> \n       <collector   DN= \"/DC=org/DC=doegrids/OU=Services/CN=FACTORY_COLLECTOR_HOSTNAME\"  \n       comment= \"Define factory collector globally for simplicity\"             \n       factory_identity= \"gfactory@FACTORY_COLLECTOR_HOSTNAME\"  \n       my_identity= \" username @FACTORY_COLLECTOR_HOSTNAME\"  \n       node= \"FACTORY_COLLECTOR_HOSTNAME\" /> \n    </collectors>  </factory>   For the security:  # from frontend.xml <security   classad_proxy= \"/tmp/vo_proxy\"   proxy_DN= \"DN of vo_proxy\" \n       proxy_selection_plugin= \"ProxyAll\" \n       security_name= \"The security name, this is used by factory\" \n       sym_key= \"aes_256_cbc\" > \n       <credentials> \n          <credential   absfname= \"/tmp/pilot_proxy\"   security_class= \"frontend\" \n          trust_domain= \"OSG\"   type= \"grid_proxy\" /> \n       </credentials>  </security>           Note that the identity of the Frontend must match what HTCondor authenticates the DN of the frontend to.  In  /etc/condor/certs/condor_mapfile , there must be an entry with  vofrontend_service  definition (in this case):  GSI   \"^\\/DC\\=org\\/DC\\=doegrids\\/OU\\=Services\\/CN\\=Some\\ Name\\ 834323 $\"   % GREEN % vofrontend_service % ENDCOLOR %",
            "title": "Security configuration"
        },
        {
            "location": "/services/install-gwms-factory/#entry-configuration",
            "text": "Entries are grid/cloud endpoints (aka Compute Elements, or gatekeepers) that can accept job requests and run pilots (which will run user jobs).\nEach entry needs to be configured to communicate to a specific gatekeeper.  An example test entry is provided in the default GlideinWMS configuration file.  At the very least, you will need to modify the entry line:  <entry   name= \" ENTRY_NAME \"   enabled= \"True\"   auth_method= \"grid_proxy\"   trust_domain= \"OSG\"   gatekeeper= \" gatekeeper.domain.tld/jobmanager-type \"   gridtype= \"gt2\"   rsl= \"(queue=default)(jobtype=single)\"  schedd_name= \" schedd_glideins2@FACTORY_HOSTNAME \"   verbosity= \"std\"   work_dir= \"OSG\" >   You will need to modify the entry  name  and  gatekeeper .  This will determine the gatekeeper that you access.  Specific gatekeepers often require specific \"rsl\" attributes that determine the job queue that you are submitting to, or other attributes.  Add them in the  rsl  attribute.    Also, be sure to distribute your entries across the various HTCondor schedd work managers to balance load.  To see the available schedd use  condor_status -schedd -l | grep Name .    Several schedd options are configured by default for you:   schedd_glideins2, schedd_glideins3, schedd_glideins4, schedd_glideins5 , as well as the default  schedd . This can be modified in the HTCondor configuration.  Add any specific options, such as limitations on jobs/pilots or glexec/voms requirements in the entry section below the above line. More details are in the  GlideinWMS Factory configuration guide .  !!! warning\n      If there is no match between  auth_metod  and  trust_domain  of the entry and the  type  and  trust_domain  listed in one of the  credentials of one of the Frontends  using this Factory, then no job can run on that entry.  The Factory must advertise the correct Resource Name of each entry for accounting purposes. Then the Factory must also advertise in the entry all the attributes that will allow to match the query expression used in the Frontends connecting to this Factory (e.g.  <factory query_expr='((stringListMember(\" PINK VO \", GLIDEIN_Supported_VOs)))'>  as explained in the  VO frontend configuration document  ).    Note  Keep an eye on this part as we're dealing with singularity.\nThen you must advertise correctly if the site supports  gLExec .\nIf it does not set  GLEXEC_BIN  to  NONE , if  gLExec  is installed via OSG set it to  OSG , \notherwise set it to the path of gLExec.   For example this snippet advertises  GLIDEIN_Supported_VOs  attribute with the supported VO so that can be used with the query above in the VO frontend and says that the resource does not support gLExec:  <entry   name= \"RESOURCE_NAME\"   ... \n    <config > \n   ...\n       <attrs> \n      ...\n          <attr   name= \"GLIDEIN_Supported_VOs\"   const= \"True\"   glidein_publish= \"True\"   job_publish= \"True\"  \n          parameter= \"True\"   publish= \"True\"   type= \"string\"   value= \" PINK VO \" /> \n          <attr   name= \"GLEXEC_BIN\"   const= \"True\"   glidein_publish= \"False\"   job_publish= \"False\"   parameter= \"True\"  \n          publish= \"True\"   type= \"string\"   value= \" NONE \" /> \n          <attr   name= \"GLIDEIN_Resource_Name\"   const= \"True\"   glidein_publish= \"True\"   job_publish= \"True\"  \n        parameter= \"True\"   publish= \"True\"   type= \"string\"   value= \" SiteNameFromOIM \" /> \n      </attrs>    Note  Specially if jobs are sent to OSG resources, it is very important to set the GLIDEIN_Resource_Name and to be consistent with the Resource Name reported in OIM because that name will be used for job accounting in Gratia. It should be the name of the Resource in OIM or the name of the Resource Group (specially if there are many gatekeepers submitting to the same cluster).  More information on options can be found  here",
            "title": "Entry configuration"
        },
        {
            "location": "/services/install-gwms-factory/#configuring-tarballs",
            "text": "Each pilot will download HTCondor binaries from the staging area.  Often, multiple binaries are needed to support various architectures and platforms. \nCurrently, you will need to provide at least one tarball for GlideinWMS to use.  (Using the system binaries is currently not supported).  Download a HTCondor tarball from  here .  Suggested is to put the binaries in  /var/lib/gwms-factory/condor , but any factory-accessible location will do just fine.  Once you have downloaded the tarball, configure it in  /etc/gwms-factory/glideinWMS.xml  like in the following:  <condor_tarball   arch= \"default\"   base_dir= \"/var/lib/gwms-factory/condor/condor-8.7.6-x86_64_RedHat6-stripped\"   os= \"rhel6\"   version= \"default\" />   Remember also to modify the  condor_os  and  condor_arch  attributes in the entries (the configured Compute Elements) to pick the correct HTCondor binary. Here  are more details on using multiple HTCondor binaries. Note that is sufficient to set the  base_dir ; the reconfigure command will prepare the tarball and add it to the XML config file.",
            "title": "Configuring Tarballs"
        },
        {
            "location": "/services/install-gwms-factory/#configuring-htcondor",
            "text": "The HTCondor configuration for the Factory is placed in  /etc/condor/config.d .   00_gwms_factory_general.config  00-restart_peaceful.config  01_gwms_factory_collectors.config  02_gwms_factory_schedds.config  03_gwms_local.config  10-batch_gahp_blahp.config   Get rid of the pre-loaded HTCondor default  root@host #  rm /etc/condor/config.d/00personal_condor.config root@host #  touch /etc/condor/config.d/00personal_condor.config  For most installations, the items you need to modify are in  03_gwms_factory_local.config .  The lines you will have to edit are:   Credentials of the machine.  You can either run using a proxy, or a service certificate.  It is recommended to use a host certificate and specify its location in the variables  GSI_DAEMON_CERT  and  GSI_DAEMON_KEY .  The host certificate should be owned by  root  and have the correct permissions, 600.  HTCondor ids in the form UID.GID (both are integers)  HTCondor admin email. Will receive messages when services fail.   # -- HTCondor user: condor  CONDOR_IDS   =  # --  Contact (via email) when problems occur  CONDOR_ADMIN   =  ############################  #   GSI   Security   config  ############################  # -- Grid Certificate directory  GSI_DAEMON_TRUSTED_CA_DIR =   / etc / grid - security / certificates  # -- Credentials  GSI_DAEMON_CERT   =    / etc / grid - security / hostcert . pem  GSI_DAEMON_KEY    =    / etc / grid - security / hostkey . pem  # -- HTCondor mapfile  CERTIFICATE_MAPFILE =   / etc / condor / certs / condor_mapfile  ###################################  #   Whitelist   of   HTCondor   daemon   DNs  ###################################  # DAEMON_LIST   =   COLLECTOR ,   MASTER ,   NEGOTIATOR ,   SCHEDD ,   STARTD",
            "title": "Configuring HTCondor"
        },
        {
            "location": "/services/install-gwms-factory/#using-other-htcondor-rpms-eg-uw-madison-htcondor-rpm",
            "text": "The above procedure will work if you are using the OSG HTCondor RPMS. You can verify that you used the OSG HTCondor RPM by using  yum list condor . The version name should include \"osg\", e.g.  8.6.9-1.1.osg34.el7 .  If you are using the UW Madison HTCondor RPMS, be aware of the following changes:   This HTCondor RPM uses a file  /etc/condor/condor_config.local  to add your local machine slot to the user pool.  If you want to disable this behavior (recommended), you should blank out that file or comment out the line in  /etc/condor/condor_config  for LOCAL_CONFIG_FILE. (Make sure that LOCAL_CONFIG_DIR is set to  /etc/condor/config.d )  Note that the variable LOCAL_DIR is set differently in UW Madison and OSG RPMs. This should not cause any more problems in the Glideinwms RPMs, but please take note if you use this variable in your job submissions or other customizations.   In general if you are using a non OSG RPM or if you added custom configuration files for HTCondor please check the order of the configuration files:  root@host #  condor_config_val -config Configuration source:      /etc/condor/condor_config  Local configuration sources:      /etc/condor/config.d/00-restart_peaceful.config      /etc/condor/config.d/00_gwms_factory_general.config      /etc/condor/config.d/01_gwms_factory_collectors.config      /etc/condor/config.d/02_gwms_factory_schedds.config      /etc/condor/config.d/03_gwms_local.config      /etc/condor/config.d/10-batch_gahp_blahp.config      /etc/condor/condor_config.local",
            "title": "Using other HTCondor RPMs, e.g. UW Madison HTCondor RPM"
        },
        {
            "location": "/services/install-gwms-factory/#restarting-htcondor",
            "text": "After configuring HTCondor, be sure to restart HTCondor:  root@host #  service condor restart",
            "title": "Restarting HTCondor"
        },
        {
            "location": "/services/install-gwms-factory/#configuring-htcondor-privilege-separation",
            "text": "Lastly, verify the settings in  /etc/condor/privsep_config .  By default,\nthe values in this file should not need to be modified.  However,\nif your user/group differs from  gfactory  /  gfactory , or if you are operating\na Factory with multiple frontends, you will have to modify this file.  This file controls the HTCondor root switchboard, which allows the Factory\nto change permissions of files, specifically the proxies and files\npassed to it by the frontends.  valid - caller - uids   =   gfactory  valid - caller - gids   =   gfactory  valid - target - uids   =   fecmsucsd   :   fehcc   :   fecmscern  valid - target - gids   =   fecmsucsd   :   fehcc   :   fecmscern  valid - dirs   =   / var / lib / gwms - factory / client - proxies  valid - dirs   =   / var / lib / gwms - factory / client - logs  valid - dirs   =   / var / log / gwms - factory / client  valid - dirs   =   / var / lib / gwms - factory  valid - dirs   =   / var / log / gwms - factory  procd - executable   =   / usr / sbin / condor_procd   Thus, the  valid-called-uids  and  valid-caller-gids  should match the user/group of your Factory user.\nThe  valid-target-uids  and  valid-target-gids  should be a colon-separated list of all the frontend\nusers and groups.  This should match the security_classes section in  /etc/gwms-factory/glideinWMS.xml .   Note  Please notice that from version 3.5, there is no need to configure HTCondor privilege separation. The Factory and all pilot jobs run under a single user (gfactory), which eliminates the need for the switchboard and setuid/user-switching.",
            "title": "Configuring HTCondor Privilege Separation"
        },
        {
            "location": "/services/install-gwms-factory/#create-a-htcondor-grid-mapfile",
            "text": "The HTCondor grid mapfile  /etc/condor/certs/condor_mapfile  is used for authentication between the glidein running on a remote worker node, and the local collector.  HTCondor uses the mapfile to map certificates to pseudo-users on the local machine.  It is important that you map the DN's of each frontend you are talking to.  Below is an example mapfile, by default found in  /etc/condor/certs/condor_mapfile :  GSI   \"^\\/DC\\=org\\/DC\\=doegrids\\/OU\\=People\\/CN\\=Some\\ Name\\ 123456$\"   frontend  GSI   (. * )   anonymous  FS   (. * )   \\ 1    Each frontend needs a line that maps to the user specified in the identity argument in the frontend security section of the Factory configuration.",
            "title": "Create a HTCondor grid mapfile."
        },
        {
            "location": "/services/install-gwms-factory/#reconfiguring-glideinwms",
            "text": "After changing the configuration of GlideinWMS and making sure that Factory is running, use the following table to find the appropriate command for your operating system (run as  root ):     If your operating system is...  Run the following command...      Enterprise Linux 7  systemctl reload gwms-factory    Enterprise Linux 6  service gwms-factory reconfig",
            "title": "Reconfiguring GlideinWMS"
        },
        {
            "location": "/services/install-gwms-factory/#upgrading-glideinwms",
            "text": "Before you start the Factory service for the first time or after an update of the RPM or after you change GlideinWMS scripts, you should always use the GlideinWMS \"upgrade\" command. To do so:    Make sure the  condor  and  gwms-factory  services are stopped (in EL6 this will be done for you).    Issue the upgrade command:    If you are using Enterprise Linux 7:  root@host #  /usr/sbin/gwms-factory upgrade    If you are using Enterprise Linux 6:  root@host #  service gwms-factory upgrade    Start the  condor  and  gwms-factory  services (see next part).    Important:  If you are upgrading to v3.5.x from 3.4.x or earlier you need some additional steps. The Factory and all pilot jobs will run under a single user (gfactory) to eliminate the need of switchboard and setuid/user-switching, no more separated users per-VO). After the RPM upgrade, you will need to::  1.   Stop   Factory   and   HTCondor  2.   Migrate   the   HTCondor   status   running   the   fact_chown   script ,   located   in   factory / tools .   Add   the   flag   --backup to have backup of everything:           :: : console           root @host   #   sudo   fact_chown   --user=gfactory --group=gfactory --backup  3.   Restart   HTCondor   and   the   Factory .   To revert to a version of GlideinWMS lower than 3.5, you need to restore the job_queue files and change back the permissions of the log directories. Those operations need to be also performed with both HTCondor and Factory stoppped.   For detailed instructions see  reference documentation",
            "title": "Upgrading GlideinWMS"
        },
        {
            "location": "/services/install-gwms-factory/#service-activation-and-deactivation",
            "text": "To  start the Factory  you must start also  HTCondor  and the  Web server  beside the Factory itself:  #   For RHEL  6 , CentOS  6 , and SL6  root@host #  service condor start root@host #  service httpd start root@host #  service gwms-factory start #    For RHEL  7 , CentOS  7 , and SL7  root@host #  systemctl start condor root@host #  systemctl start httpd root@host #  systemctl start gwms-factory   Note  Once you successfully start using the Factory service, anytime you change the  /etc/gwms-factory/glideinWMS.xml  file you will need to run a reconfig/reload command. If you change also some code you need the upgrade command mentioned above:   #    For RHEL  6 , CentOS  6 , and SL6  root@host #  service gwms-factory reconfig #    But the situation is a bit more complicated in RHEL  7 , CentOS  7 , and SL7 due to systemd restrictions  #    For reconfig:  A.   when the Factory is running  A.1   without any additional options  root@host #  /usr/sbin/gwms-factory reconfig  or  root@host #  systemctl reload gwms-factory A.2   if you want to give additional options   systemctl stop gwms-factory  /usr/sbin/gwms-factory reconfig \"and your options\"  systemctl start gwms-factory  B.   when the Factory is NOT running   root@host #  /usr/sbin/gwms-factory reconfig  ( \"and your options\" )   To enable the services so that they restart after a reboot:  #   # For RHEL  6 , CentOS  6 , and SL6  root@host #  /sbin/chkconfig fetch-crl-cron on  root@host #  /sbin/chkconfig fetch-crl-boot on  root@host #  /sbin/chkconfig condor on root@host #  /sbin/chkconfig httpd on root@host #  /sbin/chkconfig gwms-factory on #   # For RHEL  7 , CentOS  7 , and SL7  root@host #  systemctl  enable  fetch-crl-cron  root@host #  systemctl  enable  fetch-crl-boot root@host #  systemctl  enable  condor  root@host #  systemctl  enable  httpd  root@host #  systemctl  enable  gwms-factory  To stop the Factory:  #   For RHEL  6 , CentOS  6 , and SL6   root@host #  service gwms-factory stop #   For RHEL  7 , CentOS  7 , and SL7  root@host #  systemctl stop gwms-factory  And you can stop also the other services if you are not using them independently of the Factory.",
            "title": "Service Activation and Deactivation"
        },
        {
            "location": "/services/install-gwms-factory/#validating-glideinwms-factory",
            "text": "The complete validation of the Factory is the submission of actual jobs.  You can also check that the services are up and running:  root@host #  condor_status -any MyType               TargetType           Name  glidefactoryclient   None                 12345_TEST_ENTRY@gfactory_instance@  glideclient          None                 12345_TEST_ENTRY@gfactory_instance@  glidefactory         None                 TEST_ENTRY@gfactory_instance@  glidefactoryglobal   None                 gfactory_instance@gfactory_ser  glideclientglobal    None                 gfactory_instance@gfactory_ser  Scheduler            None                 hostname.fnal.gov  DaemonMaster         None                 hostname.fnal.gov  Negotiator           None                 hostname.fnal.gov  Scheduler            None                 schedd_glideins2@hostname  Scheduler            None                 schedd_glideins3@hostname  Scheduler            None                 schedd_glideins4@hostname  Scheduler            None                 schedd_glideins5@hostname  Collector            None                 wmscollector_service@hostname     You should have one \"glidefactory\" classAd for each entry that you have enabled.\nIf you have already configured the frontends, you will also have one glidefactoryclient and one glideclient classAd for each frontend / entry.    You can check also the monitoring Web page:  http://YOUR_HOST_FQDN/factory/monitor/    You can also test the local submission of a job to a resource using the test script  local_start.sh  but you must first install the  OSG client tools  and generate a proxy. After that you can run the test (replace ENTRY_NAME with the name of one of the entries in  /etc/gwms-factory/glideinWMS.xml ):",
            "title": "Validating GlideinWMS Factory"
        },
        {
            "location": "/services/install-gwms-factory/#check-web-server-configuration-for-the-monitoring",
            "text": "Verify path and specially the URL for the GlideinWMS files served by your web server:  stage   base_dir = \"/var/lib/gwms-factory/web-area/stage\"   use_symlink = \"True\"   web_base_url = \"http://HOSTNAME:PORT/factory/stage\"   This will determine the location of your web server .  Make sure that the URL is visible. Depending on your firewall or the one of your organization, you may need to change the port here and in the httpd configuration (by modifying the \"Listen\" directive in  /etc/httpd/conf/httpd.conf ).  Note that web servers are an often an attacked piece of infrastruture, so you may want to go through the Apache configuration in  /etc/httpd/conf/httpd.conf  and disable unneeded modules.",
            "title": "Check Web server configuration for the monitoring"
        },
        {
            "location": "/services/install-gwms-factory/#troubleshooting-glideinwms-factory",
            "text": "",
            "title": "Troubleshooting GlideinWMS Factory"
        },
        {
            "location": "/services/install-gwms-factory/#file-locations",
            "text": "File Description  File Location  Comment      Configuration file  /etc/gwms-factory/glideinWMS.xml  Main configuration file    Logs  /var/log/gwms-factory/server/factory  Overall server logs     /var/log/gwms-factory/server/entry_NAME  Specific entry logs (generally more useful)     /var/log/gwms-factory/client  Glidein Pilot logs seperated by user and entry    Startup script  /etc/init.d/gwms-factory     Web Directory  /var/lib/gwms-factory/web-area     Web Base  /var/lib/gwms-factory/web-base     Working Directory  /var/lib/gwms-factory/work-dir/",
            "title": "File Locations"
        },
        {
            "location": "/services/install-gwms-factory/#increase-the-log-level-and-change-rotation-policies",
            "text": "You can increase the log level of the frontend. To add a log file with all the log information add the following line with all the message types in the  process_log  section of  /etc/gwms-factory/glideinWMS.xml :  <log_retention> \n    <process_logs> \n        <process_log   extension= \"all\"   max_days= \"7.0\"   max_mbytes= \"100.0\"   min_days= \"3.0\"   msg_types= \"DEBUG,EXCEPTION,INFO,ERROR,ERR\" />   You can also change the rotation policy and choose whether compress the rotated files, all in the same section of the config files:   max_bytes is the max size of the log files  max_days it will be rotated.  compression specifies if rotated files are compressed  backup_count is the number of rotated log files kept   Further details are in the  reference documentation .",
            "title": "Increase the log level and change rotation policies"
        },
        {
            "location": "/services/install-gwms-factory/#failed-authentication-errors",
            "text": "If you get messages such as these in the logs, the Factory does not trust the frontend and will not submit glideins.  WARNING: Client fermicloud128-fnal-gov_OSG_gWMSFrontend.main (secid: frontend_name) not in white list. Skipping request   This error means that the frontend name in the security section of the Factory does not match the  security_name  in the frontend.  Client fermicloud128-fnal-gov_OSG_gWMSFrontend.main (secid: frontend_name) is not coming from a trusted source;   AuthenticatedIdentity vofrontend_condor@fermicloud130.fnal.gov!=vofrontend_factory@fermicloud130.fnal.gov.   Skipping for security reasons.   This error means that the identity in the security section of the Factory does not match what the  /etc/condor/certs/condor_mapfile  authenticates the Frontend to in HTCondor (!Authenticated Identity in the classad).  Make sure the attributes are correctly lined up as in the Frontend security configuration section above.",
            "title": "Failed authentication errors"
        },
        {
            "location": "/services/install-gwms-factory/#glideins-start-but-do-not-connect-to-user-pool-vo-frontend",
            "text": "Check the appropriate job err and out logs in  /var/log/gwms-factory/client  to see if any errors were reported.\nOften, this will be a pilot unable to access a web server or with an invalid proxy.  Also, verify that the  condor_mapfile  is correct on the VO Frontend's user pool collector and configuration.",
            "title": "Glideins start but do not connect to User pool / VO Frontend"
        },
        {
            "location": "/services/install-gwms-factory/#glideins-start-but-fail-before-running-job-with-error-proxy-not-long-lived-enough",
            "text": "If the glideins are running on a resource (entry) but the jobs are not running and the log files in  /var/log/gwms-factory/client/user_frontend/glidein_gfactory_instance/ENTRY_NAME  report an error like \"Proxy not long lived enough (86096 s left), shortened retire time ...\",  then probably the HTCondor RLM on the Compute Element is delegating the proxy and shortening its lifespan.  This can be fixed by setting  DELEGATE_JOB_GSI_CREDENTIALS = FALSE  as suggested in the  CE install document .",
            "title": "Glideins start but fail before running job with error \"Proxy not long lived enough\""
        },
        {
            "location": "/services/install-gwms-factory/#condor_root_switchboard-errors",
            "text": "Make sure that you run  service gwms-factory upgrade  instead of the more light-weight  service gwms-factory reconfig \nto ensure that all scripts are created correctly. Just make sure that gwms-factory is stopped.  Next verify  /etc/condor/privsep_config  to make sure the users and groups are listed correctly.  Lastly, verify that permissions are correct.  The parent directories (all the way to the root) of all valid-dirs in the file must be owned by root.   Note  Please notice that from version 3.5, Factory and all pilot jobs run under a single user (gfactory) to eliminate the need of switchboard and setuid/user-switching. Therefore, these errors will not exist in v3.5.",
            "title": "Condor_root_switchboard errors"
        },
        {
            "location": "/services/install-gwms-factory/#references",
            "text": "http://glideinwms.fnal.gov/doc.prd/  https://opensciencegrid.org/docs/other/install-gwms-frontend/",
            "title": "References"
        },
        {
            "location": "/services/topology/",
            "text": "Topology Service\n\n\nThis document contains information about the service that runs:\n\n\n\n\nhttps://topology.opensciencegrid.org\n\n\nhttps://topology-itb.opensciencegrid.org\n\n\nhttps://my.opensciencegrid.org\n\n\nhttps://my-itb.opensciencegrid.org\n\n\nhttps://myosg.opensciencegrid.org\n\n\nhttps://map.opensciencegrid.org\n: Generates the topology map used on \nOSG Display\n\n\n\n\nThe source code for the service is in \nhttps://github.com/opensciencegrid/topology\n, in the \nsrc/\n subdirectory.\nThis repository also contains the public part of the data that gets served.\n\n\nDeployment\n\n\nTopology is a webapp run with Apache on the host \ntopology.opensciencegrid.org\n.\nThe ITB instance runs on the host \ntopology-itb.opensciencegrid.org\n.\nThe hosts are VMs at Nebraska;\nfor SSH access, contact Derek Weitzel or Brian Bockelman.\n\n\nInstallation\n\n\nThese instructions assume an EL 7 host with the EPEL repositories available.\nThe software will be installed into \n/opt/topology\n.\nA second instance for the webhook app will be installed into \n/opt/topology-webhook\n.\n(The ITB instance should be installed into \n/opt/topology-itb\n and \n/opt/topology-itb-webhook\n instead.)\nThe following steps should be done as root.\n\n\n\n\n\n\nInstall prerequisites:\n\n\n#\n yum install python36 gridsite httpd mod_ssl\n\n\n\n\n\n\n\n\n\nClone the repository:\n\n\nFor the production topology host:\n\n\n#\n git clone https://github.com/opensciencegrid/topology /opt/topology\n\n#\n git clone https://github.com/opensciencegrid/topology /opt/topology-webhook\n\n\n\n\n\nFor the topology-itb host:\n\n\n#\n git clone https://github.com/opensciencegrid/topology /opt/topology-itb\n\n#\n git clone https://github.com/opensciencegrid/topology /opt/topology-itb-webhook\n\n\n\n\n\n\n\n\n\nSet up the virtualenv in the clone -- from \n/opt/topology\n or \n/opt/topology-itb\n:\n\n\n#\n python36 -m venv venv\n\n#\n . ./venv/bin/activate\n\n#\n pip install -r requirements-apache.txt\n\n\n\n\n\n\n\n\n\nRepeat for the webhook instance -- from \n/opt/topology-webhook\n or \n/opt/topology-itb-webhook\n.\n\n\n\n\n\n\nFile system locations\n\n\nThe following files/directories must exist and have the proper permissions:\n\n\n\n\n\n\n\n\nLocation\n\n\nPurpose\n\n\nOwnership\n\n\nMode\n\n\n\n\n\n\n\n\n\n\n/opt/topology\n\n\nProduction software install\n\n\nroot:root\n\n\n0755\n\n\n\n\n\n\n/opt/topology-itb\n\n\nITB software install\n\n\nroot:root\n\n\n0755\n\n\n\n\n\n\n/opt/topology-webhook\n\n\nProduction webhook software install\n\n\nroot:root\n\n\n0755\n\n\n\n\n\n\n/opt/topology-itb-webhook\n\n\nITB webhook software install\n\n\nroot:root\n\n\n0755\n\n\n\n\n\n\n/etc/opt/topology/config-production.py\n\n\nProduction config\n\n\nroot:root\n\n\n0644\n\n\n\n\n\n\n/etc/opt/topology/config-itb.py\n\n\nITB config\n\n\nroot:root\n\n\n0644\n\n\n\n\n\n\n/etc/opt/topology/bitbucket\n\n\nPrivate key for contact info repo\n\n\napache:root\n\n\n0600\n\n\n\n\n\n\n/etc/opt/topology/bitbucket.pub\n\n\nPublic key for contact info repo\n\n\napache:root\n\n\n0644\n\n\n\n\n\n\n/etc/opt/topology/github\n\n\nPrivate key for pushing automerge commits\n\n\ntopomerge:root\n\n\n0600\n\n\n\n\n\n\n/etc/opt/topology/github.pub\n\n\nPublic key for pushing automerge commits\n\n\ntopomerge:root\n\n\n0644\n\n\n\n\n\n\n/etc/opt/topology/github_webhook_secret\n\n\nGitHub webhook secret for validating webhooks\n\n\ntopomerge:root\n\n\n0600\n\n\n\n\n\n\n~apache/.ssh\n\n\nSSH dir for Apache\n\n\napache:root\n\n\n0700\n\n\n\n\n\n\n~apache/.ssh/known_hosts\n\n\nKnown hosts file for Apache\n\n\napache:root\n\n\n0644\n\n\n\n\n\n\n~topomerge\n\n\nHome dir for \ntopomerge\n Apache user\n\n\ntopomerge:root\n\n\n0755\n\n\n\n\n\n\n~topomerge/.ssh\n\n\nSSH dir for \ntopomerge\n Apache user\n\n\ntopomerge:root\n\n\n0700\n\n\n\n\n\n\n~topomerge/.ssh/known_hosts\n\n\nKnown hosts file for \ntopomerge\n Apache user\n\n\ntopomerge:root\n\n\n0644\n\n\n\n\n\n\n/var/cache/topology\n\n\nCheckouts of topology and contacts data for production instance\n\n\napache:apache\n\n\n0755\n\n\n\n\n\n\n/var/cache/topology-itb\n\n\nCheckouts of topology and contacts data for ITB instance\n\n\napache:apache\n\n\n0755\n\n\n\n\n\n\n/var/cache/topology-webhook\n\n\nTopology repo and state info for production webhook instance\n\n\ntopomerge:topomerge\n\n\n0755\n\n\n\n\n\n\n/var/cache/topology-itb-webhook\n\n\nTopology repo and state info for ITB webhook instance\n\n\ntopomerge:topomerge\n\n\n0755\n\n\n\n\n\n\n\n\n~apache/.ssh/known_hosts\n must contain an entry for \nbitbucket.org\n;\nuse \nssh-keyscan bitbucket.org\n to get the appropriate entry.\n\n\n~topomerge/.ssh/known_hosts\n must contain an entry for \ngithub.com\n;\nuse \nssh-keyscan github.com\n to get the appropriate entry.\n\n\nSoftware configuration\n\n\nConfiguration for the main app is under \n/etc/opt/topology/\n, in \nconfig-production.py\n and \nconfig-itb.py\n.\nThe webhook app configuration is in \nconfig-production-webhook.py\n and \nconfig-itb-webhook.py\n.\nThe files are in Python format and override default settings in \nsrc/webapp/default_config.py\n in the topology repo.\n\n\nHTTPD configuration is in \n/etc/httpd\n; we use the modules \nmod_ssl\n, \nmod_gridsite\n, and \nmod_wsgi\n.\nThe first two are installed via yum;\nthe .so file for mod_wsgi is located in \n/opt/topology/venv/lib/python3.6/site-packages/mod_wsgi/server/\n\nor \n/opt/topology-itb/venv/lib/python3.6/site-packages/mod_wsgi/server/\n for the ITB instance.\n\n\nEach of the hostnames are VHosts in the apache configuration.  Some special notes:\n\n\n\n\nhttps://map.opensciencegrid.org\n runs in the same wsgi process as the production topology, but the URL is limited to only the map code.  Further, it does not use mod_gridsite so that users are not asked to present a client certificate.\n\n\nVHosts are configured:\nServerName topology.opensciencegrid.org\n\n\nServerAlias my.opensciencegrid.org myosg.opensciencegrid.org\n\n\n\n\n\n\n\n\n\n\nData configuration\n\n\nConfiguration is in \n/etc/opt/topology/config-production.py\n and \nconfig-itb.py\n;\nand \nconfig-production-webhook.py\n and \nconfig-itb-webhook.py\n.\n\n\n\n\n\n\n\n\nVariable\n\n\nPurpose\n\n\n\n\n\n\n\n\n\n\nTOPOLOGY_DATA_DIR\n\n\nThe directory containing a clone of the \ntopology\n repository for data use\n\n\n\n\n\n\nTOPOLOGY_DATA_REPO\n\n\nThe remote tracking repository of \nTOPOLOGY_DATA_DIR\n\n\n\n\n\n\nTOPOLOGY_DATA_BRANCH\n\n\nThe remote tracking branch of \nTOPOLOGY_DATA_DIR\n\n\n\n\n\n\nWEBHOOK_DATA_DIR\n\n\nThe directory containing a mirror-clone of the \ntopology\n repository for webhook use\n\n\n\n\n\n\nWEBHOOK_DATA_REPO\n\n\nThe remote tracking repository of \nWEBHOOK_DATA_DIR\n\n\n\n\n\n\nWEBHOOK_DATA_BRANCH\n\n\nThe remote tracking branch of \nWEBHOOK_DATA_DIR\n\n\n\n\n\n\nWEBHOOK_STATE_DIR\n\n\nDirectory containing webhook state information between pull request and status hooks\n\n\n\n\n\n\nWEBHOOK_SECRET_KEY\n\n\nSecret key configured on GitHub for webhook delivery\n\n\n\n\n\n\nCONTACT_DATA_DIR\n\n\nThe directory containing a clone of the \ncontact\n repository for data use\n\n\n\n\n\n\nCONTACT_DATA_REPO\n\n\nThe remote tracking repository of \nCONTACT_DATA_DIR\n(default: \n\"git@bitbucket.org:opensciencegrid/contact.git\"\n)\n\n\n\n\n\n\nCONTACT_DATA_BRANCH\n\n\nThe remote tracking branch of \nCONTACT_DATA_BRANCH\n(default: \n\"master\"\n)\n\n\n\n\n\n\nCACHE_LIFETIME\n\n\nFrequency of automatic data updates in seconds\n(default: \n900\n)\n\n\n\n\n\n\nGIT_SSH_KEY\n\n\nLocation of ssh public key file for git access.\n\n\n\n\n\n\n/etc/opt/topology/bitbucket.pub\n for the main app, and \n/etc/opt/topology/github.pub\n for the webhook app\n\n\n\n\n\n\n\n\n\n\nPuppet ensures that the production \ncontact\n and \ntopology\n clones are up to date with their configured remote tracking\nrepo and branch.\nPuppet does not manage the ITB data directories so they need to be updated by hand during testing.\n\n\nGitHub Configuration for Webhook App\n\n\n\n\n\n\nGo to the \nwebhook settings\n page on GitHub.\n     There are four webhooks to set up; \npull_request\n and \nstatus\n for both the topology and topology-itb hosts.\n\n\n\n\n\n\n\n\nPayload URL\n\n\nContent type\n\n\nEvents to trigger webhook\n\n\n\n\n\n\n\n\n\n\nhttps://topology.opensciencegrid.org/webhook/status\n\n\napplication/json\n\n\nStatuses\n\n\n\n\n\n\nhttps://topology.opensciencegrid.org/webhook/pull_request\n\n\napplication/json\n\n\nPull requests\n\n\n\n\n\n\nhttps://topology-itb.opensciencegrid.org/webhook/status\n\n\napplication/json\n\n\nStatuses\n\n\n\n\n\n\nhttps://topology-itb.opensciencegrid.org/webhook/pull_request\n\n\napplication/json\n\n\nPull requests\n\n\n\n\n\n\n\n\nFor each webhook, \"Secret\" should be a random 40 digit hex string, which should match the contents of the file\n \n/etc/opt/topology/github_webhook_secret\n (the path configured in \nWEBHOOK_SECRET_KEY\n).\n\n\n\n\n\n\nThe OSG's dedicated GitHub user for automating pushes is currently \nosg-bot\n.\n\n\n\n\n\n\nThis user needs to have write access to the \ntopology\n repo on GitHub.\n\n\n\n\n\n\nThe ssh public key in \n/etc/opt/topology/github.pub\n should be registered with the \nosg-bot\n GitHub user.\n\n\nThis can be done by logging into GitHub as \nosg-bot\n, and adding the new ssh key under the\n \nsettings\n page.\n\n\n\n\n\n\n\n\n\n\nRequired System Packages\n\n\nCurrently the webhook app uses the \nmailx\n command to send email.\nIf not already installed, install it with:\n\n\n    \n:::\nconsole\n\n    \n#\n \nyum\n \ninstall\n \nmailx\n\n\n\n\n\n\nTesting changes on the ITB instance\n\n\nAll changes should be tested on the ITB instance before deploying to production.\nIf you can, test them on your local machine first.\nThese instructions assume that the code has not been merged to master.\n\n\n\n\n\n\nUpdate the ITB software installation at \n/opt/topology-itb\n and note the current branch:\n\n\n#\n \ncd\n /opt/topology-itb\n\n#\n git fetch --all\n\n#\n git status\n\n\n\n\n\n\n\n\n\nCheck out the branch you are testing.\n    If the target remote is not configured, \nadd it\n:\n\n\n#\n git checkout -b <BRANCH> <REMOTE>/<BRANCH NAME>\n\n\n\n\n\n\n\n\n\nVerify that you are using the intended data associated with the code you are testing:\n\n\n\n\n\n\nIf the data format has changed in an incompatible way, modify \n/etc/opt/topology/config-itb.py\n:\n\n\n\n\n\n\nBackup the ITB configuration file:\n\n\n#\n \ncd\n /etc/opt/topology\n\n#\n cp -p config-itb.py\n{\n,.bak\n}\n\n\n\n\n\n\n\n\n\n\nChange the \nTOPOLOGY_DATA_DIR\n and/or \nCONTACT_DATA_DIR\n lines to point to a new directories so the previous\n   data does not get overwritten with incompatible data.\n\n\n\n\n\n\n\n\n\n\nIf you need to use a different branch for the data, switch to it:\n\n\n\n\n\n\nCheck the branch of \nTOPOLOGY_DATA_DIR\n from  \n/etc/opt/topology/config-itb.py\n\n\n#\n \ncd\n <TOPOLOGY_DATA_DIR>\n\n#\n git fetch --all\n\n#\n git status\n\n\n\n\n\n\n\n\n\nNote the previous branch, you will need this later\n\n\n\n\nIf the target remote is not configured, \nadd it\n\n\nCheck out the target branch:\n#\n git checkout -b <BRANCH NAME> <REMOTE>/<BRANCH NAME>\n\n\n\n\n\n\n\n\n\n\n\n\n\nPull any upstream changes to ensure that your branch is up to date:\n\n\n#\n git pull\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor updates to the webhook app, follow the above instructions for the ITB webhook instance under\n\n/opt/topology-itb-webhook\n and its corresponding config file, \n/etc/opt/topology/config-itb-webhook.py\n.\n\n\n\n\n\n\nRestart \nhttpd\n:\n\n\n#\n systemctl restart httpd\n\n\n\n\n\n\n\n\n\nTest the web interface at \nhttps://topology-itb.opensciencegrid.org\n.\n\n\nErrors and output are in \n/var/log/httpd/error_log\n.\n\n\n\n\n\n\nReverting changes\n\n\n\n\n\n\nSwitch \n/opt/topology-itb\n to the previous branch:\n\n\n#\n \ncd\n /opt/topology-itb\n\n#\n git checkout <BRANCH>\n\n\n\n\n\n\n\n\n\nFor updates to the webhook app, switch \n/opt/topology-itb-webhook\n to the previous master:\n\n\n#\n \ncd\n /opt/topology-itb-webhook\n\n#\n git checkout <BRANCH>\n\n\n\n\n\n\n\n\n\nIf you made config changes to \n/etc/opt/topology/config-itb.py\n or \nconfig-itb-webhook.py\n, restore the backup.\n\n\n\n\n\n\nIf you checked out a different branch for data, revert it back to the old branch.\n\n\n\n\n\n\nRestart \nhttpd\n:\n\n\n#\n systemctl restart httpd\n\n\n\n\n\n\n\n\n\nTest the web interface at \nhttps://topology-itb.opensciencegrid.org\n.\n\n\n\n\n\n\nUpdating the production instance\n\n\nUpdating the production instance is similar to updating ITB instance.\n\n\n\n\n\n\nUpdate master on the Git clone at \n/opt/topology\n:\n\n\n#\n \ncd\n /opt/topology\n\n#\n git pull origin master\n\n\n\n\n\n\n\n\n\nFor updates to the webhook app, update master on the Git clone at \n/opt/topology-webhook\n:\n\n\n#\n \ncd\n \n/\nopt\n/\ntopology\n-\nwebhook\n\n\n#\n \ngit\n \npull\n \norigin\n \nmaster\n\n\n\n\n\n\n\n\n\n\nMake config changes to \n/etc/opt/topology/config-production.py\n and/or \nconfig-production-webhook.py\n if necessary.\n\n\n\n\n\n\nRestart \nhttpd\n:\n\n\n#\n systemctl restart httpd\n\n\n\n\n\n\n\n\n\nTest the web interface at \nhttps://topology.opensciencegrid.org\n.\n\n\nErrors and output are in \n/var/log/httpd/error_log\n.\n\n\n\n\n\n\nReverting changes\n\n\n\n\n\n\nSwitch \n/opt/topology\n to the previous master:\n\n\n#\n \ncd\n /opt/topology\n\n#\n## (use `git reflog` to find the previous commit that was used)\n\n\n#\n git reset --hard <COMMIT>\n\n\n\n\n\n\n\n\n\nFor updates to the webhook app, switch \n/opt/topology-webhook\n to the previous master:\n\n\n#\n \ncd\n \n/\nopt\n/\ntopology\n-\nwebhook\n\n\n###\n \n(\nuse\n \n`\ngit\n \nreflog\n`\n \nto\n \nfind\n \nthe\n \nprevious\n \ncommit\n \nthat\n \nwas\n \nused\n)\n\n\n#\n \ngit\n \nreset\n \n--hard <COMMIT>\n\n\n\n\n\n\n\n\n\n\nIf you made config changes to \n/etc/opt/topology/config-production.py\n or \nconfig-production-webhook.py\n, revert them.\n\n\n\n\n\n\nRestart \nhttpd\n:\n\n\n#\n systemctl restart httpd\n\n\n\n\n\n\n\n\n\nTest the web interface at \nhttps://topology.opensciencegrid.org\n.",
            "title": "Topology Service"
        },
        {
            "location": "/services/topology/#topology-service",
            "text": "This document contains information about the service that runs:   https://topology.opensciencegrid.org  https://topology-itb.opensciencegrid.org  https://my.opensciencegrid.org  https://my-itb.opensciencegrid.org  https://myosg.opensciencegrid.org  https://map.opensciencegrid.org : Generates the topology map used on  OSG Display   The source code for the service is in  https://github.com/opensciencegrid/topology , in the  src/  subdirectory.\nThis repository also contains the public part of the data that gets served.",
            "title": "Topology Service"
        },
        {
            "location": "/services/topology/#deployment",
            "text": "Topology is a webapp run with Apache on the host  topology.opensciencegrid.org .\nThe ITB instance runs on the host  topology-itb.opensciencegrid.org .\nThe hosts are VMs at Nebraska;\nfor SSH access, contact Derek Weitzel or Brian Bockelman.",
            "title": "Deployment"
        },
        {
            "location": "/services/topology/#installation",
            "text": "These instructions assume an EL 7 host with the EPEL repositories available.\nThe software will be installed into  /opt/topology .\nA second instance for the webhook app will be installed into  /opt/topology-webhook .\n(The ITB instance should be installed into  /opt/topology-itb  and  /opt/topology-itb-webhook  instead.)\nThe following steps should be done as root.    Install prerequisites:  #  yum install python36 gridsite httpd mod_ssl    Clone the repository:  For the production topology host:  #  git clone https://github.com/opensciencegrid/topology /opt/topology #  git clone https://github.com/opensciencegrid/topology /opt/topology-webhook  For the topology-itb host:  #  git clone https://github.com/opensciencegrid/topology /opt/topology-itb #  git clone https://github.com/opensciencegrid/topology /opt/topology-itb-webhook    Set up the virtualenv in the clone -- from  /opt/topology  or  /opt/topology-itb :  #  python36 -m venv venv #  . ./venv/bin/activate #  pip install -r requirements-apache.txt    Repeat for the webhook instance -- from  /opt/topology-webhook  or  /opt/topology-itb-webhook .",
            "title": "Installation"
        },
        {
            "location": "/services/topology/#file-system-locations",
            "text": "The following files/directories must exist and have the proper permissions:     Location  Purpose  Ownership  Mode      /opt/topology  Production software install  root:root  0755    /opt/topology-itb  ITB software install  root:root  0755    /opt/topology-webhook  Production webhook software install  root:root  0755    /opt/topology-itb-webhook  ITB webhook software install  root:root  0755    /etc/opt/topology/config-production.py  Production config  root:root  0644    /etc/opt/topology/config-itb.py  ITB config  root:root  0644    /etc/opt/topology/bitbucket  Private key for contact info repo  apache:root  0600    /etc/opt/topology/bitbucket.pub  Public key for contact info repo  apache:root  0644    /etc/opt/topology/github  Private key for pushing automerge commits  topomerge:root  0600    /etc/opt/topology/github.pub  Public key for pushing automerge commits  topomerge:root  0644    /etc/opt/topology/github_webhook_secret  GitHub webhook secret for validating webhooks  topomerge:root  0600    ~apache/.ssh  SSH dir for Apache  apache:root  0700    ~apache/.ssh/known_hosts  Known hosts file for Apache  apache:root  0644    ~topomerge  Home dir for  topomerge  Apache user  topomerge:root  0755    ~topomerge/.ssh  SSH dir for  topomerge  Apache user  topomerge:root  0700    ~topomerge/.ssh/known_hosts  Known hosts file for  topomerge  Apache user  topomerge:root  0644    /var/cache/topology  Checkouts of topology and contacts data for production instance  apache:apache  0755    /var/cache/topology-itb  Checkouts of topology and contacts data for ITB instance  apache:apache  0755    /var/cache/topology-webhook  Topology repo and state info for production webhook instance  topomerge:topomerge  0755    /var/cache/topology-itb-webhook  Topology repo and state info for ITB webhook instance  topomerge:topomerge  0755     ~apache/.ssh/known_hosts  must contain an entry for  bitbucket.org ;\nuse  ssh-keyscan bitbucket.org  to get the appropriate entry.  ~topomerge/.ssh/known_hosts  must contain an entry for  github.com ;\nuse  ssh-keyscan github.com  to get the appropriate entry.",
            "title": "File system locations"
        },
        {
            "location": "/services/topology/#software-configuration",
            "text": "Configuration for the main app is under  /etc/opt/topology/ , in  config-production.py  and  config-itb.py .\nThe webhook app configuration is in  config-production-webhook.py  and  config-itb-webhook.py .\nThe files are in Python format and override default settings in  src/webapp/default_config.py  in the topology repo.  HTTPD configuration is in  /etc/httpd ; we use the modules  mod_ssl ,  mod_gridsite , and  mod_wsgi .\nThe first two are installed via yum;\nthe .so file for mod_wsgi is located in  /opt/topology/venv/lib/python3.6/site-packages/mod_wsgi/server/ \nor  /opt/topology-itb/venv/lib/python3.6/site-packages/mod_wsgi/server/  for the ITB instance.  Each of the hostnames are VHosts in the apache configuration.  Some special notes:   https://map.opensciencegrid.org  runs in the same wsgi process as the production topology, but the URL is limited to only the map code.  Further, it does not use mod_gridsite so that users are not asked to present a client certificate.  VHosts are configured: ServerName topology.opensciencegrid.org  ServerAlias my.opensciencegrid.org myosg.opensciencegrid.org",
            "title": "Software configuration"
        },
        {
            "location": "/services/topology/#data-configuration",
            "text": "Configuration is in  /etc/opt/topology/config-production.py  and  config-itb.py ;\nand  config-production-webhook.py  and  config-itb-webhook.py .     Variable  Purpose      TOPOLOGY_DATA_DIR  The directory containing a clone of the  topology  repository for data use    TOPOLOGY_DATA_REPO  The remote tracking repository of  TOPOLOGY_DATA_DIR    TOPOLOGY_DATA_BRANCH  The remote tracking branch of  TOPOLOGY_DATA_DIR    WEBHOOK_DATA_DIR  The directory containing a mirror-clone of the  topology  repository for webhook use    WEBHOOK_DATA_REPO  The remote tracking repository of  WEBHOOK_DATA_DIR    WEBHOOK_DATA_BRANCH  The remote tracking branch of  WEBHOOK_DATA_DIR    WEBHOOK_STATE_DIR  Directory containing webhook state information between pull request and status hooks    WEBHOOK_SECRET_KEY  Secret key configured on GitHub for webhook delivery    CONTACT_DATA_DIR  The directory containing a clone of the  contact  repository for data use    CONTACT_DATA_REPO  The remote tracking repository of  CONTACT_DATA_DIR (default:  \"git@bitbucket.org:opensciencegrid/contact.git\" )    CONTACT_DATA_BRANCH  The remote tracking branch of  CONTACT_DATA_BRANCH (default:  \"master\" )    CACHE_LIFETIME  Frequency of automatic data updates in seconds (default:  900 )    GIT_SSH_KEY  Location of ssh public key file for git access.    /etc/opt/topology/bitbucket.pub  for the main app, and  /etc/opt/topology/github.pub  for the webhook app      Puppet ensures that the production  contact  and  topology  clones are up to date with their configured remote tracking\nrepo and branch.\nPuppet does not manage the ITB data directories so they need to be updated by hand during testing.",
            "title": "Data configuration"
        },
        {
            "location": "/services/topology/#github-configuration-for-webhook-app",
            "text": "Go to the  webhook settings  page on GitHub.\n     There are four webhooks to set up;  pull_request  and  status  for both the topology and topology-itb hosts.     Payload URL  Content type  Events to trigger webhook      https://topology.opensciencegrid.org/webhook/status  application/json  Statuses    https://topology.opensciencegrid.org/webhook/pull_request  application/json  Pull requests    https://topology-itb.opensciencegrid.org/webhook/status  application/json  Statuses    https://topology-itb.opensciencegrid.org/webhook/pull_request  application/json  Pull requests     For each webhook, \"Secret\" should be a random 40 digit hex string, which should match the contents of the file\n  /etc/opt/topology/github_webhook_secret  (the path configured in  WEBHOOK_SECRET_KEY ).    The OSG's dedicated GitHub user for automating pushes is currently  osg-bot .    This user needs to have write access to the  topology  repo on GitHub.    The ssh public key in  /etc/opt/topology/github.pub  should be registered with the  osg-bot  GitHub user.  This can be done by logging into GitHub as  osg-bot , and adding the new ssh key under the\n  settings  page.",
            "title": "GitHub Configuration for Webhook App"
        },
        {
            "location": "/services/topology/#required-system-packages",
            "text": "Currently the webhook app uses the  mailx  command to send email.\nIf not already installed, install it with:       ::: console \n     #   yum   install   mailx",
            "title": "Required System Packages"
        },
        {
            "location": "/services/topology/#testing-changes-on-the-itb-instance",
            "text": "All changes should be tested on the ITB instance before deploying to production.\nIf you can, test them on your local machine first.\nThese instructions assume that the code has not been merged to master.    Update the ITB software installation at  /opt/topology-itb  and note the current branch:  #   cd  /opt/topology-itb #  git fetch --all #  git status    Check out the branch you are testing.\n    If the target remote is not configured,  add it :  #  git checkout -b <BRANCH> <REMOTE>/<BRANCH NAME>    Verify that you are using the intended data associated with the code you are testing:    If the data format has changed in an incompatible way, modify  /etc/opt/topology/config-itb.py :    Backup the ITB configuration file:  #   cd  /etc/opt/topology #  cp -p config-itb.py { ,.bak }     Change the  TOPOLOGY_DATA_DIR  and/or  CONTACT_DATA_DIR  lines to point to a new directories so the previous\n   data does not get overwritten with incompatible data.      If you need to use a different branch for the data, switch to it:    Check the branch of  TOPOLOGY_DATA_DIR  from   /etc/opt/topology/config-itb.py  #   cd  <TOPOLOGY_DATA_DIR> #  git fetch --all #  git status    Note the previous branch, you will need this later   If the target remote is not configured,  add it  Check out the target branch: #  git checkout -b <BRANCH NAME> <REMOTE>/<BRANCH NAME>      Pull any upstream changes to ensure that your branch is up to date:  #  git pull      For updates to the webhook app, follow the above instructions for the ITB webhook instance under /opt/topology-itb-webhook  and its corresponding config file,  /etc/opt/topology/config-itb-webhook.py .    Restart  httpd :  #  systemctl restart httpd    Test the web interface at  https://topology-itb.opensciencegrid.org .  Errors and output are in  /var/log/httpd/error_log .",
            "title": "Testing changes on the ITB instance"
        },
        {
            "location": "/services/topology/#reverting-changes",
            "text": "Switch  /opt/topology-itb  to the previous branch:  #   cd  /opt/topology-itb #  git checkout <BRANCH>    For updates to the webhook app, switch  /opt/topology-itb-webhook  to the previous master:  #   cd  /opt/topology-itb-webhook #  git checkout <BRANCH>    If you made config changes to  /etc/opt/topology/config-itb.py  or  config-itb-webhook.py , restore the backup.    If you checked out a different branch for data, revert it back to the old branch.    Restart  httpd :  #  systemctl restart httpd    Test the web interface at  https://topology-itb.opensciencegrid.org .",
            "title": "Reverting changes"
        },
        {
            "location": "/services/topology/#updating-the-production-instance",
            "text": "Updating the production instance is similar to updating ITB instance.    Update master on the Git clone at  /opt/topology :  #   cd  /opt/topology #  git pull origin master    For updates to the webhook app, update master on the Git clone at  /opt/topology-webhook :  #   cd   / opt / topology - webhook  #   git   pull   origin   master     Make config changes to  /etc/opt/topology/config-production.py  and/or  config-production-webhook.py  if necessary.    Restart  httpd :  #  systemctl restart httpd    Test the web interface at  https://topology.opensciencegrid.org .  Errors and output are in  /var/log/httpd/error_log .",
            "title": "Updating the production instance"
        },
        {
            "location": "/services/topology/#reverting-changes_1",
            "text": "Switch  /opt/topology  to the previous master:  #   cd  /opt/topology # ## (use `git reflog` to find the previous commit that was used)  #  git reset --hard <COMMIT>    For updates to the webhook app, switch  /opt/topology-webhook  to the previous master:  #   cd   / opt / topology - webhook  ###   ( use   ` git   reflog `   to   find   the   previous   commit   that   was   used )  #   git   reset   --hard <COMMIT>     If you made config changes to  /etc/opt/topology/config-production.py  or  config-production-webhook.py , revert them.    Restart  httpd :  #  systemctl restart httpd    Test the web interface at  https://topology.opensciencegrid.org .",
            "title": "Reverting changes"
        },
        {
            "location": "/services/topology-contacts-data/",
            "text": "Topology and Contacts Data\n\n\nThis is internal documentation intended for OSG Operations staff.\nIt contains information about the data provided by \nhttps://topology.opensciencegrid.org\n.\n\n\nThe topology data for the service is in \nhttps://github.com/opensciencegrid/topology\n, in the \nprojects/\n, \ntopology/\n, and \nvirtual-organizations/\n subdirectories.\nThe contacts data is in \nhttps://bitbucket.org/opensciencegrid/contact/\n, in \ncontacts.yaml\n.\n\n\nTopology Data\n\n\nAdmins may request changes to data in the topology repo via either a GitHub pull request or a Freshdesk ticket.\nThese changes can be to a project, a VO, or a resource.\nThe \nregistration document\n and\n\ntopology README document\n should tell them how to do that.\n\n\nIn the case of a GitHub pull request, you will need to provide IDs using the \nnext_*_id\n tools in the Topology\n\nbin/ dir\n and potentially fix-up other data.\nTo assist the user, do one of the following, depending on the severity of the fixes required for the PR:\n\n\n\n\nFor minor issues, submit a \"Comment\" review using\n  \nGitHub suggestions\n\n  and ask the user to \nincorporate your suggestions\n.\n\n\nFor major issues, create a branch based off of their PR, make changes, and submit your own PR that\n  \ncloses\n the original user's PR.\n\n\n\n\nThe CI checks should catch most errors but you should still review the YAML changes.\nCertain things to check are:\n\n\n\n\n\n\nDo contact names and IDs match what's in the contacts data?\n    (See below for instructions on how to get that information.)\n    If the person is not in the contacts data, you will need to add them before approving the PR.\n\n\n\n\n\n\nIs the PR submitter authorized to make changes to that project/VO/resource?\n    Can you match them to a person affiliated with that project/VO/site?\n    (The contacts data now includes the GitHub usernames for some people.\n    See below for instructions on how to get that information.)\n\n\n\n\n\n\nIs their GitHub ID registered in the \ncontact database\n and are they associated with the relevant\n    resource, site, facility, or VO?\n\n\n\n\n\n\nRetiring resources\n\n\nA resource can be disabled in its topology yaml file by setting \nActive: false\n.\nHowever the resource entry should not be immediately deleted from the yaml file.\n\n\nOne reason for this is that the WLCG accounting info configured for resources is used to determine which resources to\nsend APEL numbers for.\nRemoving resources prematurely could prevent resummarized GRACC data from getting sent appropriately.\n\n\nResources that have been inactive for at least two years are eligible to be deleted from the topology database.\nThe GRACC records for this resource can be inspected in \nKibana\n.\n\n\n\n\n\n\nIn the search bar, enter \nProbeName:*\\:FQDN\n in the search bar, where \nFQDN\n is the FQDN defined for your resource\n\n\nFor example, if your resource FQDN is \ncmsgrid01.hep.wisc.edu\n you would enter \nProbeName:*\\:cmsgrid01.hep.wisc.edu\n\n\n\n\n\n\nIn the upper-right corner, use the Time Range selection to pick \"Last 2 years\"\n\n\n\n\n\n\nWith this criteria selected, Kibana will show you if it has received any records for this resource in the past two\nyears.\n\n\nIf there are no records returned, you may remove the resource from the resource group yaml file in the topology repo.\nAny downtime entries for this resource in the corresponding downtime yaml file for the resource group must be removed\nalso.\nIf you remove the last resource in the resource group yaml file, you should remove the resource group and corresponding\ndowntime yaml files as well.\n\n\nContacts Data\n\n\nThe OSG keeps contact data for administrators and maintainers of OSG resources and VOs for the purpose of distributing\nsecurity, software, and adminstrative (e.g., OSG All-Hands dates) announcements.\nAdditionally, OSG contacts have the following abilities:\n\n\n\n\nView other contacts' information (via \nHTML\n and\n  \nXML\n) with a registered certificate\n\n\nRegister resource downtimes\n\n  for resources that they are listed as an administrative contact, if they have a registered GitHub ID\n\n\n\n\nContact data is kept as editable YAML in \nhttps://bitbucket.org/opensciencegrid/contact/\n, in \ncontacts.yaml\n.\nThe YAML file contains sensitive information and is only visible to people with access to that repo.\n\n\nGetting access to the contact repo\n\n\nThe contacts repo is hosted on BitBucket.\nYou will need an Atlassian account for access to BitBucket.\nThe account you use for OSG JIRA should work.\nOnce you have an account, request access from Brian Lin, Mat Selmeci, or Derek Weitzel.\nYou should then be able to go to \nhttps://bitbucket.org/opensciencegrid/contact/\n.\n\n\nUsing the contact repo\n\n\nBitBucket is similar to GitHub except you don't make a fork of the contact repo,\nyou just clone it to your local machine.\nThis means that any pushes go directly to the main repo instead of your own fork.\n\n\n\n\nDanger\n\n\nDon't push to master.\nFor any changes, always create your own branch, push your changes to that branch, then make a pull request.\nHave someone else review and merge your pull request.\n\n\n\n\nAll contact data is stored in \ncontacts.yaml\n.\nThe contact info is keyed by a 40-character hexadecimal ID which was generated from their email address when they were first added.\nAn example entry is:\n\n\n25357f62c7ab2ae11ddda1efd272bb5435dbfacb\n:\n\n\n# ^ this is their ID\n\n  \nFullName\n:\n \nExample A. User\n\n  \nProfile\n:\n \nThis is an example user.\n\n  \nGitHub\n:\n \nExampleUser\n\n  \n# ContactInformation data requires authorization to view\n\n  \nContactInformation\n:\n\n    \nDNs\n:\n\n    \n-\n \n...\n\n    \nIM\n:\n \n...\n\n    \nPrimaryEmail\n:\n \nuser@example.net\n\n    \nPrimaryPhone\n:\n \n...\n\n\n\n\n\n\nWhen making changes to the contact data, first see if a contact is already in the YAML file.\nSearch the YAML file for their name.\nBe sure to try variations of their name if you don't find them --\nsomeone may be listed as \"Dave\" or \"David\", or have a middle name or middle initial.\n\n\nFollow the instructions below for adding or updating a contact, as appropriate.\n\n\nAdding a new contact\n\n\n\n\nDanger\n\n\nAny new contacts need to have their association with the OSG verified by a known contact within the relevant VO,\nsite, or project.\n\n\n\n\nWhen registering a new contact, first obtain the required\n\ncontact information\n.\n\n\nAfter obtaining this information and verifying their association with the OSG, fill out the values in\n\ntemplate-contacts.yaml\n and add it to \ncontacts.yaml\n.\nTo get the hash used as the ID, run \nemail-hash\n on their email address.\nFor example:\n\n\n$ \ncd\n contact  \n# this is your local clone of the \"contact\" repo\n\n$ bin/email-hash user@example.net\n25357f62c7ab2ae11ddda1efd272bb5435dbfacb\n\n\n\n\n\nThen your new entry will look like\n\n\n25357f62c7ab2ae11ddda1efd272bb5435dbfacb\n:\n\n    \nFullName\n:\n \nExample A. User\n\n    \n....\n\n\n\n\n\n\nThe \nFullName\n and \nProfile\n fields in the main section,\nand the \nPrimaryEmail\n field in the \nContactInformation\n section are required.\nThe \nPrimaryEmail\n field in the \nContactInformation\n section should match the hash that you used for the ID.\n\n\nIn addition, if they will be making pull requests against the topology repo,\ne.g. for updating site information, reporting downtime, or updating project or VO information,\nobtain their GitHub username and put it in the \nGitHub\n field.\n\n\nEditing a contact\n\n\nOnce you have found a contact in the YAML file, edit the attributes by hand.\nIf you want to add information that is not present for that contact, look at \ntemplate-contacts.yaml\n to find out what the attributes are called.\n\n\n\n\nNote\n\n\nThe ID of the contact never changes, even if the user's \nPrimaryEmail\n changes.\n\n\n\n\n\n\nImportant\n\n\nIf you change the contact's \nFullName\n, you \nmust\n make the same change to every place that the contact\nis mentioned in the \ntopology\n repo.\nGet the contact changes merged in first.",
            "title": "Topology and Contacts Data"
        },
        {
            "location": "/services/topology-contacts-data/#topology-and-contacts-data",
            "text": "This is internal documentation intended for OSG Operations staff.\nIt contains information about the data provided by  https://topology.opensciencegrid.org .  The topology data for the service is in  https://github.com/opensciencegrid/topology , in the  projects/ ,  topology/ , and  virtual-organizations/  subdirectories.\nThe contacts data is in  https://bitbucket.org/opensciencegrid/contact/ , in  contacts.yaml .",
            "title": "Topology and Contacts Data"
        },
        {
            "location": "/services/topology-contacts-data/#topology-data",
            "text": "Admins may request changes to data in the topology repo via either a GitHub pull request or a Freshdesk ticket.\nThese changes can be to a project, a VO, or a resource.\nThe  registration document  and topology README document  should tell them how to do that.  In the case of a GitHub pull request, you will need to provide IDs using the  next_*_id  tools in the Topology bin/ dir  and potentially fix-up other data.\nTo assist the user, do one of the following, depending on the severity of the fixes required for the PR:   For minor issues, submit a \"Comment\" review using\n   GitHub suggestions \n  and ask the user to  incorporate your suggestions .  For major issues, create a branch based off of their PR, make changes, and submit your own PR that\n   closes  the original user's PR.   The CI checks should catch most errors but you should still review the YAML changes.\nCertain things to check are:    Do contact names and IDs match what's in the contacts data?\n    (See below for instructions on how to get that information.)\n    If the person is not in the contacts data, you will need to add them before approving the PR.    Is the PR submitter authorized to make changes to that project/VO/resource?\n    Can you match them to a person affiliated with that project/VO/site?\n    (The contacts data now includes the GitHub usernames for some people.\n    See below for instructions on how to get that information.)    Is their GitHub ID registered in the  contact database  and are they associated with the relevant\n    resource, site, facility, or VO?",
            "title": "Topology Data"
        },
        {
            "location": "/services/topology-contacts-data/#retiring-resources",
            "text": "A resource can be disabled in its topology yaml file by setting  Active: false .\nHowever the resource entry should not be immediately deleted from the yaml file.  One reason for this is that the WLCG accounting info configured for resources is used to determine which resources to\nsend APEL numbers for.\nRemoving resources prematurely could prevent resummarized GRACC data from getting sent appropriately.  Resources that have been inactive for at least two years are eligible to be deleted from the topology database.\nThe GRACC records for this resource can be inspected in  Kibana .    In the search bar, enter  ProbeName:*\\:FQDN  in the search bar, where  FQDN  is the FQDN defined for your resource  For example, if your resource FQDN is  cmsgrid01.hep.wisc.edu  you would enter  ProbeName:*\\:cmsgrid01.hep.wisc.edu    In the upper-right corner, use the Time Range selection to pick \"Last 2 years\"    With this criteria selected, Kibana will show you if it has received any records for this resource in the past two\nyears.  If there are no records returned, you may remove the resource from the resource group yaml file in the topology repo.\nAny downtime entries for this resource in the corresponding downtime yaml file for the resource group must be removed\nalso.\nIf you remove the last resource in the resource group yaml file, you should remove the resource group and corresponding\ndowntime yaml files as well.",
            "title": "Retiring resources"
        },
        {
            "location": "/services/topology-contacts-data/#contacts-data",
            "text": "The OSG keeps contact data for administrators and maintainers of OSG resources and VOs for the purpose of distributing\nsecurity, software, and adminstrative (e.g., OSG All-Hands dates) announcements.\nAdditionally, OSG contacts have the following abilities:   View other contacts' information (via  HTML  and\n   XML ) with a registered certificate  Register resource downtimes \n  for resources that they are listed as an administrative contact, if they have a registered GitHub ID   Contact data is kept as editable YAML in  https://bitbucket.org/opensciencegrid/contact/ , in  contacts.yaml .\nThe YAML file contains sensitive information and is only visible to people with access to that repo.",
            "title": "Contacts Data"
        },
        {
            "location": "/services/topology-contacts-data/#getting-access-to-the-contact-repo",
            "text": "The contacts repo is hosted on BitBucket.\nYou will need an Atlassian account for access to BitBucket.\nThe account you use for OSG JIRA should work.\nOnce you have an account, request access from Brian Lin, Mat Selmeci, or Derek Weitzel.\nYou should then be able to go to  https://bitbucket.org/opensciencegrid/contact/ .",
            "title": "Getting access to the contact repo"
        },
        {
            "location": "/services/topology-contacts-data/#using-the-contact-repo",
            "text": "BitBucket is similar to GitHub except you don't make a fork of the contact repo,\nyou just clone it to your local machine.\nThis means that any pushes go directly to the main repo instead of your own fork.   Danger  Don't push to master.\nFor any changes, always create your own branch, push your changes to that branch, then make a pull request.\nHave someone else review and merge your pull request.   All contact data is stored in  contacts.yaml .\nThe contact info is keyed by a 40-character hexadecimal ID which was generated from their email address when they were first added.\nAn example entry is:  25357f62c7ab2ae11ddda1efd272bb5435dbfacb :  # ^ this is their ID \n   FullName :   Example A. User \n   Profile :   This is an example user. \n   GitHub :   ExampleUser \n   # ContactInformation data requires authorization to view \n   ContactInformation : \n     DNs : \n     -   ... \n     IM :   ... \n     PrimaryEmail :   user@example.net \n     PrimaryPhone :   ...   When making changes to the contact data, first see if a contact is already in the YAML file.\nSearch the YAML file for their name.\nBe sure to try variations of their name if you don't find them --\nsomeone may be listed as \"Dave\" or \"David\", or have a middle name or middle initial.  Follow the instructions below for adding or updating a contact, as appropriate.",
            "title": "Using the contact repo"
        },
        {
            "location": "/services/topology-contacts-data/#adding-a-new-contact",
            "text": "Danger  Any new contacts need to have their association with the OSG verified by a known contact within the relevant VO,\nsite, or project.   When registering a new contact, first obtain the required contact information .  After obtaining this information and verifying their association with the OSG, fill out the values in template-contacts.yaml  and add it to  contacts.yaml .\nTo get the hash used as the ID, run  email-hash  on their email address.\nFor example:  $  cd  contact   # this is your local clone of the \"contact\" repo \n$ bin/email-hash user@example.net\n25357f62c7ab2ae11ddda1efd272bb5435dbfacb  Then your new entry will look like  25357f62c7ab2ae11ddda1efd272bb5435dbfacb : \n     FullName :   Example A. User \n     ....   The  FullName  and  Profile  fields in the main section,\nand the  PrimaryEmail  field in the  ContactInformation  section are required.\nThe  PrimaryEmail  field in the  ContactInformation  section should match the hash that you used for the ID.  In addition, if they will be making pull requests against the topology repo,\ne.g. for updating site information, reporting downtime, or updating project or VO information,\nobtain their GitHub username and put it in the  GitHub  field.",
            "title": "Adding a new contact"
        },
        {
            "location": "/services/topology-contacts-data/#editing-a-contact",
            "text": "Once you have found a contact in the YAML file, edit the attributes by hand.\nIf you want to add information that is not present for that contact, look at  template-contacts.yaml  to find out what the attributes are called.   Note  The ID of the contact never changes, even if the user's  PrimaryEmail  changes.    Important  If you change the contact's  FullName , you  must  make the same change to every place that the contact\nis mentioned in the  topology  repo.\nGet the contact changes merged in first.",
            "title": "Editing a contact"
        },
        {
            "location": "/services/finalize-cache-registration/",
            "text": "Finalizing New Cache Registration\n\n\nOnce a new cache is registered with OSG, there are additional operations tasks that\nmust be performed before it is usable by clients.\n\n\nThe steps on this page are for OSG Operations; sysadmins should follow the\n\ncache registration document\n\nand open a support ticket to have these steps executed.\n\n\nUn-Authenticated Cache\n\n\n\n\n\n\nTest to make sure the cache is working by executing the following:\n\n\nconsole\n$ curl http://hcc-stash.unl.edu:8000/user/rynge/public/test.txt\nHello!\n\n\n\n\n\n\nOpen a pull request to add the cache to \ncaches.json\n file within the StashCache repo.\n\n\n\n\n\n\nOpen a pull request adding the cache to \nCVMFS_EXTERNAL_URL\n in the  \nosgstorage.org.conf\n file.\n\n\n\n\n\n\nAuthenticated Cache\n\n\nFor an authenticated cache, it will need to be added to the specific CVMFS configuration for the authenticated domain.  For example, if it is a LIGO authenticated cache, it will need to be added to the \nCVMFS_EXTERNAL_URL\n within the \nligo.osgstorage.org.conf\n file in the \nconfig.d\n directory.  A CMS authenticated cache will need to be added to the \ncms.osgstorage.org.conf\n file\n\n\n\n\n\n\nOpen a pull request adding the authenticated cache to \nCVMFS_EXTERNAL_URL\n in the appropriate domain configuration file within \nconfig.d\n.\n\n\n\n\n\n\nCoordinate with the VO to test that authorization works.  As each VO is expected to export a different directory and require different authorizations, a custom test\n   must be arranged each time.",
            "title": "Finalize Cache Registration"
        },
        {
            "location": "/services/finalize-cache-registration/#finalizing-new-cache-registration",
            "text": "Once a new cache is registered with OSG, there are additional operations tasks that\nmust be performed before it is usable by clients.  The steps on this page are for OSG Operations; sysadmins should follow the cache registration document \nand open a support ticket to have these steps executed.",
            "title": "Finalizing New Cache Registration"
        },
        {
            "location": "/services/finalize-cache-registration/#un-authenticated-cache",
            "text": "Test to make sure the cache is working by executing the following:  console\n$ curl http://hcc-stash.unl.edu:8000/user/rynge/public/test.txt\nHello!    Open a pull request to add the cache to  caches.json  file within the StashCache repo.    Open a pull request adding the cache to  CVMFS_EXTERNAL_URL  in the   osgstorage.org.conf  file.",
            "title": "Un-Authenticated Cache"
        },
        {
            "location": "/services/finalize-cache-registration/#authenticated-cache",
            "text": "For an authenticated cache, it will need to be added to the specific CVMFS configuration for the authenticated domain.  For example, if it is a LIGO authenticated cache, it will need to be added to the  CVMFS_EXTERNAL_URL  within the  ligo.osgstorage.org.conf  file in the  config.d  directory.  A CMS authenticated cache will need to be added to the  cms.osgstorage.org.conf  file    Open a pull request adding the authenticated cache to  CVMFS_EXTERNAL_URL  in the appropriate domain configuration file within  config.d .    Coordinate with the VO to test that authorization works.  As each VO is expected to export a different directory and require different authorizations, a custom test\n   must be arranged each time.",
            "title": "Authenticated Cache"
        },
        {
            "location": "/services/sending-announcements/",
            "text": "Sending Announcements\n\n\nVarious OSG teams need to send out announcement about various events (releases, security advisories, planned changes,\netc).\nThis page describes how to send announcements using the \nosg-notify\n tool.\n\n\nPrerequisites\n\n\nTo send announcements, the following conditions must be met:\n\n\n\n\nA host with an IP address listed in the\n    \nSPF Record\n\n\nA sufficiently modern Linux operating system.\n    This procedure has been tested on a FermiCloud Scientific Linux 7 VM and a Linux Mint 18.3 laptop.\n    It is known not to work on a FermiCloud Scientific Linux 6 VM.\n\n\nA valid OSG user certificate to lookup contacts in the topology database\n\n\nLocal hostname matches DNS\n\n\nDNS forward and reverse lookups in place\n\n\n\n\n[tim@submit-1 topology]$\n hostname\n\nsubmit-1.chtc.wisc.edu\n\n\n[tim@submit-1 topology]$\n host submit-1.chtc.wisc.edu\n\nsubmit-1.chtc.wisc.edu has address 128.105.244.191\n\n\n[tim@submit-1 topology]$\n host \n128\n.105.244.191\n\n191.244.105.128.in-addr.arpa domain name pointer submit-1.chtc.wisc.edu.\n\n\n\n\n\n\n\n\n(Required for security announcements)\n A GPG Key to sign the announcement\n\n\n\n\nInstallation\n\n\n\n\n\n\nInstall the pre-requisites:\n\n\n\n\n\n\nEnterprise Linux 7 (first, enable\n    \nEPEL\n)\n\n\nyum install git python-requests python2-gnupg\n\n\n\n\n\n\n\n\n\n\nUbuntu\n\n\napt install git python-requests python-gnupg\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInstall the OSG tools:\n\n\ngit clone https://github.com/opensciencegrid/topology.git\n\n\n\n\n\n\n\n\n\n\nIf you are on a FermiCloud VM, update \npostfix\n to relay through FermiLab's official mail server:\n\n\necho \"transport_maps = hash:/etc/postfix/transport\" >> /etc/postfix/main.cf\n\n\necho \"*   smtp:smtp.fnal.gov\" >> /etc/postfix/transport\n\n\npostmap hash:/etc/postfix/transport\n\n\npostfix reload\n\n\n\n\n\n\n\n\n\n\nEnsure that you can lookup contacts in the topology database by using the \nosg-topology\n tool to list the contacts:\n\n\ncd topology\n\n\nbin/osg-topology --cert publicCert.pem \\\n\n\n    --key privateKey.pem list-resource-contacts\n\n\n\n\n\n\nIf the contacts include email addresses, this is working properly.\nIf you type your password incorrectly, the authentication will silently fail and you won't get email addresses\n\n\n\n\n\n\nTest this setup by sending a message to yourself only.\n    Bonus points for using an email address that goes to a site with aggressive SPAM filtering.\n\n\n\n\n\n\nSending the announcement\n\n\nBefore using \nosg-notify\n, update your clone of the \ntopology\n repo by running:\n\n\n$\n \ncd\n topology\n\n$\n git pull\n\n\n\n\n\nUse the \nosg-notify\n tool to send the announcement using the relevant options from the following table:\n\n\n\n\n\n\n\n\nOption\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n--dry-run\n\n\nUse this option until you are ready to actually send the message\n\n\n\n\n\n\n--cert <FILE>\n\n\nFile that contains your OSG User Certificate\n\n\n\n\n\n\n--key <FILE>\n\n\nFile that contains your Private Key for your OSG User Certificate\n\n\n\n\n\n\n--no-sign\n\n\nDon't GPG sign the message (release only)\n\n\n\n\n\n\n--type production\n\n\nNot a test message\n\n\n\n\n\n\n--message <FILE>\n\n\nFile containing your message\n\n\n\n\n\n\n--subject <EMAIL SUBJECT>\n\n\nThe subject of your message\n\n\n\n\n\n\n--recipients <LIST OF EMAILS>\n\n\nList of recipient email addresses, must have at least one\n\n\n\n\n\n\n--oim-recipients resources\n\n\nSelect contact associated with resources\n\n\n\n\n\n\n--oim-contact-type <TYPE>\n\n\nReplacing \n<TYPE>\n with \nadministrative\n for release announcements or  \nsecurity\n for security announcements\n\n\n\n\n\n\n--bypass-dns-check\n\n\nUse this option to skip the check that one of the host's IP addresses matches with the hostname resolution\n\n\n\n\n\n\n\n\n\n\nSecurity requirements\n\n\nSecurity announcements must be signed using the following options:\n\n\n\n\n--sign\n: GPG sign the message\n\n\n--sign-id <KEYID>\n: The ID of the key used for singing\n\n\n--from security\n: The mail comes from the OSG Security Team\n\n\n\n\n\n\nFor release announcements use the following command:\n\n\nbin/osg-notify --cert your-cert.pem --key your-key.pem \\\n\n\n    --no-sign --type production --message message-file \\\n\n\n    --subject '<EMAIL SUBJECT>' \\\n\n\n    --recipients \"osg-general@opensciencegrid.org osg-operations@opensciencegrid.org osg-sites@opensciencegrid.org vdt-discuss@opensciencegrid.org\" \\\n\n\n    --oim-recipients resources --oim-contact-type administrative\n\n\n\n\n\n\nReplacing \n<EMAIL SUBJECT>\n with an appropriate subject for your announcement.",
            "title": "Sending Announcements"
        },
        {
            "location": "/services/sending-announcements/#sending-announcements",
            "text": "Various OSG teams need to send out announcement about various events (releases, security advisories, planned changes,\netc).\nThis page describes how to send announcements using the  osg-notify  tool.",
            "title": "Sending Announcements"
        },
        {
            "location": "/services/sending-announcements/#prerequisites",
            "text": "To send announcements, the following conditions must be met:   A host with an IP address listed in the\n     SPF Record  A sufficiently modern Linux operating system.\n    This procedure has been tested on a FermiCloud Scientific Linux 7 VM and a Linux Mint 18.3 laptop.\n    It is known not to work on a FermiCloud Scientific Linux 6 VM.  A valid OSG user certificate to lookup contacts in the topology database  Local hostname matches DNS  DNS forward and reverse lookups in place   [tim@submit-1 topology]$  hostname submit-1.chtc.wisc.edu  [tim@submit-1 topology]$  host submit-1.chtc.wisc.edu submit-1.chtc.wisc.edu has address 128.105.244.191  [tim@submit-1 topology]$  host  128 .105.244.191 191.244.105.128.in-addr.arpa domain name pointer submit-1.chtc.wisc.edu.    (Required for security announcements)  A GPG Key to sign the announcement",
            "title": "Prerequisites"
        },
        {
            "location": "/services/sending-announcements/#installation",
            "text": "Install the pre-requisites:    Enterprise Linux 7 (first, enable\n     EPEL )  yum install git python-requests python2-gnupg     Ubuntu  apt install git python-requests python-gnupg       Install the OSG tools:  git clone https://github.com/opensciencegrid/topology.git     If you are on a FermiCloud VM, update  postfix  to relay through FermiLab's official mail server:  echo \"transport_maps = hash:/etc/postfix/transport\" >> /etc/postfix/main.cf  echo \"*   smtp:smtp.fnal.gov\" >> /etc/postfix/transport  postmap hash:/etc/postfix/transport  postfix reload     Ensure that you can lookup contacts in the topology database by using the  osg-topology  tool to list the contacts:  cd topology  bin/osg-topology --cert publicCert.pem \\      --key privateKey.pem list-resource-contacts   If the contacts include email addresses, this is working properly.\nIf you type your password incorrectly, the authentication will silently fail and you won't get email addresses    Test this setup by sending a message to yourself only.\n    Bonus points for using an email address that goes to a site with aggressive SPAM filtering.",
            "title": "Installation"
        },
        {
            "location": "/services/sending-announcements/#sending-the-announcement",
            "text": "Before using  osg-notify , update your clone of the  topology  repo by running:  $   cd  topology $  git pull  Use the  osg-notify  tool to send the announcement using the relevant options from the following table:     Option  Description      --dry-run  Use this option until you are ready to actually send the message    --cert <FILE>  File that contains your OSG User Certificate    --key <FILE>  File that contains your Private Key for your OSG User Certificate    --no-sign  Don't GPG sign the message (release only)    --type production  Not a test message    --message <FILE>  File containing your message    --subject <EMAIL SUBJECT>  The subject of your message    --recipients <LIST OF EMAILS>  List of recipient email addresses, must have at least one    --oim-recipients resources  Select contact associated with resources    --oim-contact-type <TYPE>  Replacing  <TYPE>  with  administrative  for release announcements or   security  for security announcements    --bypass-dns-check  Use this option to skip the check that one of the host's IP addresses matches with the hostname resolution      Security requirements  Security announcements must be signed using the following options:   --sign : GPG sign the message  --sign-id <KEYID> : The ID of the key used for singing  --from security : The mail comes from the OSG Security Team    For release announcements use the following command:  bin/osg-notify --cert your-cert.pem --key your-key.pem \\      --no-sign --type production --message message-file \\      --subject '<EMAIL SUBJECT>' \\      --recipients \"osg-general@opensciencegrid.org osg-operations@opensciencegrid.org osg-sites@opensciencegrid.org vdt-discuss@opensciencegrid.org\" \\      --oim-recipients resources --oim-contact-type administrative   Replacing  <EMAIL SUBJECT>  with an appropriate subject for your announcement.",
            "title": "Sending the announcement"
        },
        {
            "location": "/SLA/collector/",
            "text": "GOC Condor Collector Service Level Agreement\n\n\nAbout This Document\n\n\nThis document details a service level agreement which outlines production expectations for the GOC Condor Collector and defines general support infrastructure for the service.\n\n\nVersion Control\n\n\n\n\n\n\n\n\nVersion Number\n\n\nDate\n\n\nAuthor\n\n\nComments\n\n\n\n\n\n\n\n\n\n\n1.1\n\n\n9-24-2014\n\n\nScott Teige\n\n\nFirst Draft\n\n\n\n\n\n\n\n\nExecutive Summary\n\n\nThis SLA is an agreement between OSG and OSG Stakeholders pertaining to the operation of the OSG Condor Collector service.\n\n\nOwners\n\n\nThe Condor Collector SLA is owned by OSG Operations. It will be reviewed by and agreed upon by the OSG Executive Team.\n\n\nService Name and Description\n\n\nName\n\n\nGOC Condor Collector\n\n\nDescription\n\n\nThe grid-facing component of the HTCondor compute element system available for use by all OSG VOs.\n\n\nSecurity Considerations\n\n\nAll information associated with the service is covered by the Indiana University \nprivacy policy\n.\nEffect of compromise of this service are limited to the service. A compromise may make the service unusable by compute elements using it.\n\n\nService Target Response Priorities and Response Times\n\n\nThis section deals with unplanned outages. Please see \nRequests for Service Enhancement\n for information on planned maintenance outages.\n\n\n\n\n\n\n\n\nCritical\n\n\nHigh\n\n\nElevated\n\n\nNormal\n\n\n\n\n\n\n\n\n\n\nDescription\n\n\n* *\n\n\n* *\n\n\n* *\n\n\n\n\n\n\nThis service does not have critical priority\n\n\nThis service does not have high priority\n\n\nThis issue prevents all new sites from advertising in the info service or clients from querying the service. This issue causes intermittent drops of updates or query failures.\n\n\nThe issue prevents effective monitoring or full utilization of the Condor pool\n\n\n\n\n\n\nResponse Time\n\n\n* *\n\n\n* *\n\n\n* *\n\n\n\n\n\n\nN/A\n\n\nN/A\n\n\nWithin 2 business days\n\n\nWithin 10 business days\n\n\n\n\n\n\nResolution Time\n\n\n* *\n\n\n* *\n\n\n* *\n\n\n\n\n\n\nN/A\n\n\nN/A\n\n\nThe maximum acceptable resolution time is five (5) business days\n\n\nThe maximum acceptable resolution time is ten (10) business days\n\n\n\n\n\n\nEscalates Every\n\n\n* *\n\n\n* *\n\n\n* *\n\n\n\n\n\n\nN/A\n\n\nN/A\n\n\nFive Days\n\n\nTen Days\n\n\n\n\n\n\n\n\nEscalation Contacts\n\n\n\n\n\n\n\n\nEscalation Level\n\n\nOSG Contact\n\n\n\n\n\n\n\n\n\n\n1st\n\n\nOSG Operations Infrastructure Lead\n\n\n\n\n\n\n2nd\n\n\nOSG Operations Coordinator\n\n\n\n\n\n\n3rd\n\n\nOSG Production Coordinator\n\n\n\n\n\n\n4th\n\n\nOSG Technical Director and Executive Director\n\n\n\n\n\n\n\n\nCritical Level Security Issues will be elevated immediately to the OSG Security Team.\n\n\nDetailed information on contacts are viewable on the following \nMyOSG URL\n, and are maintained within the\n\n\nAny ongoing \"Normal\" or \"Elevated\" level issues will be discussed at the weekly \nOperations\n and \nProduction\n meetings.\n\n\nService Availability and Outages\n\n\nThe GOC will strive for 90% service availability. If service availability falls below 90% monthly as monitored by the GOC a root cause analysis and service plan will be submitted to the OSG stakeholders for plans to restore an acceptable level of service availability.\n\n\nReliability and availability will be determined by 2 critical RSV probes and the OSG availability algorithm.\n   * One probe will verify the Condor collector is queryable whenever the host is pingable (i.e., reasonably not a network failure).\n   * The other probe will verify an automatically generated configuration file has been updated recently\n\n\nService Support Hours\n\n\nThis service will be run for 24x7, but support will primarily be within business hours. The exception is for security incidents.\n\n\nService Off-Hours Support Procedures\n\n\nAll operational issues should be reported as per \nCustomer Problem Reporting\n section.\n\n\nRequests for Service Enhancements\n\n\nOSG Operations will provide enhancement capabilities only as they are released by the HTCondor project or OSG Technology Group.\nRequests for customization of the deployed service may be made; OSG Operations has the right of up to one month of testing of any change.\n\n\nOSG will give 1 week of warning prior to any change in the Collector service version.\nAt any time during this week, Stakeholders are permitted to request a delay for up to 1 week after the originally scheduled upgrade.\n\n\nOSG Operations will schedule downtimes and configuration changes during normal business hours unless approved by affected stakeholders. This is done so affected stakeholders are always on-hand in case if the downtime and changes cause further issues.\n\n\nCustomer Problem Reporting\n\n\nProblems should be reported immediately at \nhttps://support.opensciencegrid.org\n\n\nResponsibilities\n\n\n\n\nThe Collector will be run by OSG Operations and accessible to HTCondor compute elements.\n\n\nProblems not associated with the hardware, operating system or network are the responsibilty of the Technology Group.\n\n\n\n\nService Measuring and Reporting\n\n\nThe GOC will provide the following reports in the intervals indicated (monthly, quarterly, semi-annually, or annually):\n\n\n\n\n\n\n\n\nReport Name\n\n\nReporting Interval\n\n\nDelivery Method\n\n\nResponsible Party\n\n\n\n\n\n\n\n\n\n\nSystem Availability and Reliability\n\n\nMonthly\n\n\nWeb Posting\n\n\nGOC\n\n\n\n\n\n\n\n\nSLA Validity Period\n\n\nThis SLA will be in affect for one year.\n\n\nSLA Review Procedure\n\n\nThis SLA will renew automatically on a yearly basis unless change or update is requested by the OSG Operations Coordinator, the OSG Executive Team or the Stakeholders.\n\n\nReferences\n\n\nAppendix A - Approval\n\n\n\n\n\n\n\n\nApproved By\n\n\nPosition\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<-- CONTENT MANAGEMENT PROJECT\n\n\n\n\nDEAR DOCUMENT OWNER\n\n\nThank you for claiming ownership for this document Please fill in your FirstLast name here:\n   * Local OWNER = RobQ\n\n\nPlease define the document area, choose one of the defined areas from the next line\nDOC_AREA = (ComputeElement|General|Trash/Trash/Integration|Monitoring|Operations|Security|Storage|Trash/Tier3|User|VO)\n   * Local DOC_AREA = Operations\n\n\ndefine the primary role the document serves, choose one of the defined roles from the next line\nDOC_ROLE = (Developer|Documenter|Scientist|Student|SysAdmin|VOManager)\n   * Local DOC_ROLE = VOManager\n\n\nPlease define the document type, choose one of the defined types from the next line\nDOC_TYPE = (HowTo|Installation|Knowledge|Navigation|Planning|Training|Troubleshooting)\n   * Local DOC_TYPE = Knowledge\n\n\nPlease define if this document in general needs to be reviewed before release ( \nYES\n | \nNO\n )\n   * Local INCLUDE_REVIEW = \nYES\n\n\nPlease define if this document in general needs to be tested before release ( \nYES\n | \nNO\n )\n   * Local INCLUDE_TEST = \nNO\n\n\nchange to \nYES\n once the document is ready to be reviewed and back to \nNO\n if that is not the case\n   * Local REVIEW_READY = \nYES\n\n\nchange to \nYES\n once the document is ready to be tested and back to \nNO\n if that is not the case\n   * Local TEST_READY = \nNO\n\n\nchange to \nYES\n only if the document has passed the review and the test (if applicable) and is ready for release\n   * Local RELEASE_READY = \nNO\n\n\nDEAR DOCUMENT REVIEWER\n\n\nThank for reviewing this document Please fill in your FirstLast name here:\n   * Local REVIEWER = ScottTeige\n\n\nPlease define the review status for this document to be in progress ( \nIN_PROGRESS\n ), failed ( \nNO\n ) or passed ( \nYES\n )\n   * Local REVIEW_PASSED = \nYES\n\n\nDEAR DOCUMENT TESTER\n\n\nThank for testing this document Please fill in your FirstLast name here:\n   * Local TESTER = ScottTeige\n\n\nPlease define the test status for this document to be in progress ( \nIN_PROGRESS\n ), failed ( \nNO\n ) or passed ( \nYES\n )\n   * Local TEST_PASSED = \nIN_PROGRESS\n\n\n\n\n-->",
            "title": "HTCondor Collector"
        },
        {
            "location": "/SLA/collector/#goc-condor-collector-service-level-agreement",
            "text": "",
            "title": "GOC Condor Collector Service Level Agreement"
        },
        {
            "location": "/SLA/collector/#about-this-document",
            "text": "This document details a service level agreement which outlines production expectations for the GOC Condor Collector and defines general support infrastructure for the service.",
            "title": "About This Document"
        },
        {
            "location": "/SLA/collector/#version-control",
            "text": "Version Number  Date  Author  Comments      1.1  9-24-2014  Scott Teige  First Draft",
            "title": "Version Control"
        },
        {
            "location": "/SLA/collector/#executive-summary",
            "text": "This SLA is an agreement between OSG and OSG Stakeholders pertaining to the operation of the OSG Condor Collector service.",
            "title": "Executive Summary"
        },
        {
            "location": "/SLA/collector/#owners",
            "text": "The Condor Collector SLA is owned by OSG Operations. It will be reviewed by and agreed upon by the OSG Executive Team.",
            "title": "Owners"
        },
        {
            "location": "/SLA/collector/#service-name-and-description",
            "text": "",
            "title": "Service Name and Description"
        },
        {
            "location": "/SLA/collector/#name",
            "text": "GOC Condor Collector",
            "title": "Name"
        },
        {
            "location": "/SLA/collector/#description",
            "text": "The grid-facing component of the HTCondor compute element system available for use by all OSG VOs.",
            "title": "Description"
        },
        {
            "location": "/SLA/collector/#security-considerations",
            "text": "All information associated with the service is covered by the Indiana University  privacy policy .\nEffect of compromise of this service are limited to the service. A compromise may make the service unusable by compute elements using it.",
            "title": "Security Considerations"
        },
        {
            "location": "/SLA/collector/#service-target-response-priorities-and-response-times",
            "text": "This section deals with unplanned outages. Please see  Requests for Service Enhancement  for information on planned maintenance outages.     Critical  High  Elevated  Normal      Description  * *  * *  * *    This service does not have critical priority  This service does not have high priority  This issue prevents all new sites from advertising in the info service or clients from querying the service. This issue causes intermittent drops of updates or query failures.  The issue prevents effective monitoring or full utilization of the Condor pool    Response Time  * *  * *  * *    N/A  N/A  Within 2 business days  Within 10 business days    Resolution Time  * *  * *  * *    N/A  N/A  The maximum acceptable resolution time is five (5) business days  The maximum acceptable resolution time is ten (10) business days    Escalates Every  * *  * *  * *    N/A  N/A  Five Days  Ten Days",
            "title": "Service Target Response Priorities and Response Times"
        },
        {
            "location": "/SLA/collector/#escalation-contacts",
            "text": "Escalation Level  OSG Contact      1st  OSG Operations Infrastructure Lead    2nd  OSG Operations Coordinator    3rd  OSG Production Coordinator    4th  OSG Technical Director and Executive Director     Critical Level Security Issues will be elevated immediately to the OSG Security Team.  Detailed information on contacts are viewable on the following  MyOSG URL , and are maintained within the  Any ongoing \"Normal\" or \"Elevated\" level issues will be discussed at the weekly  Operations  and  Production  meetings.",
            "title": "Escalation Contacts"
        },
        {
            "location": "/SLA/collector/#service-availability-and-outages",
            "text": "The GOC will strive for 90% service availability. If service availability falls below 90% monthly as monitored by the GOC a root cause analysis and service plan will be submitted to the OSG stakeholders for plans to restore an acceptable level of service availability.  Reliability and availability will be determined by 2 critical RSV probes and the OSG availability algorithm.\n   * One probe will verify the Condor collector is queryable whenever the host is pingable (i.e., reasonably not a network failure).\n   * The other probe will verify an automatically generated configuration file has been updated recently",
            "title": "Service Availability and Outages"
        },
        {
            "location": "/SLA/collector/#service-support-hours",
            "text": "This service will be run for 24x7, but support will primarily be within business hours. The exception is for security incidents.",
            "title": "Service Support Hours"
        },
        {
            "location": "/SLA/collector/#service-off-hours-support-procedures",
            "text": "All operational issues should be reported as per  Customer Problem Reporting  section.",
            "title": "Service Off-Hours Support Procedures"
        },
        {
            "location": "/SLA/collector/#requests-for-service-enhancements",
            "text": "OSG Operations will provide enhancement capabilities only as they are released by the HTCondor project or OSG Technology Group.\nRequests for customization of the deployed service may be made; OSG Operations has the right of up to one month of testing of any change.  OSG will give 1 week of warning prior to any change in the Collector service version.\nAt any time during this week, Stakeholders are permitted to request a delay for up to 1 week after the originally scheduled upgrade.  OSG Operations will schedule downtimes and configuration changes during normal business hours unless approved by affected stakeholders. This is done so affected stakeholders are always on-hand in case if the downtime and changes cause further issues.",
            "title": "Requests for Service Enhancements"
        },
        {
            "location": "/SLA/collector/#customer-problem-reporting",
            "text": "Problems should be reported immediately at  https://support.opensciencegrid.org",
            "title": "Customer Problem Reporting"
        },
        {
            "location": "/SLA/collector/#responsibilities",
            "text": "The Collector will be run by OSG Operations and accessible to HTCondor compute elements.  Problems not associated with the hardware, operating system or network are the responsibilty of the Technology Group.",
            "title": "Responsibilities"
        },
        {
            "location": "/SLA/collector/#service-measuring-and-reporting",
            "text": "The GOC will provide the following reports in the intervals indicated (monthly, quarterly, semi-annually, or annually):     Report Name  Reporting Interval  Delivery Method  Responsible Party      System Availability and Reliability  Monthly  Web Posting  GOC",
            "title": "Service Measuring and Reporting"
        },
        {
            "location": "/SLA/collector/#sla-validity-period",
            "text": "This SLA will be in affect for one year.",
            "title": "SLA Validity Period"
        },
        {
            "location": "/SLA/collector/#sla-review-procedure",
            "text": "This SLA will renew automatically on a yearly basis unless change or update is requested by the OSG Operations Coordinator, the OSG Executive Team or the Stakeholders.",
            "title": "SLA Review Procedure"
        },
        {
            "location": "/SLA/collector/#references",
            "text": "",
            "title": "References"
        },
        {
            "location": "/SLA/collector/#appendix-a-approval",
            "text": "Approved By  Position  Date            <-- CONTENT MANAGEMENT PROJECT",
            "title": "Appendix A - Approval"
        },
        {
            "location": "/SLA/collector/#dear-document-owner",
            "text": "Thank you for claiming ownership for this document Please fill in your FirstLast name here:\n   * Local OWNER = RobQ  Please define the document area, choose one of the defined areas from the next line\nDOC_AREA = (ComputeElement|General|Trash/Trash/Integration|Monitoring|Operations|Security|Storage|Trash/Tier3|User|VO)\n   * Local DOC_AREA = Operations  define the primary role the document serves, choose one of the defined roles from the next line\nDOC_ROLE = (Developer|Documenter|Scientist|Student|SysAdmin|VOManager)\n   * Local DOC_ROLE = VOManager  Please define the document type, choose one of the defined types from the next line\nDOC_TYPE = (HowTo|Installation|Knowledge|Navigation|Planning|Training|Troubleshooting)\n   * Local DOC_TYPE = Knowledge  Please define if this document in general needs to be reviewed before release (  YES  |  NO  )\n   * Local INCLUDE_REVIEW =  YES  Please define if this document in general needs to be tested before release (  YES  |  NO  )\n   * Local INCLUDE_TEST =  NO  change to  YES  once the document is ready to be reviewed and back to  NO  if that is not the case\n   * Local REVIEW_READY =  YES  change to  YES  once the document is ready to be tested and back to  NO  if that is not the case\n   * Local TEST_READY =  NO  change to  YES  only if the document has passed the review and the test (if applicable) and is ready for release\n   * Local RELEASE_READY =  NO",
            "title": "DEAR DOCUMENT OWNER"
        },
        {
            "location": "/SLA/collector/#dear-document-reviewer",
            "text": "Thank for reviewing this document Please fill in your FirstLast name here:\n   * Local REVIEWER = ScottTeige  Please define the review status for this document to be in progress (  IN_PROGRESS  ), failed (  NO  ) or passed (  YES  )\n   * Local REVIEW_PASSED =  YES",
            "title": "DEAR DOCUMENT REVIEWER"
        },
        {
            "location": "/SLA/collector/#dear-document-tester",
            "text": "Thank for testing this document Please fill in your FirstLast name here:\n   * Local TESTER = ScottTeige  Please define the test status for this document to be in progress (  IN_PROGRESS  ), failed (  NO  ) or passed (  YES  )\n   * Local TEST_PASSED =  IN_PROGRESS",
            "title": "DEAR DOCUMENT TESTER"
        },
        {
            "location": "/SLA/myosg/",
            "text": "MyOSG Service Level Agreement\n\n\nVersion Control\n\n\n\n\n\n\n\n\nVersion Number\n\n\nDate\n\n\nAuthor\n\n\nComments\n\n\n\n\n\n\n\n\n\n\n1.1\n\n\n5-26-2009\n\n\nRob Quick\n\n\nFirst Draft\n\n\n\n\n\n\n1.2\n\n\n9-10-2009\n\n\nRob Quick\n\n\nSecond Draft\n\n\n\n\n\n\n1.3\n\n\n9-15-2009\n\n\nRob Quick\n\n\nFinal Draft, Ready for Circulation\n\n\n\n\n\n\n1.4\n\n\n11-30-2009\n\n\nRob Quick\n\n\nFNAL Updates\n\n\n\n\n\n\n\n\nExecutive Summary\n\n\nThis SLA is an agreement between OSG Operations at Indiana University and the OSG Management describing details of the MyOSG presentation tools. The MyOSG service runs on hardware at Indiana University and gathers data from several OSG sources including but not limited to: RSV, OIM, BDII, GIP-Validator, and Gratia. MyOSG displays information from many different sources in many different formats. This SLA is inclusive only of selected service used to provide job level decisions or data movement, most notably the XML status based information. A complete list of services presented is in Appendix B.\n\n\nOwners\n\n\nThe MyOSG SLA is owned by OSG Operations Center and Indiana University and will be reviewed and agreed upon by the OSG Executive Team.\n\n\nService Name and Description\n\n\nName\n\n\nGOC Production MyOSG\n\n\nDescription\n\n\nThe GOC Production MyOSG service presents information from several OSG information sources and presents the information in various forms including web page, Universal Widget API, and XML. The MyOSG service consists of a web server and software consolidators to translate incoming data from several sources to be displayed in MyOSG.\n\n\nSecurity Considerations\n\n\nMyOSG distributes some private contact information based on certificate authentication. It will be subject to Indiana University institutional policy. Direct access to any of the hardware or software will be restricted to OSG Operations staff. Other OSG staff, upon request and GOC review, may be given access to a development version of MyOSG for experimenting with new software, changes in information providers, or for other testing. OSG Operations reserves the right to allow or not allow such requests based on its internal review.\n\n\nService Target Response Priorities and Response Times\n\n\nThis section deals with unplanned outages. Please see \nRequests for Service Enhancement\n for information on planned maintenance outages.\n\n\n\n\n\n\n\n\nCritical\n\n\nHigh\n\n\nElevated\n\n\nNormal\n\n\n\n\n\n\n\n\n\n\nWork Outage\n\n\n* *\n\n\n* *\n\n\n* *\n\n\n\n\n\n\nThe issue causes a full service outage rendering the status information provided by MyOSG used by any other OSG services to move data or submit jobs\n\n\nThe issue causes a full service outage rendering the status information provided by MyOSG used by any other OSG services to move data or submit jobs\n\n\nThe issue causes short (less than 15 minute) periods of unstable or inconsistent performance\n\n\nThe issue causes minor (less than 5 minutes) periods of unstable or inconsistent performance\n\n\n\n\n\n\nNumber of Clients Affected\n\n\n* *\n\n\n* *\n\n\n* *\n\n\n\n\n\n\nThe issue affects all MyOSG users\n\n\nThe issue affects a subset of MyOSG users\n\n\nThe issue may or may not affect all MyOSG users\n\n\nThe issue affects only a small number of MyOSG users\n\n\n\n\n\n\nWorkaround\n\n\n* *\n\n\n* *\n\n\n* *\n\n\n\n\n\n\nNo workaround will be necessary\n\n\nNo workaround will be necessary\n\n\nNo workaround will be necessary\n\n\nNo workaround will be necessary\n\n\n\n\n\n\nResponse Time\n\n\n* *\n\n\n* *\n\n\n* *\n\n\n\n\n\n\nWithin one (1) hour\n\n\nIssue will be addressed by the next business day\n\n\nWithin the next business day\n\n\nWithin the next business day\n\n\n\n\n\n\nResolution Time\n\n\n* *\n\n\n* *\n\n\n* *\n\n\n\n\n\n\nThe maximum acceptable resolution time is four (4) continuous hours, after initial response time\n\n\nThe maximum acceptable resolution time is 24 continuous hours, after the initial response time\n\n\nThe maximum acceptable resolution time is five (7) business days\n\n\nThe maximum acceptable resolution time is (30) business days\n\n\n\n\n\n\nEscalates Every\n\n\n* *\n\n\n* *\n\n\n* *\n\n\n\n\n\n\nOne Hour\n\n\nTwo Hour\n\n\nOne Day\n\n\nOne Week\n\n\n\n\n\n\n\n\nEscalation Contacts\n\n\n\n\n\n\n\n\nEscalation Level\n\n\nOSG Contact\n\n\n\n\n\n\n\n\n\n\n1st\n\n\nOSG Operations Infrastructure Lead\n\n\n\n\n\n\n2nd\n\n\nOSG Operations Coordinator\n\n\n\n\n\n\n3rd\n\n\nOSG Production Coordinator\n\n\n\n\n\n\n4th\n\n\nOSG Technical Director and Executive Director\n\n\n\n\n\n\n\n\nDetailed information on contacts are viewable on the following \nMyOSG URL\n, and are maintained within\nthe \nOSG Information Management\n system (for editing purposes only).\n\n\nAny ongoing \"Normal\" or \"Elevated\" level issues will be discussed at the weekly \nOperations\n and \nProduction\n meetings.\n\n\nService Availability and Outages\n\n\nThe GOC will strive for 99% service availability. If service availability falls below 99% monthly as monitored by the GOC on two consecutive months a root cause analysis and service plan will be submitted to the OSG stakeholders for plans to restore an acceptable level of service availability.\n\n\nA maximum of two non-scheduled outages will be accepted by OSG during each six month period of service. If the GOC experiences more than the allotted outage, a service plan will be submitted to the OSG stakeholders with plans to restore the service to an acceptable level of operations.\n\n\nService Support Hours\n\n\nThe MyOSG service is supported 24x7 by the GOC and Indiana University. Critical and High level issues will result in response within (1) hour. All other issues will be investigated by the next business day.\n\n\nService Off-Hours Support Procedures\n\n\nAll MyOSG issues should be reported to the OSG immediately by \ntrouble ticket\n web submission. If the problem is deemed critical, a GOC staff member will be alerted immediately.\n\n\nRequests for Service Enhancement\n\n\nThis section deals with planned maintenance outages. Please see \nService Target Response Priorities and Response Times\n for information on unplanned outages.\n\n\nThe OSG Operations will respond to customer requests for service enhancements based on GOC determination of the necessity and desirability of the enhancement. No alteration or deletions will be brought to the production MyOSG without a minimum of ten (10) business days of testing. New human-interface features or new machine readable interfaces may be brought to production with five (5) business days of testing from the feature requester. The GOC reserves the right to enhance the physical environment of the service based on IU and GOC needs. No enhancement will occur without advanced notice to the OSG community.\n\n\nCustomer Problem Reporting\n\n\nMyOSG problems should be reported immediately by creating a problem ticket at \nhttps://support.opensciencegrid.org\n\n\nResponsibilities\n\n\nCustomer Responsibilities\n\n\nMyOSG Customers agree to:\n\n\n\n\nUse the MyOSG to gather information about OSG resources for purposes of VO approved work only.\n\n\nAlert the GOC if they are going to use the MyOSG in a non-standard way, this includes testing or anticipated mass increases in usage.\n\n\nContact the GOC by means outlined in the \nCustomer Problem Reporting\n section of this document if they encounter any service issues.\n\n\nBe willing and available to provide critical information within one hour of reporting a critical or high priority incident or one business day for any other criticality.\n\n\nProvide testing for MyOSG-ITB within the time frame defined in the \nRequests for Service Enhancements\n section.\n\n\nAlert the GOC and MyOSG developers when problems are encountered during testing.\n\n\nThe customer or GOC will be allowed to request an additional ten (10) business days of testing if problems are encountered or testing is not completed at then end of the initial ten (10) business day testing period. The customer must alert the GOC two (2) business days before the scheduled release if they would like this additional testing time.\n\n\n\n\nOSG Operations Responsibilities\n\n\nGeneral responsibilities:\n   * Create and add appropriate documentation to the OSG TWiki for appropriate use of MyOSG.\n   * Meet response times associated with the priority assigned to Customer issues.\n   * Maintain appropriately trained staff.\n   * Notify the community of any changes to the machine readable MyOSG pages without (20) business days notification. Notification will be sent to the OSG-Operations mailing list and to each OSG Support Center, along with discussion in the \nOSG Operations Meeting\n.\n   * Operations will provide backward compatible views to the best of our ability, due to changes requested in OIM or MyOSG by OSG stakeholders this can not be guaranteed.\n   * Insure the compatibility of downtime XML data for six (6) months.\n   * The OSG and GOC are not responsible if a customer does not provide testing during the testing period. In such cases, the GOC has final discretion in what remedial actions to take.\n\n\nGOC Service Desk Responsibilities:\n   * Log and track all Customer requests for service through the OSG ticketing system.\n\n\nDatabase & Application Services responsibilities:\n   * Schedule maintenance (downtime) outside of normal business hours (Eastern Time) unless circumstances warrant performing maintenance at another time.\n   * Announce and negotiate maintenance with stakeholders to assure minimal interruption to production workload.\n   * Alert the community of scheduled maintenance periods at least five business days prior to the start of a service affecting maintenance window.\n\n\nService Measuring and Reporting\n\n\nThe GOC will provide the customer with the following reports in the intervals indicated (monthly, quarterly, semi-annually, or annually):\n\n\n\n\n\n\n\n\nReport Name\n\n\nReporting Interval\n\n\nDelivery Method\n\n\nResponsible Party\n\n\n\n\n\n\n\n\n\n\nSystem Uptime\n\n\nMonthly\n\n\nWeb Posting\n\n\nGOC\n\n\n\n\n\n\nService Uptime\n\n\nMonthly\n\n\nWeb Posting\n\n\nGOC\n\n\n\n\n\n\nReport of Critical and High Priority Issues\n\n\nQuarterly\n\n\nWeb Posting\n\n\nGOC\n\n\n\n\n\n\n\n\nSLA Validity Period\n\n\nThis SLA will be in affect for one year.\n\n\nSLA Review Procedure\n\n\nThis SLA will renew automatically on a yearly basis unless change or update is requested by the OSG Operations Coordinator, the OSG Executive Team or the Stakeholders.\n\n\nReferences\n\n\nAppendix A - Customer Information\n\n\nAll MyOSG end-users who are members of an OSG VO are considered customers.\n\n\nAppendix B - Other Service Dependencies\n\n\nMyOSG is critically dependent on the following services to serve status information:\n   * Local Network, Hardware, OS, Apache and MySQL on the MyOSG instance\n\n\nMyOSG using the following OSG services to display information but is not directly dependent on their uptime:\n   * OIM (Dependent on Replicated OIM Instance; if OIM is down, cached information is used)\n   * RSV Data Collection (Dependent on RSV results; if RSV collector or site-RSV-clients are down, then cached information is used)\n   * BDII\n   * GIP-Validator\n   * Gratia (Nebraska Interface)\n\n\nAppendix C - Supported Hardware and Software\n\n\nSupported Hardware\n\n\nThe following hardware is supported:\n   * Physical devices used to provide the MyOSG service.\n   * Physical devices used to provide the environment used to house the MyOSG service.\n\n\nHardware Services\n\n\nThe following hardware services are provided:\n   * Recommendations. OSG Operations will be responsible for specifying and recommending for purchase or lease hardware meeting customers' needs.\n   * Installation. OSG Operations will install, configure and customize system hardware and operating systems.\n   * Upgrades. OSG Operations is responsible for specifying and recommending for purchase any hardware upgrades.\n   * Diagnosis. OSG Operations will diagnose problems with service related hardware.\n   * Repair. OSG Operations analysts are not hardware technicians and receive no training in hardware maintenance, nor do we have the test equipment and tools necessary to do such work.\n\n\nPerforming repairs under warranty: Any work to be performed under warranty may be referred to the warranty service provider at the discretion of the Service Provider analyst(s). Service Provider analysts will not undertake work that will void warranties on customer hardware unless specifically requested and authorized by customer's management in writing.\n\n\nObtaining repair services: The Service Provider analyst will recommend a service vendor whenever he/she feels the repair work requires specialized skills or tools.\n\n\n\n\nBackup. Service Provider agrees to fully back up all Service Provider-supported software and data nightly every business day.\n\n\n\n\nSoftware Services\n\n\nService Provider agrees to cover software support services, including software installations and upgrades. All software maintenance periods will be announced via the policy put forth\nin the \nOSG Operations Responsibilities\n section of this document.\n\n\nSoftware Costs\n\n\nIU and the Grid Operations Center bears all costs for new and replacement software.\n\n\nAppendix D - Approval\n\n\n\n\n\n\n\n\nApproved By\n\n\nPosition\n\n\nDate\n\n\n\n\n\n\n\n\n\n\nRob Quick\n\n\nOSG Operations Coordinator\n\n\n11-10-09\n\n\n\n\n\n\nAbhishek Singh Rana \n (Based on public review by VOs; \n input from Fermilab-VO & Gratia, \n NYSGrid, SBGrid, STAR)\n\n\nVOs Group Coordinator\n\n\n12-04-2009\n\n\n\n\n\n\nBrian Bockelman\n\n\nMyOSG Liason / Measurements and Metrics\n\n\n11-10-2009\n\n\n\n\n\n\n\n\nAppendix E - Availability and Reliability Publishing\n\n\n\n\n\n\n\n\nMonth\n\n\nYear\n\n\nAvailability\n\n\nReliability\n\n\n\n\n\n\n\n\n\n\nMay\n\n\n2010\n\n\n100.00%\n\n\n100.00%\n\n\n\n\n\n\nJune\n\n\n2010\n\n\n100.00%\n\n\n100.00%\n\n\n\n\n\n\nJuly\n\n\n2010\n\n\n99.98%\n\n\n99.98%\n\n\n\n\n\n\nAugust\n\n\n2010\n\n\n99.93%\n\n\n99.93%\n\n\n\n\n\n\nSeptember\n\n\n2010\n\n\n99.95%\n\n\n100.00%\n\n\n\n\n\n\nOctober\n\n\n2010\n\n\n99.98%\n\n\n99.98%\n\n\n\n\n\n\nNovember\n\n\n2010\n\n\n99.91%\n\n\n99.91%\n\n\n\n\n\n\nDecember\n\n\n2010\n\n\n99.98%\n\n\n99.98%\n\n\n\n\n\n\nJanuary\n\n\n2011\n\n\n99.98%\n\n\n99.98%\n\n\n\n\n\n\nFebruary\n\n\n2011\n\n\n99.93%\n\n\n99.93%\n\n\n\n\n\n\nMarch\n\n\n2011\n\n\n99.91%\n\n\n99.91%\n\n\n\n\n\n\nApril\n\n\n2011\n\n\n99.98%\n\n\n99.98%\n\n\n\n\n\n\n\n\n\n\n[[ServiceLevelAgreements#Supporting_Documents][Recent availability statistics]]",
            "title": "MyOSG"
        },
        {
            "location": "/SLA/myosg/#myosg-service-level-agreement",
            "text": "",
            "title": "MyOSG Service Level Agreement"
        },
        {
            "location": "/SLA/myosg/#version-control",
            "text": "Version Number  Date  Author  Comments      1.1  5-26-2009  Rob Quick  First Draft    1.2  9-10-2009  Rob Quick  Second Draft    1.3  9-15-2009  Rob Quick  Final Draft, Ready for Circulation    1.4  11-30-2009  Rob Quick  FNAL Updates",
            "title": "Version Control"
        },
        {
            "location": "/SLA/myosg/#executive-summary",
            "text": "This SLA is an agreement between OSG Operations at Indiana University and the OSG Management describing details of the MyOSG presentation tools. The MyOSG service runs on hardware at Indiana University and gathers data from several OSG sources including but not limited to: RSV, OIM, BDII, GIP-Validator, and Gratia. MyOSG displays information from many different sources in many different formats. This SLA is inclusive only of selected service used to provide job level decisions or data movement, most notably the XML status based information. A complete list of services presented is in Appendix B.",
            "title": "Executive Summary"
        },
        {
            "location": "/SLA/myosg/#owners",
            "text": "The MyOSG SLA is owned by OSG Operations Center and Indiana University and will be reviewed and agreed upon by the OSG Executive Team.",
            "title": "Owners"
        },
        {
            "location": "/SLA/myosg/#service-name-and-description",
            "text": "",
            "title": "Service Name and Description"
        },
        {
            "location": "/SLA/myosg/#name",
            "text": "GOC Production MyOSG",
            "title": "Name"
        },
        {
            "location": "/SLA/myosg/#description",
            "text": "The GOC Production MyOSG service presents information from several OSG information sources and presents the information in various forms including web page, Universal Widget API, and XML. The MyOSG service consists of a web server and software consolidators to translate incoming data from several sources to be displayed in MyOSG.",
            "title": "Description"
        },
        {
            "location": "/SLA/myosg/#security-considerations",
            "text": "MyOSG distributes some private contact information based on certificate authentication. It will be subject to Indiana University institutional policy. Direct access to any of the hardware or software will be restricted to OSG Operations staff. Other OSG staff, upon request and GOC review, may be given access to a development version of MyOSG for experimenting with new software, changes in information providers, or for other testing. OSG Operations reserves the right to allow or not allow such requests based on its internal review.",
            "title": "Security Considerations"
        },
        {
            "location": "/SLA/myosg/#service-target-response-priorities-and-response-times",
            "text": "This section deals with unplanned outages. Please see  Requests for Service Enhancement  for information on planned maintenance outages.     Critical  High  Elevated  Normal      Work Outage  * *  * *  * *    The issue causes a full service outage rendering the status information provided by MyOSG used by any other OSG services to move data or submit jobs  The issue causes a full service outage rendering the status information provided by MyOSG used by any other OSG services to move data or submit jobs  The issue causes short (less than 15 minute) periods of unstable or inconsistent performance  The issue causes minor (less than 5 minutes) periods of unstable or inconsistent performance    Number of Clients Affected  * *  * *  * *    The issue affects all MyOSG users  The issue affects a subset of MyOSG users  The issue may or may not affect all MyOSG users  The issue affects only a small number of MyOSG users    Workaround  * *  * *  * *    No workaround will be necessary  No workaround will be necessary  No workaround will be necessary  No workaround will be necessary    Response Time  * *  * *  * *    Within one (1) hour  Issue will be addressed by the next business day  Within the next business day  Within the next business day    Resolution Time  * *  * *  * *    The maximum acceptable resolution time is four (4) continuous hours, after initial response time  The maximum acceptable resolution time is 24 continuous hours, after the initial response time  The maximum acceptable resolution time is five (7) business days  The maximum acceptable resolution time is (30) business days    Escalates Every  * *  * *  * *    One Hour  Two Hour  One Day  One Week",
            "title": "Service Target Response Priorities and Response Times"
        },
        {
            "location": "/SLA/myosg/#escalation-contacts",
            "text": "Escalation Level  OSG Contact      1st  OSG Operations Infrastructure Lead    2nd  OSG Operations Coordinator    3rd  OSG Production Coordinator    4th  OSG Technical Director and Executive Director     Detailed information on contacts are viewable on the following  MyOSG URL , and are maintained within\nthe  OSG Information Management  system (for editing purposes only).  Any ongoing \"Normal\" or \"Elevated\" level issues will be discussed at the weekly  Operations  and  Production  meetings.",
            "title": "Escalation Contacts"
        },
        {
            "location": "/SLA/myosg/#service-availability-and-outages",
            "text": "The GOC will strive for 99% service availability. If service availability falls below 99% monthly as monitored by the GOC on two consecutive months a root cause analysis and service plan will be submitted to the OSG stakeholders for plans to restore an acceptable level of service availability.  A maximum of two non-scheduled outages will be accepted by OSG during each six month period of service. If the GOC experiences more than the allotted outage, a service plan will be submitted to the OSG stakeholders with plans to restore the service to an acceptable level of operations.",
            "title": "Service Availability and Outages"
        },
        {
            "location": "/SLA/myosg/#service-support-hours",
            "text": "The MyOSG service is supported 24x7 by the GOC and Indiana University. Critical and High level issues will result in response within (1) hour. All other issues will be investigated by the next business day.",
            "title": "Service Support Hours"
        },
        {
            "location": "/SLA/myosg/#service-off-hours-support-procedures",
            "text": "All MyOSG issues should be reported to the OSG immediately by  trouble ticket  web submission. If the problem is deemed critical, a GOC staff member will be alerted immediately.",
            "title": "Service Off-Hours Support Procedures"
        },
        {
            "location": "/SLA/myosg/#requests-for-service-enhancement",
            "text": "This section deals with planned maintenance outages. Please see  Service Target Response Priorities and Response Times  for information on unplanned outages.  The OSG Operations will respond to customer requests for service enhancements based on GOC determination of the necessity and desirability of the enhancement. No alteration or deletions will be brought to the production MyOSG without a minimum of ten (10) business days of testing. New human-interface features or new machine readable interfaces may be brought to production with five (5) business days of testing from the feature requester. The GOC reserves the right to enhance the physical environment of the service based on IU and GOC needs. No enhancement will occur without advanced notice to the OSG community.",
            "title": "Requests for Service Enhancement"
        },
        {
            "location": "/SLA/myosg/#customer-problem-reporting",
            "text": "MyOSG problems should be reported immediately by creating a problem ticket at  https://support.opensciencegrid.org",
            "title": "Customer Problem Reporting"
        },
        {
            "location": "/SLA/myosg/#responsibilities",
            "text": "",
            "title": "Responsibilities"
        },
        {
            "location": "/SLA/myosg/#customer-responsibilities",
            "text": "MyOSG Customers agree to:   Use the MyOSG to gather information about OSG resources for purposes of VO approved work only.  Alert the GOC if they are going to use the MyOSG in a non-standard way, this includes testing or anticipated mass increases in usage.  Contact the GOC by means outlined in the  Customer Problem Reporting  section of this document if they encounter any service issues.  Be willing and available to provide critical information within one hour of reporting a critical or high priority incident or one business day for any other criticality.  Provide testing for MyOSG-ITB within the time frame defined in the  Requests for Service Enhancements  section.  Alert the GOC and MyOSG developers when problems are encountered during testing.  The customer or GOC will be allowed to request an additional ten (10) business days of testing if problems are encountered or testing is not completed at then end of the initial ten (10) business day testing period. The customer must alert the GOC two (2) business days before the scheduled release if they would like this additional testing time.",
            "title": "Customer Responsibilities"
        },
        {
            "location": "/SLA/myosg/#osg-operations-responsibilities",
            "text": "General responsibilities:\n   * Create and add appropriate documentation to the OSG TWiki for appropriate use of MyOSG.\n   * Meet response times associated with the priority assigned to Customer issues.\n   * Maintain appropriately trained staff.\n   * Notify the community of any changes to the machine readable MyOSG pages without (20) business days notification. Notification will be sent to the OSG-Operations mailing list and to each OSG Support Center, along with discussion in the  OSG Operations Meeting .\n   * Operations will provide backward compatible views to the best of our ability, due to changes requested in OIM or MyOSG by OSG stakeholders this can not be guaranteed.\n   * Insure the compatibility of downtime XML data for six (6) months.\n   * The OSG and GOC are not responsible if a customer does not provide testing during the testing period. In such cases, the GOC has final discretion in what remedial actions to take.  GOC Service Desk Responsibilities:\n   * Log and track all Customer requests for service through the OSG ticketing system.  Database & Application Services responsibilities:\n   * Schedule maintenance (downtime) outside of normal business hours (Eastern Time) unless circumstances warrant performing maintenance at another time.\n   * Announce and negotiate maintenance with stakeholders to assure minimal interruption to production workload.\n   * Alert the community of scheduled maintenance periods at least five business days prior to the start of a service affecting maintenance window.",
            "title": "OSG Operations Responsibilities"
        },
        {
            "location": "/SLA/myosg/#service-measuring-and-reporting",
            "text": "The GOC will provide the customer with the following reports in the intervals indicated (monthly, quarterly, semi-annually, or annually):     Report Name  Reporting Interval  Delivery Method  Responsible Party      System Uptime  Monthly  Web Posting  GOC    Service Uptime  Monthly  Web Posting  GOC    Report of Critical and High Priority Issues  Quarterly  Web Posting  GOC",
            "title": "Service Measuring and Reporting"
        },
        {
            "location": "/SLA/myosg/#sla-validity-period",
            "text": "This SLA will be in affect for one year.",
            "title": "SLA Validity Period"
        },
        {
            "location": "/SLA/myosg/#sla-review-procedure",
            "text": "This SLA will renew automatically on a yearly basis unless change or update is requested by the OSG Operations Coordinator, the OSG Executive Team or the Stakeholders.",
            "title": "SLA Review Procedure"
        },
        {
            "location": "/SLA/myosg/#references",
            "text": "",
            "title": "References"
        },
        {
            "location": "/SLA/myosg/#appendix-a-customer-information",
            "text": "All MyOSG end-users who are members of an OSG VO are considered customers.",
            "title": "Appendix A - Customer Information"
        },
        {
            "location": "/SLA/myosg/#appendix-b-other-service-dependencies",
            "text": "MyOSG is critically dependent on the following services to serve status information:\n   * Local Network, Hardware, OS, Apache and MySQL on the MyOSG instance  MyOSG using the following OSG services to display information but is not directly dependent on their uptime:\n   * OIM (Dependent on Replicated OIM Instance; if OIM is down, cached information is used)\n   * RSV Data Collection (Dependent on RSV results; if RSV collector or site-RSV-clients are down, then cached information is used)\n   * BDII\n   * GIP-Validator\n   * Gratia (Nebraska Interface)",
            "title": "Appendix B - Other Service Dependencies"
        },
        {
            "location": "/SLA/myosg/#appendix-c-supported-hardware-and-software",
            "text": "",
            "title": "Appendix C - Supported Hardware and Software"
        },
        {
            "location": "/SLA/myosg/#supported-hardware",
            "text": "The following hardware is supported:\n   * Physical devices used to provide the MyOSG service.\n   * Physical devices used to provide the environment used to house the MyOSG service.",
            "title": "Supported Hardware"
        },
        {
            "location": "/SLA/myosg/#hardware-services",
            "text": "The following hardware services are provided:\n   * Recommendations. OSG Operations will be responsible for specifying and recommending for purchase or lease hardware meeting customers' needs.\n   * Installation. OSG Operations will install, configure and customize system hardware and operating systems.\n   * Upgrades. OSG Operations is responsible for specifying and recommending for purchase any hardware upgrades.\n   * Diagnosis. OSG Operations will diagnose problems with service related hardware.\n   * Repair. OSG Operations analysts are not hardware technicians and receive no training in hardware maintenance, nor do we have the test equipment and tools necessary to do such work.  Performing repairs under warranty: Any work to be performed under warranty may be referred to the warranty service provider at the discretion of the Service Provider analyst(s). Service Provider analysts will not undertake work that will void warranties on customer hardware unless specifically requested and authorized by customer's management in writing.  Obtaining repair services: The Service Provider analyst will recommend a service vendor whenever he/she feels the repair work requires specialized skills or tools.   Backup. Service Provider agrees to fully back up all Service Provider-supported software and data nightly every business day.",
            "title": "Hardware Services"
        },
        {
            "location": "/SLA/myosg/#software-services",
            "text": "Service Provider agrees to cover software support services, including software installations and upgrades. All software maintenance periods will be announced via the policy put forth\nin the  OSG Operations Responsibilities  section of this document.",
            "title": "Software Services"
        },
        {
            "location": "/SLA/myosg/#software-costs",
            "text": "IU and the Grid Operations Center bears all costs for new and replacement software.",
            "title": "Software Costs"
        },
        {
            "location": "/SLA/myosg/#appendix-d-approval",
            "text": "Approved By  Position  Date      Rob Quick  OSG Operations Coordinator  11-10-09    Abhishek Singh Rana   (Based on public review by VOs;   input from Fermilab-VO & Gratia,   NYSGrid, SBGrid, STAR)  VOs Group Coordinator  12-04-2009    Brian Bockelman  MyOSG Liason / Measurements and Metrics  11-10-2009",
            "title": "Appendix D - Approval"
        },
        {
            "location": "/SLA/myosg/#appendix-e-availability-and-reliability-publishing",
            "text": "Month  Year  Availability  Reliability      May  2010  100.00%  100.00%    June  2010  100.00%  100.00%    July  2010  99.98%  99.98%    August  2010  99.93%  99.93%    September  2010  99.95%  100.00%    October  2010  99.98%  99.98%    November  2010  99.91%  99.91%    December  2010  99.98%  99.98%    January  2011  99.98%  99.98%    February  2011  99.93%  99.93%    March  2011  99.91%  99.91%    April  2011  99.98%  99.98%      [[ServiceLevelAgreements#Supporting_Documents][Recent availability statistics]]",
            "title": "Appendix E - Availability and Reliability Publishing"
        },
        {
            "location": "/SLA/oasis/",
            "text": "OASIS Service Level Agreement -- \nDRAFT\n\n\nVersion Control\n\n\n\n\n\n\n\n\nVersion Number\n\n\nDate\n\n\nAuthor\n\n\nComments\n\n\n\n\n\n\n\n\n\n\n0.1\n\n\n2-21-2013\n\n\nScott Teige\n\n\nFirst Draft\n\n\n\n\n\n\n0.2\n\n\n3-28-2013\n\n\nRob Quick\n\n\nRevision to First Draft\n\n\n\n\n\n\n0.3\n\n\n4-9-2013\n\n\nScott Teige\n\n\nRevision to Second Draft\n\n\n\n\n\n\n\n\nExecutive Summary\n\n\nThis SLA is an agreement between OSG Operations and the OSG Management and Stakeholders describing details of the OASIS software distribution system.\n\n\nOwners\n\n\nThis SLA is owned by OSG Operations and Indiana University and will be reviewed and agreed upon by the OSG Executive Team and OSG Stakeholders.\n\n\nService Name and Description\n\n\nName\n\n\nOASIS (OSG Application and Software Installation Service)\n\n\nDescription\n\n\nThe OASIS service provides OSG Virtual Organizations with a central location for application software. The content hosted on OASIS can be made available on OSG compute resources. The service consists of three virtual machines, a stratum 0 CERN Virtual Machine File System (CVMFS) server, a stratum 1 replica of the stratum 0 and a node accessible for login by VO OASIS managers.\n\n\nThis SLA covers the stratum 0 server and the interactive login node only. The stratum 1 SLA is available \nhere\n.\n\n\nSecurity Considerations\n\n\nThe OASIS stratum 0 server is accessible only by GOC and OSG technology group staff. The OASIS interactive node (oasis-login.opensciencegrid.org) is accessible by GOC and OSG technology group staff and to registered VO software managers via gsissh. A person becomes a VO software manager only by explicit approval of GOC staff within the OIM Topology Database.\n\n\nService Target Response Priorities and Response Times\n\n\nThis section deals with unplanned outages. Please see \nRequests for Service Enhancement\n for information on planned maintenance outages.\n\n\n\n\n\n\n\n\nCritical\n\n\nHigh\n\n\nElevated\n\n\nNormal\n\n\n\n\n\n\n\n\n\n\nWork Outage\n\n\n* *\n\n\n* *\n\n\n* *\n\n\n\n\n\n\nThe issue causes a compromise of the stratum 0\n\n\nThe issue causes a full service outage rendering the service unavailable\n\n\nThe issue causes short (less than 15 minute) periods of unstable or inconsistent performance\n\n\nThe issue causes minor (less than 5 minutes) periods of unstable or inconsistent performance\n\n\n\n\n\n\nNumber of Clients Affected\n\n\n* *\n\n\n* *\n\n\n* *\n\n\n\n\n\n\nThe issue affects all users or OSG resources\n\n\nThe issue affects all users\n\n\nThe issue may or may not affect all users\n\n\nThe issue affects only a small number of users\n\n\n\n\n\n\nResponse Time\n\n\n* *\n\n\n* *\n\n\n* *\n\n\n\n\n\n\nWithin one (1) hour\n\n\nWithin the next business day\n\n\nWithin the next business day\n\n\nWithin five (5) business days\n\n\n\n\n\n\nResolution Time\n\n\n* *\n\n\n* *\n\n\n* *\n\n\n\n\n\n\nThe maximum acceptable resolution time is four (4) continuous hours, after initial response time\n\n\nThe maximum acceptable resolution time is one full (1) business day\n\n\nThe maximum acceptable resolution time is five (5) business days\n\n\nThe maximum acceptable resolution time is thirty (30) business days\n\n\n\n\n\n\nEscalates Every\n\n\n* *\n\n\n* *\n\n\n* *\n\n\n\n\n\n\nOne hour\n\n\nOne Day\n\n\nOne Week\n\n\nOne Month\n\n\n\n\n\n\n\n\nEscalation Contacts\n\n\n\n\n\n\n\n\nEscalation Level\n\n\nOSG Contact\n\n\n\n\n\n\n\n\n\n\n1st\n\n\nOSG Operations Infrastructure Lead\n\n\n\n\n\n\n2nd\n\n\nOSG Operations Coordinator\n\n\n\n\n\n\n3rd\n\n\nOSG Production Coordinator\n\n\n\n\n\n\n4th\n\n\nOSG Technical Director and Executive Director\n\n\n\n\n\n\n\n\nDetailed information on contacts are viewable on the following \n2F15\n2F2009&end_type=now&end_date=09\n2F15\n2F2009&site_10047=on&rg=on&rg_336=on&gridtype=on&gridtype_1=on&active=on&active_value=1&disable_value=1\">MyOSG URL\n, and are maintained within the\n\n\nAny ongoing \"Normal\" or \"Elevated\" level issues will be discussed at the weekly \nOperations\n and \nProduction\n meetings.\n\n\nService Availability and Outages\n\n\nThe GOC will strive for 97% service availability. If service availability falls below 97% monthly as monitored by the GOC on two consecutive months a root cause analysis and service plan will be submitted to the OSG stakeholders for plans to restore an acceptable level of service availability.\n\n\nService Support Hours\n\n\nThe service is supported 24x7 by the GOC and Indiana University. All issues will be investigated by the next business day.\n\n\nService Off-Hours Support Procedures\n\n\nUsers should contact the OSG \nhttps://support.opensciencegrid.org\n.\n\n\nRequests for Service Enhancements\n\n\nThis section deals with planned maintenance outages. Please see \nService Target Response Priorities and Response Times\n for information on unplanned outages.\n\n\nThe OSG Operations will respond to customer requests for service enhancements based on GOC determination of the necessity and desirability of the enhancement. The GOC reserves the right to enhance the physical environment of the service based on IU and GOC needs. No enhancement will occur without advanced notice to the OSG community.\n\n\nCustomer Problem Reporting\n\n\nThe GOC provides operators 24x7x365. Service problems should be reported immediately by one of the following mechanisms.\n\n\n\n\nCreating a problem ticket at https://ticket.grid.iu.edu/goc/submit (\npreferred\n)\n\n\nEmailing a description to goc@opensciencegrid.org\n\n\nCalling the GOC phone at 317-278-9699\n\n\n\n\nResponsibilities\n\n\nCustomer Responsibilities\n\n\nOASIS customers agree to:\n\n\n\n\nUse the service for purposes of OSG approved work only.\n\n\nAlert the GOC if they are going to use the Service in a non-standard way, this includes testing or anticipated mass increases in usage.\n\n\nContact support by means outlined in the \nCustomer Problem Reporting\n section of this document if they encounter any service issues.\n\n\nBe willing and available to provide information within one business day for any High level issues reported.\n\n\n\n\nResponsibilities\n\n\nGOC operations:\n   * Maintain the physical machine hosting the service\n   * Assure the service is accessible via its advertised URL\n   * Make changes and updates within the normal GOC \nrelease schedule\n\n   * Meet response times associated with the priority assigned by users.\n   * Maintain appropriately trained staff.\n\n\nGOC Service Desk Responsibilities:\n   * Log and track all Customer requests for service through the OSG ticketing system.\n\n\nDatabase & Application Services responsibilities:\n   * Announce and negotiate maintenance with stakeholders to assure minimal interruption to normal workload.\n   * Alert the community of scheduled maintenance periods at least five (5) business days prior to the start of a service affecting maintenance window.\n\n\nService Measuring and Reporting\n\n\nThe GOC will provide the customer with the following reports in the intervals indicated (monthly, quarterly, semi-annually, or annually):\n\n\n\n\n\n\n\n\nReport Name\n\n\nReporting Interval\n\n\nDelivery Method\n\n\nResponsible Party\n\n\n\n\n\n\n\n\n\n\nSystem Uptime\n\n\nMonthly\n\n\nWeb Posting\n\n\nGOC\n\n\n\n\n\n\nService Uptime\n\n\nMonthly\n\n\nWeb Posting\n\n\nGOC\n\n\n\n\n\n\nReport of Critical and High Priority Issues\n\n\nQuarterly\n\n\nWeb Posting\n\n\nGOC\n\n\n\n\n\n\n\n\nThese reports will be posted in Appendix E of this document.\n\n\nSLA Validity Period\n\n\nThis SLA will be in affect for one year.\n\n\nSLA Review Procedure\n\n\nThis SLA will renew automatically on a yearly basis unless change or update is requested by the OSG Operations Coordinator, the OSG Executive Team or the Stakeholders.\n\n\nReferences\n\n\nAppendix A - Customer Information\n\n\nAll service end-users who are members of an OASIS enabled VO are considered customers.\nAny VO can be OASIS enabled on request, larger sustained VOs should consider setting up individual OASIS services.\n\n\nAppendix B - Other Service Dependencies\n\n\nThe service is dependent on the following services to collect and distribute information:\n   * Local Network and Hardware\n\n\nAppendix C - Supported Hardware and Software\n\n\nSupported Hardware\n\n\nThe following hardware is supported:\n   * Physical devices used to provide the service.\n   * Physical devices used to provide the environment used to house the service.\n\n\nHardware Services\n\n\nThe following hardware services are provided:\n   * Recommendations. OSG Operations will be responsible for specifying and recommending for purchase or lease hardware meeting customers' needs.\n   * Installation. OSG Operations will install, configure and customize system hardware and operating systems.\n   * Upgrades. OSG Operations is responsible for specifying and recommending for purchase any hardware upgrades.\n   * Diagnosis. OSG Operations will diagnose problems with service related hardware.\n   * Repair. OSG Operations analysts are not hardware technicians and receive no training in hardware maintenance, nor do we have the test equipment and tools necessary to do such work.\n\n\nPerforming repairs under warranty: Any work to be performed under warranty may be referred to the warranty service provider at the discretion of the Service Provider analyst(s). Service Provider analysts will not undertake work that will void warranties on customer hardware unless specifically requested and authorized by customer's management in writing.\n\n\nObtaining repair services: The Service Provider analyst will recommend a service vendor whenever he/she feels the repair work requires specialized skills or tools.\n\n\n\n\nBackup. Service Provider agrees to fully back up all Service Provider-supported software and data nightly every business day.\n\n\n\n\nSoftware Services\n\n\nService Provider agrees to cover software support services, including software installations and upgrades. All software maintenance periods will be announced via the policy put forth in the \nOSG Operations Responsibilities\n section of this document.\n\n\nSoftware Costs\n\n\nIU and the Grid Operations Center bears all costs for new and replacement software contingent on funding from the OSG Grant.\n\n\nAppendix D - Approval\n\n\n\n\n\n\n\n\nApproved By\n\n\nPosition\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAppendix E - Metric Reports\n\n\n\n\n[[ServiceLevelAgreements#Supporting_Documents][Recent availability statistics]]",
            "title": "OASIS Stratum-0"
        },
        {
            "location": "/SLA/oasis/#oasis-service-level-agreement-draft",
            "text": "",
            "title": "OASIS Service Level Agreement -- DRAFT"
        },
        {
            "location": "/SLA/oasis/#version-control",
            "text": "Version Number  Date  Author  Comments      0.1  2-21-2013  Scott Teige  First Draft    0.2  3-28-2013  Rob Quick  Revision to First Draft    0.3  4-9-2013  Scott Teige  Revision to Second Draft",
            "title": "Version Control"
        },
        {
            "location": "/SLA/oasis/#executive-summary",
            "text": "This SLA is an agreement between OSG Operations and the OSG Management and Stakeholders describing details of the OASIS software distribution system.",
            "title": "Executive Summary"
        },
        {
            "location": "/SLA/oasis/#owners",
            "text": "This SLA is owned by OSG Operations and Indiana University and will be reviewed and agreed upon by the OSG Executive Team and OSG Stakeholders.",
            "title": "Owners"
        },
        {
            "location": "/SLA/oasis/#service-name-and-description",
            "text": "",
            "title": "Service Name and Description"
        },
        {
            "location": "/SLA/oasis/#name",
            "text": "OASIS (OSG Application and Software Installation Service)",
            "title": "Name"
        },
        {
            "location": "/SLA/oasis/#description",
            "text": "The OASIS service provides OSG Virtual Organizations with a central location for application software. The content hosted on OASIS can be made available on OSG compute resources. The service consists of three virtual machines, a stratum 0 CERN Virtual Machine File System (CVMFS) server, a stratum 1 replica of the stratum 0 and a node accessible for login by VO OASIS managers.  This SLA covers the stratum 0 server and the interactive login node only. The stratum 1 SLA is available  here .",
            "title": "Description"
        },
        {
            "location": "/SLA/oasis/#security-considerations",
            "text": "The OASIS stratum 0 server is accessible only by GOC and OSG technology group staff. The OASIS interactive node (oasis-login.opensciencegrid.org) is accessible by GOC and OSG technology group staff and to registered VO software managers via gsissh. A person becomes a VO software manager only by explicit approval of GOC staff within the OIM Topology Database.",
            "title": "Security Considerations"
        },
        {
            "location": "/SLA/oasis/#service-target-response-priorities-and-response-times",
            "text": "This section deals with unplanned outages. Please see  Requests for Service Enhancement  for information on planned maintenance outages.     Critical  High  Elevated  Normal      Work Outage  * *  * *  * *    The issue causes a compromise of the stratum 0  The issue causes a full service outage rendering the service unavailable  The issue causes short (less than 15 minute) periods of unstable or inconsistent performance  The issue causes minor (less than 5 minutes) periods of unstable or inconsistent performance    Number of Clients Affected  * *  * *  * *    The issue affects all users or OSG resources  The issue affects all users  The issue may or may not affect all users  The issue affects only a small number of users    Response Time  * *  * *  * *    Within one (1) hour  Within the next business day  Within the next business day  Within five (5) business days    Resolution Time  * *  * *  * *    The maximum acceptable resolution time is four (4) continuous hours, after initial response time  The maximum acceptable resolution time is one full (1) business day  The maximum acceptable resolution time is five (5) business days  The maximum acceptable resolution time is thirty (30) business days    Escalates Every  * *  * *  * *    One hour  One Day  One Week  One Month",
            "title": "Service Target Response Priorities and Response Times"
        },
        {
            "location": "/SLA/oasis/#escalation-contacts",
            "text": "Escalation Level  OSG Contact      1st  OSG Operations Infrastructure Lead    2nd  OSG Operations Coordinator    3rd  OSG Production Coordinator    4th  OSG Technical Director and Executive Director     Detailed information on contacts are viewable on the following  2F15 2F2009&end_type=now&end_date=09 2F15 2F2009&site_10047=on&rg=on&rg_336=on&gridtype=on&gridtype_1=on&active=on&active_value=1&disable_value=1\">MyOSG URL , and are maintained within the  Any ongoing \"Normal\" or \"Elevated\" level issues will be discussed at the weekly  Operations  and  Production  meetings.",
            "title": "Escalation Contacts"
        },
        {
            "location": "/SLA/oasis/#service-availability-and-outages",
            "text": "The GOC will strive for 97% service availability. If service availability falls below 97% monthly as monitored by the GOC on two consecutive months a root cause analysis and service plan will be submitted to the OSG stakeholders for plans to restore an acceptable level of service availability.",
            "title": "Service Availability and Outages"
        },
        {
            "location": "/SLA/oasis/#service-support-hours",
            "text": "The service is supported 24x7 by the GOC and Indiana University. All issues will be investigated by the next business day.",
            "title": "Service Support Hours"
        },
        {
            "location": "/SLA/oasis/#service-off-hours-support-procedures",
            "text": "Users should contact the OSG  https://support.opensciencegrid.org .",
            "title": "Service Off-Hours Support Procedures"
        },
        {
            "location": "/SLA/oasis/#requests-for-service-enhancements",
            "text": "This section deals with planned maintenance outages. Please see  Service Target Response Priorities and Response Times  for information on unplanned outages.  The OSG Operations will respond to customer requests for service enhancements based on GOC determination of the necessity and desirability of the enhancement. The GOC reserves the right to enhance the physical environment of the service based on IU and GOC needs. No enhancement will occur without advanced notice to the OSG community.",
            "title": "Requests for Service Enhancements"
        },
        {
            "location": "/SLA/oasis/#customer-problem-reporting",
            "text": "The GOC provides operators 24x7x365. Service problems should be reported immediately by one of the following mechanisms.   Creating a problem ticket at https://ticket.grid.iu.edu/goc/submit ( preferred )  Emailing a description to goc@opensciencegrid.org  Calling the GOC phone at 317-278-9699",
            "title": "Customer Problem Reporting"
        },
        {
            "location": "/SLA/oasis/#responsibilities",
            "text": "",
            "title": "Responsibilities"
        },
        {
            "location": "/SLA/oasis/#customer-responsibilities",
            "text": "OASIS customers agree to:   Use the service for purposes of OSG approved work only.  Alert the GOC if they are going to use the Service in a non-standard way, this includes testing or anticipated mass increases in usage.  Contact support by means outlined in the  Customer Problem Reporting  section of this document if they encounter any service issues.  Be willing and available to provide information within one business day for any High level issues reported.",
            "title": "Customer Responsibilities"
        },
        {
            "location": "/SLA/oasis/#responsibilities_1",
            "text": "GOC operations:\n   * Maintain the physical machine hosting the service\n   * Assure the service is accessible via its advertised URL\n   * Make changes and updates within the normal GOC  release schedule \n   * Meet response times associated with the priority assigned by users.\n   * Maintain appropriately trained staff.  GOC Service Desk Responsibilities:\n   * Log and track all Customer requests for service through the OSG ticketing system.  Database & Application Services responsibilities:\n   * Announce and negotiate maintenance with stakeholders to assure minimal interruption to normal workload.\n   * Alert the community of scheduled maintenance periods at least five (5) business days prior to the start of a service affecting maintenance window.",
            "title": "Responsibilities"
        },
        {
            "location": "/SLA/oasis/#service-measuring-and-reporting",
            "text": "The GOC will provide the customer with the following reports in the intervals indicated (monthly, quarterly, semi-annually, or annually):     Report Name  Reporting Interval  Delivery Method  Responsible Party      System Uptime  Monthly  Web Posting  GOC    Service Uptime  Monthly  Web Posting  GOC    Report of Critical and High Priority Issues  Quarterly  Web Posting  GOC     These reports will be posted in Appendix E of this document.",
            "title": "Service Measuring and Reporting"
        },
        {
            "location": "/SLA/oasis/#sla-validity-period",
            "text": "This SLA will be in affect for one year.",
            "title": "SLA Validity Period"
        },
        {
            "location": "/SLA/oasis/#sla-review-procedure",
            "text": "This SLA will renew automatically on a yearly basis unless change or update is requested by the OSG Operations Coordinator, the OSG Executive Team or the Stakeholders.",
            "title": "SLA Review Procedure"
        },
        {
            "location": "/SLA/oasis/#references",
            "text": "",
            "title": "References"
        },
        {
            "location": "/SLA/oasis/#appendix-a-customer-information",
            "text": "All service end-users who are members of an OASIS enabled VO are considered customers.\nAny VO can be OASIS enabled on request, larger sustained VOs should consider setting up individual OASIS services.",
            "title": "Appendix A - Customer Information"
        },
        {
            "location": "/SLA/oasis/#appendix-b-other-service-dependencies",
            "text": "The service is dependent on the following services to collect and distribute information:\n   * Local Network and Hardware",
            "title": "Appendix B - Other Service Dependencies"
        },
        {
            "location": "/SLA/oasis/#appendix-c-supported-hardware-and-software",
            "text": "",
            "title": "Appendix C - Supported Hardware and Software"
        },
        {
            "location": "/SLA/oasis/#supported-hardware",
            "text": "The following hardware is supported:\n   * Physical devices used to provide the service.\n   * Physical devices used to provide the environment used to house the service.",
            "title": "Supported Hardware"
        },
        {
            "location": "/SLA/oasis/#hardware-services",
            "text": "The following hardware services are provided:\n   * Recommendations. OSG Operations will be responsible for specifying and recommending for purchase or lease hardware meeting customers' needs.\n   * Installation. OSG Operations will install, configure and customize system hardware and operating systems.\n   * Upgrades. OSG Operations is responsible for specifying and recommending for purchase any hardware upgrades.\n   * Diagnosis. OSG Operations will diagnose problems with service related hardware.\n   * Repair. OSG Operations analysts are not hardware technicians and receive no training in hardware maintenance, nor do we have the test equipment and tools necessary to do such work.  Performing repairs under warranty: Any work to be performed under warranty may be referred to the warranty service provider at the discretion of the Service Provider analyst(s). Service Provider analysts will not undertake work that will void warranties on customer hardware unless specifically requested and authorized by customer's management in writing.  Obtaining repair services: The Service Provider analyst will recommend a service vendor whenever he/she feels the repair work requires specialized skills or tools.   Backup. Service Provider agrees to fully back up all Service Provider-supported software and data nightly every business day.",
            "title": "Hardware Services"
        },
        {
            "location": "/SLA/oasis/#software-services",
            "text": "Service Provider agrees to cover software support services, including software installations and upgrades. All software maintenance periods will be announced via the policy put forth in the  OSG Operations Responsibilities  section of this document.",
            "title": "Software Services"
        },
        {
            "location": "/SLA/oasis/#software-costs",
            "text": "IU and the Grid Operations Center bears all costs for new and replacement software contingent on funding from the OSG Grant.",
            "title": "Software Costs"
        },
        {
            "location": "/SLA/oasis/#appendix-d-approval",
            "text": "Approved By  Position  Date",
            "title": "Appendix D - Approval"
        },
        {
            "location": "/SLA/oasis/#appendix-e-metric-reports",
            "text": "[[ServiceLevelAgreements#Supporting_Documents][Recent availability statistics]]",
            "title": "Appendix E - Metric Reports"
        },
        {
            "location": "/SLA/oasis-replica/",
            "text": "OASIS Replica Service Level Agreement\n\n\nVersion Control\n\n\n\n\n\n\n\n\nVersion Number\n\n\nDate\n\n\nAuthor\n\n\nComments\n\n\n\n\n\n\n\n\n\n\n0.1\n\n\n2-27-2013\n\n\nScott Teige\n\n\nFirst Draft\n\n\n\n\n\n\n0.2\n\n\n3-28-2013\n\n\nRob Quick\n\n\nRevision of First Draft\n\n\n\n\n\n\n\n\nExecutive Summary\n\n\nThis SLA is an agreement between OSG Operations and the OSG Management and Stakeholders describing details of the OASIS software distribution system.\n\n\nOwners\n\n\nThis SLA is owned by OSG Operations and Indiana University and will be reviewed and agreed upon by the OSG Executive Team and OSG Stakeholders.\n\n\nService Name and Description\n\n\nName\n\n\nOASIS (OSG Application and Software Installation Service) system\n\n\nDescription\n\n\nThe OASIS service provides OSG Virtual Organizations with a central location for application software. The content hosted on OASIS can be made available on OSG compute resources. The service consists of three virtual machines, a stratum 0 CERN Virtual Machine File System (CVMFS) server, a stratum 1 replica of the stratum 0 and a node accessible for login by VO OASIS managers.\n\n\nThis SLA covers the stratum 1 replica. The stratum 0 and login SLA is available \nhere\n.\n\n\nSecurity Considerations\n\n\nThe OASIS replica is accessible for login only by GOC and OSG technology group staff. It is publicly available as read-only.\n\n\nService Target Response Priorities and Response Times\n\n\nThis section deals with unplanned outages. Please see \nRequests for Service Enhancement\n for information on planned maintenance outages.\n\n\n\n\n\n\n\n\nCritical\n\n\nHigh\n\n\nElevated\n\n\nNormal\n\n\n\n\n\n\n\n\n\n\nWork Outage\n\n\n* *\n\n\n* *\n\n\n* *\n\n\n\n\n\n\nThe issue causes a full service or customer outage or a compromise to all stratum 1 replicas\n\n\nThe issue causes a full service outage or a compromise on the GOC replica instance\n\n\nThe issue causes short (less than 5 minute) periods of unstable or inconsistent performance\n\n\nThe issue causes minor (less than 10 seconds) periods of unstable or inconsistent performance\n\n\n\n\n\n\nNumber of Clients Affected\n\n\n* *\n\n\n* *\n\n\n* *\n\n\n\n\n\n\nThe issue affects all replica users or OSG resources\n\n\nThe issue affects all replica users or OSG resources\n\n\nThe issue may or may not affect all replica users or OSG resources\n\n\nThe issue affects only a small number of replica users or OSG resources\n\n\n\n\n\n\nWorkaround\n\n\n* *\n\n\n* *\n\n\n* *\n\n\n\n\n\n\nAll replicas are unavailable\n\n\nreplica failover mechanisms are used to direct usage away from the source of the outage\n\n\nreplica failover mechanisms are used to direct usage away from the source of the outage\n\n\nNo workaround will be necessary\n\n\n\n\n\n\nResponse Time\n\n\n* *\n\n\n* *\n\n\n* *\n\n\n\n\n\n\nWithin one (1) hour\n\n\nWorkaround is addressed within (1) hour. Issue will be addressed by the next business day\n\n\nWithin the next business day\n\n\nWithin the next business day\n\n\n\n\n\n\nResolution Time\n\n\n* *\n\n\n* *\n\n\n* *\n\n\n\n\n\n\nThe maximum acceptable resolution time is four (4) continuous hours, after initial response time\n\n\nThe maximum acceptable resolution time is 24 continuous hours, after the initial response time\n\n\nThe maximum acceptable resolution time is five (7) business days\n\n\nThe maximum acceptable resolution time is (30) business days\n\n\n\n\n\n\nEscalates Every\n\n\n* *\n\n\n* *\n\n\n* *\n\n\n\n\n\n\nOne Hour\n\n\nTwo Hour\n\n\nOne Day\n\n\nOne Week\n\n\n\n\n\n\n\n\nEscalation Contacts\n\n\n\n\n\n\n\n\nEscalation Level\n\n\nOSG Contact\n\n\n\n\n\n\n\n\n\n\n1st\n\n\nOSG Operations Infrastructure Lead\n\n\n\n\n\n\n2nd\n\n\nOSG Operations Coordinator\n\n\n\n\n\n\n3rd\n\n\nOSG Production Coordinator\n\n\n\n\n\n\n4th\n\n\nOSG Technical Director and Executive Director\n\n\n\n\n\n\n\n\nDetailed information on contacts are viewable on the following \n2F15\n2F2009&end_type=now&end_date=09\n2F15\n2F2009&site_10047=on&rg=on&rg_336=on&gridtype=on&gridtype_1=on&active=on&active_value=1&disable_value=1\">MyOSG URL\n, and are maintained within the\n\n\nAny ongoing \"Normal\" or \"Elevated\" level issues will be discussed at the weekly \nOperations\n and \nProduction\n meetings.\n\n\nService Availability and Outages\n\n\nThe GOC will strive for 99% service availability. If service availability falls below 99% monthly as monitored by the GOC on two consecutive months a root cause analysis and service plan will be submitted to the OSG stakeholders for plans to restore an acceptable level of service availability.\n\n\nService Support Hours\n\n\nThe service is supported 24x7 by the GOC and Indiana University. All issues will be investigated by the next business day.\n\n\nService Off-Hours Support Procedures\n\n\nUsers should contact the GOC via the \nGOC trouble ticket\n system.\n\n\nRequests for Service Enhancement\n\n\nThis section deals with planned maintenance outages. Please see \nService Target Response Priorities and Response Times\n for information on unplanned outages.\n\n\nThe OSG Operations will respond to customer requests for service enhancements based on GOC determination of the necessity and desirability of the enhancement. The GOC reserves the right to enhance the physical environment of the service based on IU and GOC needs. No enhancement will occur without advanced notice to the OSG community.\n\n\nCustomer Problem Reporting\n\n\nService problems should be reported immediately by creating a problem ticket at \nhttps://support.opensciencegrid.org\n.\n\n\nResponsibilities\n\n\nCustomer Responsibilities\n\n\nOASIS customers agree to:\n\n\n\n\nUse the service for purposes of OSG approved work only.\n\n\nAlert the GOC if they are going to use the Service in a non-standard way, this includes testing or anticipated mass increases in usage.\n\n\nContact support by means outlined in the \nCustomer Problem Reporting\n section of this document if they encounter any service issues.\n\n\nBe willing and available to provide information within one business day for any High level issues reported.\n\n\n\n\nResponsibilities\n\n\nGOC operations:\n   * Maintain the physical machine hosting the service\n   * Assure the service is accessible via its advertised URL\n   * Make changes and updates within the normal GOC release schedule documented at https://github.com/opensciencegrid/operations/blob/master/SLA/ReleaseSchedule.\n   * Meet response times associated with the priority assigned by users.\n   * Maintain appropriately trained staff.\n\n\nGOC Service Desk Responsibilities:\n   * Log and track all Customer requests for service through the OSG ticketing system.\n\n\nDatabase & Application Services responsibilities:\n   * Announce and negotiate maintenance with stakeholders to assure minimal interruption to normal workload.\n   * Alert the community of scheduled maintenance periods at least five (5) business days prior to the start of a service affecting maintenance window.\n\n\nService Measuring and Reporting\n\n\nThe GOC will provide the customer with the following reports in the intervals indicated (monthly, quarterly, semi-annually, or annually):\n\n\n\n\n\n\n\n\nReport Name\n\n\nReporting Interval\n\n\nDelivery Method\n\n\nResponsible Party\n\n\n\n\n\n\n\n\n\n\nSystem Uptime\n\n\nMonthly\n\n\nWeb Posting\n\n\nGOC\n\n\n\n\n\n\nService Uptime\n\n\nMonthly\n\n\nWeb Posting\n\n\nGOC\n\n\n\n\n\n\nReport of Critical and High Priority Issues\n\n\nQuarterly\n\n\nWeb Posting\n\n\nGOC\n\n\n\n\n\n\n\n\nThese reports will be posted in Appendix E of this document.\n\n\nSLA Validity Period\n\n\nThis SLA will be in affect for one year.\n\n\nSLA Review Procedure\n\n\nThis SLA will renew automatically on a yearly basis unless change or update is requested by the OSG Operations Coordinator, the OSG Executive Team or the Stakeholders.\n\n\nReferences\n\n\nAppendix A - Customer Information\n\n\nAll service end-users who are members of an OASIS enabled VO and OSG Staff are considered customers.\nAny VO can be OASIS enabled on request but it is anticipated that LHC associated VOs will use their existing CVMFS instances and will have no need for OASIS.\n\n\nAppendix B - Other Service Dependencies\n\n\nThe service is dependent on the following services to collect and distribute information:\n   * Local Network and Hardware\n\n\nAppendix C - Supported Hardware and Software\n\n\nSupported Hardware\n\n\nThe following hardware is supported:\n   * Physical devices used to provide the service.\n   * Physical devices used to provide the environment used to house the service.\n\n\nHardware Services\n\n\nThe following hardware services are provided:\n   * Recommendations. OSG Operations will be responsible for specifying and recommending for purchase or lease hardware meeting customers' needs.\n   * Installation. OSG Operations will install, configure and customize system hardware and operating systems.\n   * Upgrades. OSG Operations is responsible for specifying and recommending for purchase any hardware upgrades.\n   * Diagnosis. OSG Operations will diagnose problems with service related hardware.\n   * Repair. OSG Operations analysts are not hardware technicians and receive no training in hardware maintenance, nor do we have the test equipment and tools necessary to do such work.\n\n\nPerforming repairs under warranty: Any work to be performed under warranty may be referred to the warranty service provider at the discretion of the Service Provider analyst(s). Service Provider analysts will not undertake work that will void warranties on customer hardware unless specifically requested and authorized by customer's management in writing.\n\n\nObtaining repair services: The Service Provider analyst will recommend a service vendor whenever he/she feels the repair work requires specialized skills or tools.\n\n\n\n\nBackup. Service Provider agrees to fully back up all Service Provider-supported software and data nightly every business day.\n\n\n\n\nSoftware Services\n\n\nService Provider agrees to cover software support services, including software installations and upgrades. All software maintenance periods will be announced via the policy put forth in the \nOSG Operations Responsibilities\n section of this document.\n\n\nSoftware Costs\n\n\nIU and the Grid Operations Center bears all costs for new and replacement software contingent on the OSG Grant.\n\n\nAppendix D - Approval\n\n\n\n\n\n\n\n\nApproved By\n\n\nPosition\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAppendix E - Metric Reports\n\n\n\n\n[[ServiceLevelAgreements#Supporting_Documents][Recent availability statistics]]",
            "title": "OASIS Replica"
        },
        {
            "location": "/SLA/oasis-replica/#oasis-replica-service-level-agreement",
            "text": "",
            "title": "OASIS Replica Service Level Agreement"
        },
        {
            "location": "/SLA/oasis-replica/#version-control",
            "text": "Version Number  Date  Author  Comments      0.1  2-27-2013  Scott Teige  First Draft    0.2  3-28-2013  Rob Quick  Revision of First Draft",
            "title": "Version Control"
        },
        {
            "location": "/SLA/oasis-replica/#executive-summary",
            "text": "This SLA is an agreement between OSG Operations and the OSG Management and Stakeholders describing details of the OASIS software distribution system.",
            "title": "Executive Summary"
        },
        {
            "location": "/SLA/oasis-replica/#owners",
            "text": "This SLA is owned by OSG Operations and Indiana University and will be reviewed and agreed upon by the OSG Executive Team and OSG Stakeholders.",
            "title": "Owners"
        },
        {
            "location": "/SLA/oasis-replica/#service-name-and-description",
            "text": "",
            "title": "Service Name and Description"
        },
        {
            "location": "/SLA/oasis-replica/#name",
            "text": "OASIS (OSG Application and Software Installation Service) system",
            "title": "Name"
        },
        {
            "location": "/SLA/oasis-replica/#description",
            "text": "The OASIS service provides OSG Virtual Organizations with a central location for application software. The content hosted on OASIS can be made available on OSG compute resources. The service consists of three virtual machines, a stratum 0 CERN Virtual Machine File System (CVMFS) server, a stratum 1 replica of the stratum 0 and a node accessible for login by VO OASIS managers.  This SLA covers the stratum 1 replica. The stratum 0 and login SLA is available  here .",
            "title": "Description"
        },
        {
            "location": "/SLA/oasis-replica/#security-considerations",
            "text": "The OASIS replica is accessible for login only by GOC and OSG technology group staff. It is publicly available as read-only.",
            "title": "Security Considerations"
        },
        {
            "location": "/SLA/oasis-replica/#service-target-response-priorities-and-response-times",
            "text": "This section deals with unplanned outages. Please see  Requests for Service Enhancement  for information on planned maintenance outages.     Critical  High  Elevated  Normal      Work Outage  * *  * *  * *    The issue causes a full service or customer outage or a compromise to all stratum 1 replicas  The issue causes a full service outage or a compromise on the GOC replica instance  The issue causes short (less than 5 minute) periods of unstable or inconsistent performance  The issue causes minor (less than 10 seconds) periods of unstable or inconsistent performance    Number of Clients Affected  * *  * *  * *    The issue affects all replica users or OSG resources  The issue affects all replica users or OSG resources  The issue may or may not affect all replica users or OSG resources  The issue affects only a small number of replica users or OSG resources    Workaround  * *  * *  * *    All replicas are unavailable  replica failover mechanisms are used to direct usage away from the source of the outage  replica failover mechanisms are used to direct usage away from the source of the outage  No workaround will be necessary    Response Time  * *  * *  * *    Within one (1) hour  Workaround is addressed within (1) hour. Issue will be addressed by the next business day  Within the next business day  Within the next business day    Resolution Time  * *  * *  * *    The maximum acceptable resolution time is four (4) continuous hours, after initial response time  The maximum acceptable resolution time is 24 continuous hours, after the initial response time  The maximum acceptable resolution time is five (7) business days  The maximum acceptable resolution time is (30) business days    Escalates Every  * *  * *  * *    One Hour  Two Hour  One Day  One Week",
            "title": "Service Target Response Priorities and Response Times"
        },
        {
            "location": "/SLA/oasis-replica/#escalation-contacts",
            "text": "Escalation Level  OSG Contact      1st  OSG Operations Infrastructure Lead    2nd  OSG Operations Coordinator    3rd  OSG Production Coordinator    4th  OSG Technical Director and Executive Director     Detailed information on contacts are viewable on the following  2F15 2F2009&end_type=now&end_date=09 2F15 2F2009&site_10047=on&rg=on&rg_336=on&gridtype=on&gridtype_1=on&active=on&active_value=1&disable_value=1\">MyOSG URL , and are maintained within the  Any ongoing \"Normal\" or \"Elevated\" level issues will be discussed at the weekly  Operations  and  Production  meetings.",
            "title": "Escalation Contacts"
        },
        {
            "location": "/SLA/oasis-replica/#service-availability-and-outages",
            "text": "The GOC will strive for 99% service availability. If service availability falls below 99% monthly as monitored by the GOC on two consecutive months a root cause analysis and service plan will be submitted to the OSG stakeholders for plans to restore an acceptable level of service availability.",
            "title": "Service Availability and Outages"
        },
        {
            "location": "/SLA/oasis-replica/#service-support-hours",
            "text": "The service is supported 24x7 by the GOC and Indiana University. All issues will be investigated by the next business day.",
            "title": "Service Support Hours"
        },
        {
            "location": "/SLA/oasis-replica/#service-off-hours-support-procedures",
            "text": "Users should contact the GOC via the  GOC trouble ticket  system.",
            "title": "Service Off-Hours Support Procedures"
        },
        {
            "location": "/SLA/oasis-replica/#requests-for-service-enhancement",
            "text": "This section deals with planned maintenance outages. Please see  Service Target Response Priorities and Response Times  for information on unplanned outages.  The OSG Operations will respond to customer requests for service enhancements based on GOC determination of the necessity and desirability of the enhancement. The GOC reserves the right to enhance the physical environment of the service based on IU and GOC needs. No enhancement will occur without advanced notice to the OSG community.",
            "title": "Requests for Service Enhancement"
        },
        {
            "location": "/SLA/oasis-replica/#customer-problem-reporting",
            "text": "Service problems should be reported immediately by creating a problem ticket at  https://support.opensciencegrid.org .",
            "title": "Customer Problem Reporting"
        },
        {
            "location": "/SLA/oasis-replica/#responsibilities",
            "text": "",
            "title": "Responsibilities"
        },
        {
            "location": "/SLA/oasis-replica/#customer-responsibilities",
            "text": "OASIS customers agree to:   Use the service for purposes of OSG approved work only.  Alert the GOC if they are going to use the Service in a non-standard way, this includes testing or anticipated mass increases in usage.  Contact support by means outlined in the  Customer Problem Reporting  section of this document if they encounter any service issues.  Be willing and available to provide information within one business day for any High level issues reported.",
            "title": "Customer Responsibilities"
        },
        {
            "location": "/SLA/oasis-replica/#responsibilities_1",
            "text": "GOC operations:\n   * Maintain the physical machine hosting the service\n   * Assure the service is accessible via its advertised URL\n   * Make changes and updates within the normal GOC release schedule documented at https://github.com/opensciencegrid/operations/blob/master/SLA/ReleaseSchedule.\n   * Meet response times associated with the priority assigned by users.\n   * Maintain appropriately trained staff.  GOC Service Desk Responsibilities:\n   * Log and track all Customer requests for service through the OSG ticketing system.  Database & Application Services responsibilities:\n   * Announce and negotiate maintenance with stakeholders to assure minimal interruption to normal workload.\n   * Alert the community of scheduled maintenance periods at least five (5) business days prior to the start of a service affecting maintenance window.",
            "title": "Responsibilities"
        },
        {
            "location": "/SLA/oasis-replica/#service-measuring-and-reporting",
            "text": "The GOC will provide the customer with the following reports in the intervals indicated (monthly, quarterly, semi-annually, or annually):     Report Name  Reporting Interval  Delivery Method  Responsible Party      System Uptime  Monthly  Web Posting  GOC    Service Uptime  Monthly  Web Posting  GOC    Report of Critical and High Priority Issues  Quarterly  Web Posting  GOC     These reports will be posted in Appendix E of this document.",
            "title": "Service Measuring and Reporting"
        },
        {
            "location": "/SLA/oasis-replica/#sla-validity-period",
            "text": "This SLA will be in affect for one year.",
            "title": "SLA Validity Period"
        },
        {
            "location": "/SLA/oasis-replica/#sla-review-procedure",
            "text": "This SLA will renew automatically on a yearly basis unless change or update is requested by the OSG Operations Coordinator, the OSG Executive Team or the Stakeholders.",
            "title": "SLA Review Procedure"
        },
        {
            "location": "/SLA/oasis-replica/#references",
            "text": "",
            "title": "References"
        },
        {
            "location": "/SLA/oasis-replica/#appendix-a-customer-information",
            "text": "All service end-users who are members of an OASIS enabled VO and OSG Staff are considered customers.\nAny VO can be OASIS enabled on request but it is anticipated that LHC associated VOs will use their existing CVMFS instances and will have no need for OASIS.",
            "title": "Appendix A - Customer Information"
        },
        {
            "location": "/SLA/oasis-replica/#appendix-b-other-service-dependencies",
            "text": "The service is dependent on the following services to collect and distribute information:\n   * Local Network and Hardware",
            "title": "Appendix B - Other Service Dependencies"
        },
        {
            "location": "/SLA/oasis-replica/#appendix-c-supported-hardware-and-software",
            "text": "",
            "title": "Appendix C - Supported Hardware and Software"
        },
        {
            "location": "/SLA/oasis-replica/#supported-hardware",
            "text": "The following hardware is supported:\n   * Physical devices used to provide the service.\n   * Physical devices used to provide the environment used to house the service.",
            "title": "Supported Hardware"
        },
        {
            "location": "/SLA/oasis-replica/#hardware-services",
            "text": "The following hardware services are provided:\n   * Recommendations. OSG Operations will be responsible for specifying and recommending for purchase or lease hardware meeting customers' needs.\n   * Installation. OSG Operations will install, configure and customize system hardware and operating systems.\n   * Upgrades. OSG Operations is responsible for specifying and recommending for purchase any hardware upgrades.\n   * Diagnosis. OSG Operations will diagnose problems with service related hardware.\n   * Repair. OSG Operations analysts are not hardware technicians and receive no training in hardware maintenance, nor do we have the test equipment and tools necessary to do such work.  Performing repairs under warranty: Any work to be performed under warranty may be referred to the warranty service provider at the discretion of the Service Provider analyst(s). Service Provider analysts will not undertake work that will void warranties on customer hardware unless specifically requested and authorized by customer's management in writing.  Obtaining repair services: The Service Provider analyst will recommend a service vendor whenever he/she feels the repair work requires specialized skills or tools.   Backup. Service Provider agrees to fully back up all Service Provider-supported software and data nightly every business day.",
            "title": "Hardware Services"
        },
        {
            "location": "/SLA/oasis-replica/#software-services",
            "text": "Service Provider agrees to cover software support services, including software installations and upgrades. All software maintenance periods will be announced via the policy put forth in the  OSG Operations Responsibilities  section of this document.",
            "title": "Software Services"
        },
        {
            "location": "/SLA/oasis-replica/#software-costs",
            "text": "IU and the Grid Operations Center bears all costs for new and replacement software contingent on the OSG Grant.",
            "title": "Software Costs"
        },
        {
            "location": "/SLA/oasis-replica/#appendix-d-approval",
            "text": "Approved By  Position  Date",
            "title": "Appendix D - Approval"
        },
        {
            "location": "/SLA/oasis-replica/#appendix-e-metric-reports",
            "text": "[[ServiceLevelAgreements#Supporting_Documents][Recent availability statistics]]",
            "title": "Appendix E - Metric Reports"
        },
        {
            "location": "/SLA/oim/",
            "text": "OSG Information Management Service Level Agreement\n\n\nVersion Control\n\n\n\n\n\n\n\n\nVersion Number\n\n\nDate\n\n\nAuthor\n\n\nComments\n\n\n\n\n\n\n\n\n\n\n1.1\n\n\n3-17-2010\n\n\nRob Quick\n\n\nFirst Draft\n\n\n\n\n\n\n1.2\n\n\n4-2-2013\n\n\nRob Quick\n\n\nNew additions due to OSG PKI Interface\n\n\n\n\n\n\n\n\nExecutive Summary\n\n\nThis SLA is an agreement between OSG Operations and the OSG Management and Stakeholders describing details of the OSG Information Management (OIM) registration and topology database and it's API. The OIM service runs on hardware at Indiana University and provides a resource topology database along with a registration API.\n\n\nOwners\n\n\nThis SLA is owned by OSG Operations and Indiana University and will be reviewed and agreed upon by the OSG Executive Team and OSG Stakeholders.\n\n\nService Name and Description\n\n\nName\n\n\nProduction OIM Service\n\n\nDescription\n\n\nThe OIM service is the topology management system for the OSG. It holds information about people and resources involved in the OSG. The OIM service consists of a MySQL database and a web-based API.\n\n\nOIM is also used to satisfy certificate requests made by OSG users.\n\n\nSecurity Considerations\n\n\nAll information collected and distributed by the OIM is protected by OSG RA certificate authentication. While some information is deemed public, there is restricted access to personal information such as contact email and phone records.\n\n\nService Target Response Priorities and Response Times\n\n\nThis section deals with unplanned outages. Please see \nRequests for Service Enhancement\n for information on planned maintenance outages.\n\n\n\n\n\n\n\n\nCritical\n\n\nHigh\n\n\nElevated\n\n\nNormal\n\n\n\n\n\n\n\n\n\n\nWork Outage\n\n\n* *\n\n\n* *\n\n\n* *\n\n\n\n\n\n\nThe OIM Service does not have critical priority\n\n\nThe issue causes a full service outage rendering OIM unavailable for registration or distribution\n\n\nThe issue causes short (less than 15 minute) periods of unstable or inconsistent performance\n\n\nThe issue causes minor (less than 5 minutes) periods of unstable or inconsistent performance\n\n\n\n\n\n\nNumber of Clients Affected\n\n\n* *\n\n\n* *\n\n\n* *\n\n\n\n\n\n\nN/A\n\n\nThe issue affects all OIM users\n\n\nThe issue may or may not affect all users\n\n\nThe issue affects only a small number of users\n\n\n\n\n\n\nResponse Time\n\n\n* *\n\n\n* *\n\n\n* *\n\n\n\n\n\n\nN/A\n\n\nWithin the next business day\n\n\nWithin the next business day\n\n\nWithin five (5) business days\n\n\n\n\n\n\nResolution Time\n\n\n* *\n\n\n* *\n\n\n* *\n\n\n\n\n\n\nN/A\n\n\nThe maximum acceptable resolution time is one full (1) business day\n\n\nThe maximum acceptable resolution time is five (5) business days\n\n\nThe maximum acceptable resolution time is thirty (30) business days\n\n\n\n\n\n\nEscalates Every\n\n\n* *\n\n\n* *\n\n\n* *\n\n\n\n\n\n\nN/A\n\n\nOne Day\n\n\nOne Week\n\n\nOne Month\n\n\n\n\n\n\n\n\nEscalation Contacts\n\n\n\n\n\n\n\n\nEscalation Level\n\n\nOSG Contact\n\n\n\n\n\n\n\n\n\n\n1st\n\n\nOSG Operations Infrastructure Lead\n\n\n\n\n\n\n2nd\n\n\nOSG Operations Coordinator\n\n\n\n\n\n\n3rd\n\n\nOSG Production Coordinator\n\n\n\n\n\n\n4th\n\n\nOSG Technical Director and Executive Director\n\n\n\n\n\n\n\n\nDetailed information on contacts are viewable on the following \nMyOSG URL\n, and are maintained within the\n\n\nAny ongoing \"Normal\" or \"Elevated\" level issues will be discussed at the weekly \nOperations\n and \nProduction\n meetings.\n\n\nService Availability and Outages\n\n\nThe GOC will strive for 97% service availability. If service availability falls below 97% monthly as monitored by the GOC on two consecutive months a root cause analysis and service plan will be submitted to the OSG stakeholders for plans to restore an acceptable level of service availability.\n\n\nService Support Hours\n\n\nThe software service is supported 24x7 by the GOC and Indiana University. All issues will be investigated by the next business day.\n\n\nService Off-Hours Support Procedures\n\n\nAll software issues should be reported to the OSG immediately by \ntrouble ticket\n web submission.\n\n\nRequests for Service Enhancement\n\n\nThis section deals with planned maintenance outages. Please see \nService Target Response Priorities and Response Times\n for information on unplanned outages.\n\n\nThe OSG Operations will respond to customer requests for service enhancements based on GOC determination of the necessity and desirability of the enhancement. The GOC reserves the right to enhance the physical environment of the service based on IU and GOC needs. No enhancement will occur without advanced notice to the OSG community.\n\n\nCustomer Problem Reporting\n\n\nThe GOC provides operators 24x7x365. OIM service problems should be reported immediately by one of the following mechanisms.\n\n\n\n\nCreating a problem ticket at https://ticket.grid.iu.edu/goc/oim (\npreferred\n)\n\n\nCalling the GOC phone at 317-278-9699\n\n\nEmailing a description to goc@opensciencegrid.org\n\n\n\n\nResponsibilities\n\n\nCustomer Responsibilities\n\n\nOIM customers agree to:\n\n\n\n\nUse the OIM service for purposes of VO or OSG approved work only.\n\n\nAlert the GOC if they are going to use the OIM Service in a non-standard way, this includes testing or anticipated mass increases in usage.\n\n\nContact the GOC by means outlined in the \nCustomer Problem Reporting\n section of this document if they encounter any service issues.\n\n\nBe willing and available to provide information within one business day for any High level issues reported.\n\n\nProvide testing for the OSG OIM service within the time frame defined in the \nRequests for Service Enhancements\n section.\n\n\nAlert the GOC when problems are encountered during testing.\n\n\n\n\nOSG Operations Responsibilities\n\n\nGeneral responsibilities:\n   * Create and add appropriate documentation to the OSG TWiki for appropriate use of the OIM.\n   * Meet response times associated with the priority assigned to Customer issues.\n   * Maintain appropriately trained staff.\n   * The OSG and GOC are not responsible if a customer does not provide testing during the testing period. In such cases, the GOC has final discretion in what remedial actions to take.\n   * Make changes and updates within the normal GOC release schedule documented at https://github.com/opensciencegrid/operations/blob/master/SLA/ReleaseSchedule.\nProposed Change:\n   * Update the OSG RA and GA communities of changes that affect the OIM PKI usage.\n\n\nGOC Service Desk Responsibilities:\n   * Log and track all Customer requests for service through the OSG ticketing system.\n\n\nDatabase & Application Services responsibilities:\n   * Announce and negotiate maintenance with stakeholders to assure minimal interruption to normal workload.\n   * Alert the community of scheduled maintenance periods at least five (5) business days prior to the start of a service affecting maintenance window.\n\n\nService Measuring and Reporting\n\n\nThe GOC will provide the customer with the following reports in the intervals indicated (monthly, quarterly, semi-annually, or annually):\n\n\n\n\n\n\n\n\nReport Name\n\n\nReporting Interval\n\n\nDelivery Method\n\n\nResponsible Party\n\n\n\n\n\n\n\n\n\n\nSystem Uptime\n\n\nMonthly\n\n\nWeb Posting\n\n\nGOC\n\n\n\n\n\n\nService Uptime\n\n\nMonthly\n\n\nWeb Posting\n\n\nGOC\n\n\n\n\n\n\nReport of Critical and High Priority Issues\n\n\nQuarterly\n\n\nWeb Posting\n\n\nGOC\n\n\n\n\n\n\n\n\nThese reports will be posted in Appendix E of this document.\n\n\nSLA Validity Period\n\n\nThis SLA will be in affect for one year.\n\n\nSLA Review Procedure\n\n\nThis SLA will renew automatically on a yearly basis unless change or update is requested by the OSG Operations Coordinator, the OSG Executive Team or the Stakeholders.\n\n\nReferences\n\n\nAppendix A - Customer Information\n\n\nAll OIM service end-users who are members of an OSG VO and OSG Staff are considered customers.\n\n\nAppendix B - Other Service Dependencies\n\n\nThe OIM service is dependent on the following services to collect and distribute information:\n   * Local Network, Hardware, OS, Apache, and MySQL\n\n\nAppendix C - Supported Hardware and Software\n\n\nSupported Hardware\n\n\nThe following hardware is supported:\n   * Physical devices used to provide the service.\n   * Physical devices used to provide the environment used to house the service.\n\n\nHardware Services\n\n\nThe following hardware services are provided:\n   * Recommendations. OSG Operations will be responsible for specifying and recommending for purchase or lease hardware meeting customers' needs.\n   * Installation. OSG Operations will install, configure and customize system hardware and operating systems.\n   * Upgrades. OSG Operations is responsible for specifying and recommending for purchase any hardware upgrades.\n   * Diagnosis. OSG Operations will diagnose problems with service related hardware.\n   * Repair. OSG Operations analysts are not hardware technicians and receive no training in hardware maintenance, nor do we have the test equipment and tools necessary to do such work.\n\n\nPerforming repairs under warranty: Any work to be performed under warranty may be referred to the warranty service provider at the discretion of the Service Provider analyst(s). Service Provider analysts will not undertake work that will void warranties on customer hardware unless specifically requested and authorized by customer's management in writing.\n\n\nObtaining repair services: The Service Provider analyst will recommend a service vendor whenever he/she feels the repair work requires specialized skills or tools.\n\n\n\n\nBackup. Service Provider agrees to fully back up all Service Provider-supported software and data nightly every business day.\n\n\n\n\nSoftware Services\n\n\nService Provider agrees to cover software support services, including software installations and upgrades. All software maintenance periods will be announced via the policy put forth in the \nOSG Operations Responsibilities\n section of this document.\n\n\nSoftware Costs\n\n\nIU and the Grid Operations Center bears all costs for new and replacement software.\n\n\nAppendix D - Approval\n\n\n\n\n\n\n\n\nApproved By\n\n\nPosition\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAppendix E - Metric Reports\n\n\n\n\n[[ServiceLevelAgreements#Supporting_Documents][Recent availability statistics]]",
            "title": "OIM"
        },
        {
            "location": "/SLA/oim/#osg-information-management-service-level-agreement",
            "text": "",
            "title": "OSG Information Management Service Level Agreement"
        },
        {
            "location": "/SLA/oim/#version-control",
            "text": "Version Number  Date  Author  Comments      1.1  3-17-2010  Rob Quick  First Draft    1.2  4-2-2013  Rob Quick  New additions due to OSG PKI Interface",
            "title": "Version Control"
        },
        {
            "location": "/SLA/oim/#executive-summary",
            "text": "This SLA is an agreement between OSG Operations and the OSG Management and Stakeholders describing details of the OSG Information Management (OIM) registration and topology database and it's API. The OIM service runs on hardware at Indiana University and provides a resource topology database along with a registration API.",
            "title": "Executive Summary"
        },
        {
            "location": "/SLA/oim/#owners",
            "text": "This SLA is owned by OSG Operations and Indiana University and will be reviewed and agreed upon by the OSG Executive Team and OSG Stakeholders.",
            "title": "Owners"
        },
        {
            "location": "/SLA/oim/#service-name-and-description",
            "text": "",
            "title": "Service Name and Description"
        },
        {
            "location": "/SLA/oim/#name",
            "text": "Production OIM Service",
            "title": "Name"
        },
        {
            "location": "/SLA/oim/#description",
            "text": "The OIM service is the topology management system for the OSG. It holds information about people and resources involved in the OSG. The OIM service consists of a MySQL database and a web-based API.  OIM is also used to satisfy certificate requests made by OSG users.",
            "title": "Description"
        },
        {
            "location": "/SLA/oim/#security-considerations",
            "text": "All information collected and distributed by the OIM is protected by OSG RA certificate authentication. While some information is deemed public, there is restricted access to personal information such as contact email and phone records.",
            "title": "Security Considerations"
        },
        {
            "location": "/SLA/oim/#service-target-response-priorities-and-response-times",
            "text": "This section deals with unplanned outages. Please see  Requests for Service Enhancement  for information on planned maintenance outages.     Critical  High  Elevated  Normal      Work Outage  * *  * *  * *    The OIM Service does not have critical priority  The issue causes a full service outage rendering OIM unavailable for registration or distribution  The issue causes short (less than 15 minute) periods of unstable or inconsistent performance  The issue causes minor (less than 5 minutes) periods of unstable or inconsistent performance    Number of Clients Affected  * *  * *  * *    N/A  The issue affects all OIM users  The issue may or may not affect all users  The issue affects only a small number of users    Response Time  * *  * *  * *    N/A  Within the next business day  Within the next business day  Within five (5) business days    Resolution Time  * *  * *  * *    N/A  The maximum acceptable resolution time is one full (1) business day  The maximum acceptable resolution time is five (5) business days  The maximum acceptable resolution time is thirty (30) business days    Escalates Every  * *  * *  * *    N/A  One Day  One Week  One Month",
            "title": "Service Target Response Priorities and Response Times"
        },
        {
            "location": "/SLA/oim/#escalation-contacts",
            "text": "Escalation Level  OSG Contact      1st  OSG Operations Infrastructure Lead    2nd  OSG Operations Coordinator    3rd  OSG Production Coordinator    4th  OSG Technical Director and Executive Director     Detailed information on contacts are viewable on the following  MyOSG URL , and are maintained within the  Any ongoing \"Normal\" or \"Elevated\" level issues will be discussed at the weekly  Operations  and  Production  meetings.",
            "title": "Escalation Contacts"
        },
        {
            "location": "/SLA/oim/#service-availability-and-outages",
            "text": "The GOC will strive for 97% service availability. If service availability falls below 97% monthly as monitored by the GOC on two consecutive months a root cause analysis and service plan will be submitted to the OSG stakeholders for plans to restore an acceptable level of service availability.",
            "title": "Service Availability and Outages"
        },
        {
            "location": "/SLA/oim/#service-support-hours",
            "text": "The software service is supported 24x7 by the GOC and Indiana University. All issues will be investigated by the next business day.",
            "title": "Service Support Hours"
        },
        {
            "location": "/SLA/oim/#service-off-hours-support-procedures",
            "text": "All software issues should be reported to the OSG immediately by  trouble ticket  web submission.",
            "title": "Service Off-Hours Support Procedures"
        },
        {
            "location": "/SLA/oim/#requests-for-service-enhancement",
            "text": "This section deals with planned maintenance outages. Please see  Service Target Response Priorities and Response Times  for information on unplanned outages.  The OSG Operations will respond to customer requests for service enhancements based on GOC determination of the necessity and desirability of the enhancement. The GOC reserves the right to enhance the physical environment of the service based on IU and GOC needs. No enhancement will occur without advanced notice to the OSG community.",
            "title": "Requests for Service Enhancement"
        },
        {
            "location": "/SLA/oim/#customer-problem-reporting",
            "text": "The GOC provides operators 24x7x365. OIM service problems should be reported immediately by one of the following mechanisms.   Creating a problem ticket at https://ticket.grid.iu.edu/goc/oim ( preferred )  Calling the GOC phone at 317-278-9699  Emailing a description to goc@opensciencegrid.org",
            "title": "Customer Problem Reporting"
        },
        {
            "location": "/SLA/oim/#responsibilities",
            "text": "",
            "title": "Responsibilities"
        },
        {
            "location": "/SLA/oim/#customer-responsibilities",
            "text": "OIM customers agree to:   Use the OIM service for purposes of VO or OSG approved work only.  Alert the GOC if they are going to use the OIM Service in a non-standard way, this includes testing or anticipated mass increases in usage.  Contact the GOC by means outlined in the  Customer Problem Reporting  section of this document if they encounter any service issues.  Be willing and available to provide information within one business day for any High level issues reported.  Provide testing for the OSG OIM service within the time frame defined in the  Requests for Service Enhancements  section.  Alert the GOC when problems are encountered during testing.",
            "title": "Customer Responsibilities"
        },
        {
            "location": "/SLA/oim/#osg-operations-responsibilities",
            "text": "General responsibilities:\n   * Create and add appropriate documentation to the OSG TWiki for appropriate use of the OIM.\n   * Meet response times associated with the priority assigned to Customer issues.\n   * Maintain appropriately trained staff.\n   * The OSG and GOC are not responsible if a customer does not provide testing during the testing period. In such cases, the GOC has final discretion in what remedial actions to take.\n   * Make changes and updates within the normal GOC release schedule documented at https://github.com/opensciencegrid/operations/blob/master/SLA/ReleaseSchedule.\nProposed Change:\n   * Update the OSG RA and GA communities of changes that affect the OIM PKI usage.  GOC Service Desk Responsibilities:\n   * Log and track all Customer requests for service through the OSG ticketing system.  Database & Application Services responsibilities:\n   * Announce and negotiate maintenance with stakeholders to assure minimal interruption to normal workload.\n   * Alert the community of scheduled maintenance periods at least five (5) business days prior to the start of a service affecting maintenance window.",
            "title": "OSG Operations Responsibilities"
        },
        {
            "location": "/SLA/oim/#service-measuring-and-reporting",
            "text": "The GOC will provide the customer with the following reports in the intervals indicated (monthly, quarterly, semi-annually, or annually):     Report Name  Reporting Interval  Delivery Method  Responsible Party      System Uptime  Monthly  Web Posting  GOC    Service Uptime  Monthly  Web Posting  GOC    Report of Critical and High Priority Issues  Quarterly  Web Posting  GOC     These reports will be posted in Appendix E of this document.",
            "title": "Service Measuring and Reporting"
        },
        {
            "location": "/SLA/oim/#sla-validity-period",
            "text": "This SLA will be in affect for one year.",
            "title": "SLA Validity Period"
        },
        {
            "location": "/SLA/oim/#sla-review-procedure",
            "text": "This SLA will renew automatically on a yearly basis unless change or update is requested by the OSG Operations Coordinator, the OSG Executive Team or the Stakeholders.",
            "title": "SLA Review Procedure"
        },
        {
            "location": "/SLA/oim/#references",
            "text": "",
            "title": "References"
        },
        {
            "location": "/SLA/oim/#appendix-a-customer-information",
            "text": "All OIM service end-users who are members of an OSG VO and OSG Staff are considered customers.",
            "title": "Appendix A - Customer Information"
        },
        {
            "location": "/SLA/oim/#appendix-b-other-service-dependencies",
            "text": "The OIM service is dependent on the following services to collect and distribute information:\n   * Local Network, Hardware, OS, Apache, and MySQL",
            "title": "Appendix B - Other Service Dependencies"
        },
        {
            "location": "/SLA/oim/#appendix-c-supported-hardware-and-software",
            "text": "",
            "title": "Appendix C - Supported Hardware and Software"
        },
        {
            "location": "/SLA/oim/#supported-hardware",
            "text": "The following hardware is supported:\n   * Physical devices used to provide the service.\n   * Physical devices used to provide the environment used to house the service.",
            "title": "Supported Hardware"
        },
        {
            "location": "/SLA/oim/#hardware-services",
            "text": "The following hardware services are provided:\n   * Recommendations. OSG Operations will be responsible for specifying and recommending for purchase or lease hardware meeting customers' needs.\n   * Installation. OSG Operations will install, configure and customize system hardware and operating systems.\n   * Upgrades. OSG Operations is responsible for specifying and recommending for purchase any hardware upgrades.\n   * Diagnosis. OSG Operations will diagnose problems with service related hardware.\n   * Repair. OSG Operations analysts are not hardware technicians and receive no training in hardware maintenance, nor do we have the test equipment and tools necessary to do such work.  Performing repairs under warranty: Any work to be performed under warranty may be referred to the warranty service provider at the discretion of the Service Provider analyst(s). Service Provider analysts will not undertake work that will void warranties on customer hardware unless specifically requested and authorized by customer's management in writing.  Obtaining repair services: The Service Provider analyst will recommend a service vendor whenever he/she feels the repair work requires specialized skills or tools.   Backup. Service Provider agrees to fully back up all Service Provider-supported software and data nightly every business day.",
            "title": "Hardware Services"
        },
        {
            "location": "/SLA/oim/#software-services",
            "text": "Service Provider agrees to cover software support services, including software installations and upgrades. All software maintenance periods will be announced via the policy put forth in the  OSG Operations Responsibilities  section of this document.",
            "title": "Software Services"
        },
        {
            "location": "/SLA/oim/#software-costs",
            "text": "IU and the Grid Operations Center bears all costs for new and replacement software.",
            "title": "Software Costs"
        },
        {
            "location": "/SLA/oim/#appendix-d-approval",
            "text": "Approved By  Position  Date",
            "title": "Appendix D - Approval"
        },
        {
            "location": "/SLA/oim/#appendix-e-metric-reports",
            "text": "[[ServiceLevelAgreements#Supporting_Documents][Recent availability statistics]]",
            "title": "Appendix E - Metric Reports"
        },
        {
            "location": "/SLA/xdlogin/",
            "text": "XD-Login Service Level Agreement\n\n\nVersion Control\n\n\n\n\n\n\n\n\nVersion Number\n\n\nDate\n\n\nAuthor\n\n\nComments\n\n\n\n\n\n\n\n\n\n\n0.1\n\n\n2-18-2013\n\n\nScott Teige\n\n\nFirst Draft\n\n\n\n\n\n\n\n\nExecutive Summary\n\n\nThis SLA is an agreement between OSG Operations, Information Sciences Institute (ISI) and the OSG Management and Stakeholders describing details\nof the OSG-XD job submission system.\n\n\nOwners\n\n\nThis SLA is owned by OSG Operations, Indiana University and ISI and will be reviewed and agreed upon by the OSG Executive Team and OSG Stakeholders.\n\n\nService Name and Description\n\n\nName\n\n\nThe service is built on two physical machines hosted by the GOC. These are\nthe production XD-Login job submission node and an adjunct machine, OSG-FLOCK.\n\n\nDescription\n\n\nThe XD-Login submission node allows OSG resources to be accessed by XSEDE users. A complete description is available \nhere.\n\n\nSecurity Considerations\n\n\nThe XD-Login submission node supports Single Sign On through the XSEDE User Portal, and also from the command line using gsissh with a grid certificate for authentication.\nOSG-FLOCK is not publicly accesible.\n\n\nService Target Response Priorities and Response Times\n\n\nThis section deals with unplanned outages. Please see \nRequests for Service Enhancement\n for information on planned maintenance outages.\n\n\n\n\n\n\n\n\nCritical\n\n\nHigh\n\n\nElevated\n\n\nNormal\n\n\n\n\n\n\n\n\n\n\nWork Outage\n\n\n* *\n\n\n* *\n\n\n* *\n\n\n\n\n\n\nThis Service does not have critical priority\n\n\nThe issue causes a full service outage rendering the service unavailable\n\n\nThe issue causes short (less than 15 minute) periods of unstable or inconsistent performance\n\n\nThe issue causes minor (less than 5 minutes) periods of unstable or inconsistent performance\n\n\n\n\n\n\nNumber of Clients Affected\n\n\n* *\n\n\n* *\n\n\n* *\n\n\n\n\n\n\nN/A\n\n\nThe issue affects all users\n\n\nThe issue may or may not affect all users\n\n\nThe issue affects only a small number of users\n\n\n\n\n\n\nResponse Time\n\n\n* *\n\n\n* *\n\n\n* *\n\n\n\n\n\n\nN/A\n\n\nWithin the next business day\n\n\nWithin the next business day\n\n\nWithin five (5) business days\n\n\n\n\n\n\nResolution Time\n\n\n* *\n\n\n* *\n\n\n* *\n\n\n\n\n\n\nN/A\n\n\nThe maximum acceptable resolution time is one full (1) business day\n\n\nThe maximum acceptable resolution time is five (5) business days\n\n\nThe maximum acceptable resolution time is thirty (30) business days\n\n\n\n\n\n\nEscalates Every\n\n\n* *\n\n\n* *\n\n\n* *\n\n\n\n\n\n\nN/A\n\n\nOne Day\n\n\nOne Week\n\n\nOne Month\n\n\n\n\n\n\n\n\nEscalation Contacts\n\n\n\n\n\n\n\n\nEscalation Level\n\n\nOSG Contact\n\n\n\n\n\n\n\n\n\n\n1st\n\n\nOSG Operations Infrastructure Lead\n\n\n\n\n\n\n2nd\n\n\nOSG Operations Coordinator\n\n\n\n\n\n\n3rd\n\n\nOSG Production Coordinator\n\n\n\n\n\n\n4th\n\n\nOSG Technical Director and Executive Director\n\n\n\n\n\n\n\n\nDetailed information on contacts are viewable on the following \n2F15\n2F2009&end_type=now&end_date=09\n2F15\n2F2009&site_10047=on&rg=on&rg_336=on&gridtype=on&gridtype_1=on&active=on&active_value=1&disable_value=1\">MyOSG URL\n, and are maintained within the\n\n\nAny ongoing \"Normal\" or \"Elevated\" level issues will be discussed at the weekly \nOperations\n and \nProduction\n meetings.\n\n\nService Availability and Outages\n\n\nThe GOC will strive for 97% service availability. If service availability falls below 97% monthly as monitored by the GOC on two consecutive months a root cause analysis and service plan will be submitted to the OSG stakeholders for plans to restore an acceptable level of service availability.\n\n\nService Support Hours\n\n\nThe service is supported 24x7 by the GOC and Indiana University. All issues will be investigated by the next business day.\n\n\nService Off-Hours Support Procedures\n\n\nXSEDE users should report problems via the \nXSEDE trouble ticket\n system. Others should contact the GOC via the\n\n\nRequests for Service Enhancements\n\n\nThis section deals with planned maintenance outages. Please see \nService Target Response Priorities and Response Times\n for information on unplanned outages.\n\n\nThe OSG Operations will respond to customer requests for service enhancements based on GOC determination of the necessity and desirability of the enhancement. The GOC reserves the right to enhance the physical environment of the service based on IU and GOC needs. No enhancement will occur without advanced notice to the OSG community.\n\n\nCustomer Problem Reporting\n\n\nThe GOC provides operators 24x7x365. Service problems should be reported immediately by one of the following mechanisms.\n\n\n\n\nCreating a problem ticket at https://tickets.xsede.org (\npreferred for XSEDE users\n)\n\n\nCreating a problem ticket at https://ticket.grid.iu.edu/goc/submit (\npreferred for other users\n)\n\n\nEmailing a description to XD-Login-support@opensciencegrid.org\n\n\nCalling the GOC phone at 317-278-9699\n\n\n\n\nResponsibilities\n\n\nCustomer Responsibilities\n\n\nXD-Login customers agree to:\n\n\n\n\nUse the service for purposes of XSEDE or OSG approved work only.\n\n\nAlert the GOC if they are going to use the Service in a non-standard way, this includes testing or anticipated mass increases in usage.\n\n\nContact support by means outlined in the \nCustomer Problem Reporting\n section of this document if they encounter any service issues.\n\n\nBe willing and available to provide information within one business day for any High level issues reported.\n\n\n\n\nResponsibilities\n\n\nGOC operations:\n   * Maintain the physical machine hosting the service\n   * Assure the service is accessible via its advertised URL\n   * Make changes and updates within the normal GOC \nrelease schedule\n\n   * Meet response times associated with the priority assigned by users for issues related to the hardware.\n   * Maintain appropriately trained staff.\n   * The OSG and GOC are not responsible if a customer does not provide testing during the testing period. In such cases, the GOC has final discretion in what remedial actions to take.\n\n\nISI:\n   * Is responsible for all aspects of the service not listed above.\n\n\nGOC Service Desk Responsibilities:\n   * Log and track all Customer requests for service through the OSG ticketing system.\n\n\nDatabase & Application Services responsibilities:\n   * Announce and negotiate maintenance with stakeholders to assure minimal interruption to normal workload.\n   * Alert the community of scheduled maintenance periods at least five (5) business days prior to the start of a service affecting maintenance window.\n\n\nService Measuring and Reporting\n\n\nThe GOC will provide the customer with the following reports in the intervals indicated (monthly, quarterly, semi-annually, or annually):\n\n\n\n\n\n\n\n\nReport Name\n\n\nReporting Interval\n\n\nDelivery Method\n\n\nResponsible Party\n\n\n\n\n\n\n\n\n\n\nSystem Uptime\n\n\nMonthly\n\n\nWeb Posting\n\n\nGOC\n\n\n\n\n\n\nService Uptime\n\n\nMonthly\n\n\nWeb Posting\n\n\nGOC\n\n\n\n\n\n\nReport of Critical and High Priority Issues\n\n\nQuarterly\n\n\nWeb Posting\n\n\nGOC\n\n\n\n\n\n\n\n\nThese reports will be posted in Appendix E of this document.\n\n\nSLA Validity Period\n\n\nThis SLA will be in affect for one year.\n\n\nSLA Review Procedure\n\n\nThis SLA will renew automatically on a yearly basis unless change or update is requested by the OSG Operations Coordinator, a representitive of ISI, the OSG Executive Team or the Stakeholders.\n\n\nReferences\n\n\nAppendix A - Customer Information\n\n\nAll service end-users who are members of an OSG VO and OSG Staff are considered customers.\nAll XSEDE staff and all XSEDE users with an OSG allocation are also customers.\n\n\nAppendix B - Other Service Dependencies\n\n\nThe service is dependent on the following services to collect and distribute information:\n   * Local Network and Hardware\n\n\nAppendix C - Supported Hardware and Software\n\n\nSupported Hardware\n\n\nThe following hardware is supported:\n   * Physical devices used to provide the service.\n   * Physical devices used to provide the environment used to house the service.\n\n\nHardware Services\n\n\nThe following hardware services are provided:\n   * Recommendations. OSG Operations will be responsible for specifying and recommending for purchase or lease hardware meeting customers' needs.\n   * Installation. OSG Operations will install, configure and customize system hardware and operating systems.\n   * Upgrades. OSG Operations is responsible for specifying and recommending for purchase any hardware upgrades.\n   * Diagnosis. OSG Operations will diagnose problems with service related hardware.\n   * Repair. OSG Operations analysts are not hardware technicians and receive no training in hardware maintenance, nor do we have the test equipment and tools necessary to do such work.\n\n\nPerforming repairs under warranty: Any work to be performed under warranty may be referred to the warranty service provider at the discretion of the Service Provider analyst(s). Service Provider analysts will not undertake work that will void warranties on customer hardware unless specifically requested and authorized by customer's management in writing.\n\n\nObtaining repair services: The Service Provider analyst will recommend a service vendor whenever he/she feels the repair work requires specialized skills or tools.\n\n\n\n\nBackup. Service Provider agrees to fully back up all Service Provider-supported software and data nightly every business day.\n\n\n\n\nSoftware Services\n\n\nService Provider agrees to cover software support services, including software installations and upgrades. All software maintenance periods will be announced via the policy put forth in the \nOSG Operations Responsibilities\n section of this document.\n\n\nSoftware Costs\n\n\nIU and the Grid Operations Center bears all costs for new and replacement software.\n\n\nAppendix D - Approval\n\n\n\n\n\n\n\n\nApproved By\n\n\nPosition\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAppendix E - Metric Reports\n\n\n\n\n[[ServiceLevelAgreements#Supporting_Documents][Recent availability statistics]]",
            "title": "XDLogin"
        },
        {
            "location": "/SLA/xdlogin/#xd-login-service-level-agreement",
            "text": "",
            "title": "XD-Login Service Level Agreement"
        },
        {
            "location": "/SLA/xdlogin/#version-control",
            "text": "Version Number  Date  Author  Comments      0.1  2-18-2013  Scott Teige  First Draft",
            "title": "Version Control"
        },
        {
            "location": "/SLA/xdlogin/#executive-summary",
            "text": "This SLA is an agreement between OSG Operations, Information Sciences Institute (ISI) and the OSG Management and Stakeholders describing details\nof the OSG-XD job submission system.",
            "title": "Executive Summary"
        },
        {
            "location": "/SLA/xdlogin/#owners",
            "text": "This SLA is owned by OSG Operations, Indiana University and ISI and will be reviewed and agreed upon by the OSG Executive Team and OSG Stakeholders.",
            "title": "Owners"
        },
        {
            "location": "/SLA/xdlogin/#service-name-and-description",
            "text": "",
            "title": "Service Name and Description"
        },
        {
            "location": "/SLA/xdlogin/#name",
            "text": "The service is built on two physical machines hosted by the GOC. These are\nthe production XD-Login job submission node and an adjunct machine, OSG-FLOCK.",
            "title": "Name"
        },
        {
            "location": "/SLA/xdlogin/#description",
            "text": "The XD-Login submission node allows OSG resources to be accessed by XSEDE users. A complete description is available  here.",
            "title": "Description"
        },
        {
            "location": "/SLA/xdlogin/#security-considerations",
            "text": "The XD-Login submission node supports Single Sign On through the XSEDE User Portal, and also from the command line using gsissh with a grid certificate for authentication.\nOSG-FLOCK is not publicly accesible.",
            "title": "Security Considerations"
        },
        {
            "location": "/SLA/xdlogin/#service-target-response-priorities-and-response-times",
            "text": "This section deals with unplanned outages. Please see  Requests for Service Enhancement  for information on planned maintenance outages.     Critical  High  Elevated  Normal      Work Outage  * *  * *  * *    This Service does not have critical priority  The issue causes a full service outage rendering the service unavailable  The issue causes short (less than 15 minute) periods of unstable or inconsistent performance  The issue causes minor (less than 5 minutes) periods of unstable or inconsistent performance    Number of Clients Affected  * *  * *  * *    N/A  The issue affects all users  The issue may or may not affect all users  The issue affects only a small number of users    Response Time  * *  * *  * *    N/A  Within the next business day  Within the next business day  Within five (5) business days    Resolution Time  * *  * *  * *    N/A  The maximum acceptable resolution time is one full (1) business day  The maximum acceptable resolution time is five (5) business days  The maximum acceptable resolution time is thirty (30) business days    Escalates Every  * *  * *  * *    N/A  One Day  One Week  One Month",
            "title": "Service Target Response Priorities and Response Times"
        },
        {
            "location": "/SLA/xdlogin/#escalation-contacts",
            "text": "Escalation Level  OSG Contact      1st  OSG Operations Infrastructure Lead    2nd  OSG Operations Coordinator    3rd  OSG Production Coordinator    4th  OSG Technical Director and Executive Director     Detailed information on contacts are viewable on the following  2F15 2F2009&end_type=now&end_date=09 2F15 2F2009&site_10047=on&rg=on&rg_336=on&gridtype=on&gridtype_1=on&active=on&active_value=1&disable_value=1\">MyOSG URL , and are maintained within the  Any ongoing \"Normal\" or \"Elevated\" level issues will be discussed at the weekly  Operations  and  Production  meetings.",
            "title": "Escalation Contacts"
        },
        {
            "location": "/SLA/xdlogin/#service-availability-and-outages",
            "text": "The GOC will strive for 97% service availability. If service availability falls below 97% monthly as monitored by the GOC on two consecutive months a root cause analysis and service plan will be submitted to the OSG stakeholders for plans to restore an acceptable level of service availability.",
            "title": "Service Availability and Outages"
        },
        {
            "location": "/SLA/xdlogin/#service-support-hours",
            "text": "The service is supported 24x7 by the GOC and Indiana University. All issues will be investigated by the next business day.",
            "title": "Service Support Hours"
        },
        {
            "location": "/SLA/xdlogin/#service-off-hours-support-procedures",
            "text": "XSEDE users should report problems via the  XSEDE trouble ticket  system. Others should contact the GOC via the",
            "title": "Service Off-Hours Support Procedures"
        },
        {
            "location": "/SLA/xdlogin/#requests-for-service-enhancements",
            "text": "This section deals with planned maintenance outages. Please see  Service Target Response Priorities and Response Times  for information on unplanned outages.  The OSG Operations will respond to customer requests for service enhancements based on GOC determination of the necessity and desirability of the enhancement. The GOC reserves the right to enhance the physical environment of the service based on IU and GOC needs. No enhancement will occur without advanced notice to the OSG community.",
            "title": "Requests for Service Enhancements"
        },
        {
            "location": "/SLA/xdlogin/#customer-problem-reporting",
            "text": "The GOC provides operators 24x7x365. Service problems should be reported immediately by one of the following mechanisms.   Creating a problem ticket at https://tickets.xsede.org ( preferred for XSEDE users )  Creating a problem ticket at https://ticket.grid.iu.edu/goc/submit ( preferred for other users )  Emailing a description to XD-Login-support@opensciencegrid.org  Calling the GOC phone at 317-278-9699",
            "title": "Customer Problem Reporting"
        },
        {
            "location": "/SLA/xdlogin/#responsibilities",
            "text": "",
            "title": "Responsibilities"
        },
        {
            "location": "/SLA/xdlogin/#customer-responsibilities",
            "text": "XD-Login customers agree to:   Use the service for purposes of XSEDE or OSG approved work only.  Alert the GOC if they are going to use the Service in a non-standard way, this includes testing or anticipated mass increases in usage.  Contact support by means outlined in the  Customer Problem Reporting  section of this document if they encounter any service issues.  Be willing and available to provide information within one business day for any High level issues reported.",
            "title": "Customer Responsibilities"
        },
        {
            "location": "/SLA/xdlogin/#responsibilities_1",
            "text": "GOC operations:\n   * Maintain the physical machine hosting the service\n   * Assure the service is accessible via its advertised URL\n   * Make changes and updates within the normal GOC  release schedule \n   * Meet response times associated with the priority assigned by users for issues related to the hardware.\n   * Maintain appropriately trained staff.\n   * The OSG and GOC are not responsible if a customer does not provide testing during the testing period. In such cases, the GOC has final discretion in what remedial actions to take.  ISI:\n   * Is responsible for all aspects of the service not listed above.  GOC Service Desk Responsibilities:\n   * Log and track all Customer requests for service through the OSG ticketing system.  Database & Application Services responsibilities:\n   * Announce and negotiate maintenance with stakeholders to assure minimal interruption to normal workload.\n   * Alert the community of scheduled maintenance periods at least five (5) business days prior to the start of a service affecting maintenance window.",
            "title": "Responsibilities"
        },
        {
            "location": "/SLA/xdlogin/#service-measuring-and-reporting",
            "text": "The GOC will provide the customer with the following reports in the intervals indicated (monthly, quarterly, semi-annually, or annually):     Report Name  Reporting Interval  Delivery Method  Responsible Party      System Uptime  Monthly  Web Posting  GOC    Service Uptime  Monthly  Web Posting  GOC    Report of Critical and High Priority Issues  Quarterly  Web Posting  GOC     These reports will be posted in Appendix E of this document.",
            "title": "Service Measuring and Reporting"
        },
        {
            "location": "/SLA/xdlogin/#sla-validity-period",
            "text": "This SLA will be in affect for one year.",
            "title": "SLA Validity Period"
        },
        {
            "location": "/SLA/xdlogin/#sla-review-procedure",
            "text": "This SLA will renew automatically on a yearly basis unless change or update is requested by the OSG Operations Coordinator, a representitive of ISI, the OSG Executive Team or the Stakeholders.",
            "title": "SLA Review Procedure"
        },
        {
            "location": "/SLA/xdlogin/#references",
            "text": "",
            "title": "References"
        },
        {
            "location": "/SLA/xdlogin/#appendix-a-customer-information",
            "text": "All service end-users who are members of an OSG VO and OSG Staff are considered customers.\nAll XSEDE staff and all XSEDE users with an OSG allocation are also customers.",
            "title": "Appendix A - Customer Information"
        },
        {
            "location": "/SLA/xdlogin/#appendix-b-other-service-dependencies",
            "text": "The service is dependent on the following services to collect and distribute information:\n   * Local Network and Hardware",
            "title": "Appendix B - Other Service Dependencies"
        },
        {
            "location": "/SLA/xdlogin/#appendix-c-supported-hardware-and-software",
            "text": "",
            "title": "Appendix C - Supported Hardware and Software"
        },
        {
            "location": "/SLA/xdlogin/#supported-hardware",
            "text": "The following hardware is supported:\n   * Physical devices used to provide the service.\n   * Physical devices used to provide the environment used to house the service.",
            "title": "Supported Hardware"
        },
        {
            "location": "/SLA/xdlogin/#hardware-services",
            "text": "The following hardware services are provided:\n   * Recommendations. OSG Operations will be responsible for specifying and recommending for purchase or lease hardware meeting customers' needs.\n   * Installation. OSG Operations will install, configure and customize system hardware and operating systems.\n   * Upgrades. OSG Operations is responsible for specifying and recommending for purchase any hardware upgrades.\n   * Diagnosis. OSG Operations will diagnose problems with service related hardware.\n   * Repair. OSG Operations analysts are not hardware technicians and receive no training in hardware maintenance, nor do we have the test equipment and tools necessary to do such work.  Performing repairs under warranty: Any work to be performed under warranty may be referred to the warranty service provider at the discretion of the Service Provider analyst(s). Service Provider analysts will not undertake work that will void warranties on customer hardware unless specifically requested and authorized by customer's management in writing.  Obtaining repair services: The Service Provider analyst will recommend a service vendor whenever he/she feels the repair work requires specialized skills or tools.   Backup. Service Provider agrees to fully back up all Service Provider-supported software and data nightly every business day.",
            "title": "Hardware Services"
        },
        {
            "location": "/SLA/xdlogin/#software-services",
            "text": "Service Provider agrees to cover software support services, including software installations and upgrades. All software maintenance periods will be announced via the policy put forth in the  OSG Operations Responsibilities  section of this document.",
            "title": "Software Services"
        },
        {
            "location": "/SLA/xdlogin/#software-costs",
            "text": "IU and the Grid Operations Center bears all costs for new and replacement software.",
            "title": "Software Costs"
        },
        {
            "location": "/SLA/xdlogin/#appendix-d-approval",
            "text": "Approved By  Position  Date",
            "title": "Appendix D - Approval"
        },
        {
            "location": "/SLA/xdlogin/#appendix-e-metric-reports",
            "text": "[[ServiceLevelAgreements#Supporting_Documents][Recent availability statistics]]",
            "title": "Appendix E - Metric Reports"
        },
        {
            "location": "/external-oasis-repos/",
            "text": "External OASIS Repositories\n\n\nWe offer hosting of non-OSG CVMFS repositories on OASIS. This means that requests to create, rename, remove, or blanking OASIS repositories will come in as GOC tickets. This document contains instructions for handling those tickets.\n\n\n\n\nAlso see\n\n\n\n\nPolicy for OSG Mirroring of External CVMFS repositories\n\n\nExternal OASIS repository\n\n\n\n\n\n\nRequests to Host a Repository on OASIS\n\n\n\n\n\n\nEnsure that the repository administrator is valid for the VO. This can be done by (a) OSG already having a\n    relationship with the person or (b) the contacting the VO manager to find out. Also, the person should be\n    listed in the OSG topology \ncontacts list\n.\n\n\n\n\n\n\nReview provided URL and verify that it is appropriate for the VO. Then, add the repository URL to the topology \n    for given VO under the \nOASISRepoURLs\n. This should cause the repository's configuration\n    to be added to the OSG Stratum-0 within 15 minutes after URL is added into the topology.\n    For example, if new URL is for the VO DUNE \nhttp://hcc-cvmfs-repo.unl.edu:8000/cvmfs/dune.osgstorage.org\n \n    edit the following under the OASIS section and create PR:\n\n\ngit clone git://github.com/opensciencegrid/topology.git\n\n\nvim topology/virtual-organizations/DUNE.yaml\n\n\n...\n\n\nOASIS:\n\n\n  OASISRepoURLs:\n\n\n  - http://hcc-cvmfs-repo.unl.edu:8000/cvmfs/dune.osgstorage.org/\n\n\n...\n\n\n\n\n\n\nWhen PR is approved, check on the \noasis.opensciencegrid.org\n host whether the new repository was successfuly signed.\nThere should be sign of it in the log file \n/var/log/oasis/generate_whitelists.log\n:\n\n\nTue Sep 25 17:34:02 2018 Running add_osg_repository http://hcc-cvmfs-repo.unl.edu:8000/cvmfs/dune.osgstorage.org\ndune.osgstorage.org: Signing 7 day whitelist with masterkeycard... done\n\n\n\n\n\n\nIf the respository ends in a new domain name \ndomain.name\n that has not been distributed before, then place a copy of the\n    \ndomain.name.pub\n public key into \n/srv/etc/keys\n on both \noasis-replica\n and\n    \noasis-replica-itb\n. If you do not have that key, then ask the repository service representative how to obtain it. In\n    order to support CVMFS client versions 2.2.X, also make a symbolic link of \ndomain.name.conf\n\n    \n/cvmfs/config-osg.opensciencegrid.org/etc/cvmfs/domain.d\n pointing to \ndefault.conf\n. This symbolic link has to be\n    created on the \noasis-itb\n machine's copy of the \nconfig-osg.opensciencegrid.org\n repository and then copied to\n    production with the \ncopy_config_osg\n command on the oasis machine.\n\n\n\n\n\n\nIf the repository name does not match \n*.opensciencegrid.org\n or \n*.osgstorage.org\n, skip this step and go on to your next step.\n    If it does match one of those two patterns, then respond to the ticket to tell the administrator to continue with their next step (their step 4).\n\n    We don't want them to continue before 15 minutes has elapsed after step 2 above, so either wait that much time or tell them the time they may proceed (15 minutes after you updated OIM).\n    Then wait until the admin has updated the ticket to indicate that they have completed their step before moving on. \n\n\n\n\n\n\nAsk the administrator of the BNL stratum 1 \nJohn S. De Stefano Jr.\n also add the new repository. The BNL Stratum-1 administrator\n    should set the service to read from\n    \nhttp://oasis-replica.opensciencegrid.org:8000/cvmfs/\nexample.opensciencegrid.org\n. When the BNL\n    Stratum-1 operator has reported back that the replication is ready, respond to the requester that the repository is\n    fully replicated on the OSG and close the ticket.\n\n\n\n\n\n\nRequests to Change the URL of an External Repository\n\n\nIf there is a request to change the URL of an external repository, update the registered value in \nOASISRepoURLs\n for the respective VO in the topology.\n\nTell the requester that it is ready 15 minutes after topology is updated.\n\n\nRequests to Remove an External Repository\n\n\n\n\nAfter validating that the ticket submitter is authorized by the VO's OASIS manager, delete the registered value\n    for \nexample.opensciencegrid.org\n in OIM for the VO in OASIS Repo URLs.\n\n\nAdd the FNAL and BNL Stratum-1 operators to the ticket and ask them to remove the repository. Wait for the\n    Stratum-1 operators to finish before proceeding.\n\n\nRun \ncvmfs_server rmfs -f \nexample.opensciencegrid.org\n and \nrm -r\n    /oasissrv/cvmfs/\nexample.opensciencegrid.org\n on \noasis-replica-itb\n and \noasis-replica\n\n\nRun the following command on \noasis-itb\n and \noasis\n:\nrm -r /srv/cvmfs/\nexample.opensciencegrid.org\n\n\n\n\n\n\n\n\n\n\nRequests to Blank an External Repository\n\n\n\n\nIf there is a need to shut down the distribution of a repository, run \nblank_osg_repository\n on \noasis-replica\n and\n    give it the full name of the repository. This will rename the repository directories to a name with the current\n    timestamp and replace it with a blank repository.  It includes a step to run on the \noasis\n machine, and attempts to\n    do it with \nssh\n, but if that fails it prints instructions on how to finish by logging in to the \noasis\n machine\n    manually.\n\n\nWhen it is time to put the repository back into production, run \nunblank_osg_repository\n on \noasis-replica\n and\n    gives it the full name of the repository again. This will find the directory with the old timestamp and put it back\n    into service. This step also attempts to \nssh\n to the \noasis\n machine.",
            "title": "External OASIS repositories"
        },
        {
            "location": "/external-oasis-repos/#external-oasis-repositories",
            "text": "We offer hosting of non-OSG CVMFS repositories on OASIS. This means that requests to create, rename, remove, or blanking OASIS repositories will come in as GOC tickets. This document contains instructions for handling those tickets.   Also see   Policy for OSG Mirroring of External CVMFS repositories  External OASIS repository",
            "title": "External OASIS Repositories"
        },
        {
            "location": "/external-oasis-repos/#requests-to-host-a-repository-on-oasis",
            "text": "Ensure that the repository administrator is valid for the VO. This can be done by (a) OSG already having a\n    relationship with the person or (b) the contacting the VO manager to find out. Also, the person should be\n    listed in the OSG topology  contacts list .    Review provided URL and verify that it is appropriate for the VO. Then, add the repository URL to the topology \n    for given VO under the  OASISRepoURLs . This should cause the repository's configuration\n    to be added to the OSG Stratum-0 within 15 minutes after URL is added into the topology.\n    For example, if new URL is for the VO DUNE  http://hcc-cvmfs-repo.unl.edu:8000/cvmfs/dune.osgstorage.org  \n    edit the following under the OASIS section and create PR:  git clone git://github.com/opensciencegrid/topology.git  vim topology/virtual-organizations/DUNE.yaml  ...  OASIS:    OASISRepoURLs:    - http://hcc-cvmfs-repo.unl.edu:8000/cvmfs/dune.osgstorage.org/  ...   When PR is approved, check on the  oasis.opensciencegrid.org  host whether the new repository was successfuly signed.\nThere should be sign of it in the log file  /var/log/oasis/generate_whitelists.log :  Tue Sep 25 17:34:02 2018 Running add_osg_repository http://hcc-cvmfs-repo.unl.edu:8000/cvmfs/dune.osgstorage.org\ndune.osgstorage.org: Signing 7 day whitelist with masterkeycard... done    If the respository ends in a new domain name  domain.name  that has not been distributed before, then place a copy of the\n     domain.name.pub  public key into  /srv/etc/keys  on both  oasis-replica  and\n     oasis-replica-itb . If you do not have that key, then ask the repository service representative how to obtain it. In\n    order to support CVMFS client versions 2.2.X, also make a symbolic link of  domain.name.conf \n     /cvmfs/config-osg.opensciencegrid.org/etc/cvmfs/domain.d  pointing to  default.conf . This symbolic link has to be\n    created on the  oasis-itb  machine's copy of the  config-osg.opensciencegrid.org  repository and then copied to\n    production with the  copy_config_osg  command on the oasis machine.    If the repository name does not match  *.opensciencegrid.org  or  *.osgstorage.org , skip this step and go on to your next step.\n    If it does match one of those two patterns, then respond to the ticket to tell the administrator to continue with their next step (their step 4). \n    We don't want them to continue before 15 minutes has elapsed after step 2 above, so either wait that much time or tell them the time they may proceed (15 minutes after you updated OIM).\n    Then wait until the admin has updated the ticket to indicate that they have completed their step before moving on.     Ask the administrator of the BNL stratum 1  John S. De Stefano Jr.  also add the new repository. The BNL Stratum-1 administrator\n    should set the service to read from\n     http://oasis-replica.opensciencegrid.org:8000/cvmfs/ example.opensciencegrid.org . When the BNL\n    Stratum-1 operator has reported back that the replication is ready, respond to the requester that the repository is\n    fully replicated on the OSG and close the ticket.",
            "title": "Requests to Host a Repository on OASIS"
        },
        {
            "location": "/external-oasis-repos/#requests-to-change-the-url-of-an-external-repository",
            "text": "If there is a request to change the URL of an external repository, update the registered value in  OASISRepoURLs  for the respective VO in the topology. \nTell the requester that it is ready 15 minutes after topology is updated.",
            "title": "Requests to Change the URL of an External Repository"
        },
        {
            "location": "/external-oasis-repos/#requests-to-remove-an-external-repository",
            "text": "After validating that the ticket submitter is authorized by the VO's OASIS manager, delete the registered value\n    for  example.opensciencegrid.org  in OIM for the VO in OASIS Repo URLs.  Add the FNAL and BNL Stratum-1 operators to the ticket and ask them to remove the repository. Wait for the\n    Stratum-1 operators to finish before proceeding.  Run  cvmfs_server rmfs -f  example.opensciencegrid.org  and  rm -r\n    /oasissrv/cvmfs/ example.opensciencegrid.org  on  oasis-replica-itb  and  oasis-replica  Run the following command on  oasis-itb  and  oasis : rm -r /srv/cvmfs/ example.opensciencegrid.org",
            "title": "Requests to Remove an External Repository"
        },
        {
            "location": "/external-oasis-repos/#requests-to-blank-an-external-repository",
            "text": "If there is a need to shut down the distribution of a repository, run  blank_osg_repository  on  oasis-replica  and\n    give it the full name of the repository. This will rename the repository directories to a name with the current\n    timestamp and replace it with a blank repository.  It includes a step to run on the  oasis  machine, and attempts to\n    do it with  ssh , but if that fails it prints instructions on how to finish by logging in to the  oasis  machine\n    manually.  When it is time to put the repository back into production, run  unblank_osg_repository  on  oasis-replica  and\n    gives it the full name of the repository again. This will find the directory with the old timestamp and put it back\n    into service. This step also attempts to  ssh  to the  oasis  machine.",
            "title": "Requests to Blank an External Repository"
        }
    ]
}